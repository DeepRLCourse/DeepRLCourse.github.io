# Deep Learning

## Neural Networks Basics
### Perceptrons and Activation Functions
- Perceptron model and its limitations
- Activation functions (e.g., ReLU, sigmoid, tanh)
- Applications in RL (e.g., policy and value function approximation)

### Feedforward Networks
- Architecture of feedforward neural networks
- Universal approximation theorem
- Applications in RL (e.g., function approximation in Q-learning)

### Backpropagation and Gradient Descent
- Chain rule and computation graphs
- Gradient descent and its variants (batch, mini-batch, stochastic)
- Applications in RL (e.g., training policy networks)

---

## Deep Architectures
### Convolutional Neural Networks (CNNs)
- Convolutional layers and filters
- Pooling and feature extraction
- Applications in RL (e.g., image-based state representation)

### Recurrent Neural Networks (RNNs) and LSTMs
- RNN architecture and vanishing gradient problem
- Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs)
- Applications in RL (e.g., sequential decision-making, POMDPs)

### Transformers and Attention Mechanisms
- Self-attention mechanism and multi-head attention
- Transformer architecture
- Applications in RL (e.g., state representation, multi-agent communication)

---

## Optimization for Deep Learning
### Stochastic Gradient Descent (SGD) Variants
- Momentum, Nesterov accelerated gradient (NAG)
- Adaptive methods (Adam, RMSprop, Adagrad)
- Applications in RL (e.g., optimizing policy and value networks)

### Regularization Techniques
- Dropout and its variants (e.g., spatial dropout)
- Weight decay (L2 regularization) and L1 regularization
- Applications in RL (e.g., preventing overfitting in function approximation)

### Advanced Optimization Techniques
- Learning rate scheduling (e.g., cosine annealing, step decay)
- Gradient clipping and normalization
- Applications in RL (e.g., stabilizing training in deep RL algorithms)

---

## Deep Learning Frameworks
### PyTorch Basics
- Tensors and automatic differentiation
- Building and training neural networks
- Applications in RL (e.g., implementing DQN, PPO)

### Jax Basics
- Functional programming and JIT compilation
- Gradients and optimization in Jax
- Applications in RL (e.g., scalable and efficient RL implementations)