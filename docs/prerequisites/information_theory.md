# Information Theory

## Basic Concepts
### Entropy
- Definition and intuition
- Properties of entropy (non-negativity, symmetry, additivity)
- Joint entropy and conditional entropy

### Mutual Information
- Definition and interpretation
- Relationship with entropy and conditional entropy
- Applications in feature selection and representation learning

### Kullback-Leibler (KL) Divergence
- Definition and properties
- Connection to entropy and cross-entropy
- Applications in RL (e.g., policy optimization, trust region methods)

### Shannonâ€™s Source Coding Theorem
- Statement and intuition
- Implications for data compression
- Connection to lossless and lossy compression

---

## Advanced Concepts
### Cross-Entropy
- Definition and relationship with KL divergence
- Applications in RL (e.g., loss functions for policy gradients)

### Information Bottleneck
- Definition and intuition
- Applications in representation learning and RL

### Rate-Distortion Theory
- Basics of rate-distortion trade-offs
- Applications in RL for efficient state representation
