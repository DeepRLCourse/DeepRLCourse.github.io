{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Welcome to Spring 2025 offering of Deep Reinforcement Learning course at Sharif University of Technology! We are excited to have you join us on this journey into the world of deep reinforcement learning.</p>"},{"location":"#course-description","title":"Course Description","text":"<p>This course provides an in-depth introduction to the field of deep reinforcement learning. Initially, we will explore reinforcement learning conceptually and practically to help you grasp the fundamental concepts. This phase will take place before Nowrouz. After Nowrouz, we will delve deeper into the subject, focusing on advanced topics. The course will cover both classical reinforcement learning and deep reinforcement learning, including interesting topics such as multi-agent RL, offline methods, and meta RL. By the end of the course, you will have a solid understanding of how to apply deep reinforcement learning to solve complex problems in various domains.</p>"},{"location":"#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the fundamentals of reinforcement learning</li> <li>Apply reinforcement learning to various domains</li> <li>Use deep learning techniques to handle large state spaces in RL</li> <li>Master the concepts and gain practical understanding of RL</li> <li>Gain hands-on experience with important RL problems</li> <li>Equip students with enough theoretical knowledge to understand research papers</li> </ul>"},{"location":"#instructor","title":"Instructor","text":"<ul> <li> <p>Dr. Mohammad Hossein Rohban</p> <p>Instructor</p> <p>rohban@sharif.edu</p> <p> </p> </li> </ul>"},{"location":"#guests","title":"Guests","text":"<p> Richard Sutton      <p> </p> <p> Chris Watkins      <p> </p> <p> Michael Littman      <p> </p> <p> Peter Stone      <p> </p> <p> Jakob Foerster      <p> </p> <p> Nan Jiang      <p> </p> <p> Benjamin Eysenbach      <p> </p> <p> Ian Osband      <p> </p> <p> Jeff Clune      <p> </p> <p> Peter Dayan      <p> </p> <p> Ida Momennejad      <p> </p> <p> Abhishek Gupta      <p> </p> <p> Benjamin Van Roy      <p> </p> <p> Wolfram Schultz      <p> </p> <p> Mark Ho      <p> </p> <p> Pascal Poupart </p> <p> </p> <p> Peter Norvig </p> <p> </p> <p> Karl Friston </p> <p> </p> <p> Amy Zhang </p> <p> </p> <p> Adam White </p> <p> </p> <p> Anne Collins </p> <p> </p> <p> Luis Serrano </p> <p> </p> <p> Martha White </p> <p> </p> <p> Christopher Amato </p> <p> </p> <p> Marlos Machado </p> <p> </p>"},{"location":"#schedule","title":"Schedule","text":""},{"location":"#conceptualpractical","title":"Conceptual/Practical","text":"Week # Topic of the Week Lecture 1 Lecture 2 Homework Week 1 Introduction to RL \u06f2\u06f1 \u0628\u0647\u0645\u0646(February 9) \u06f2\u06f3 \u0628\u0647\u0645\u0646(February 11) HW 1 Week 2 Value-Based Methods \u06f2\u06f8 \u0628\u0647\u0645\u0646(February 16) \u06f3\u06f0 \u0628\u0647\u0645\u0646(February 18) HW 2 Week 3 Policy-Based Methods \u06f5 \u0627\u0633\u0641\u0646\u062f(February 23) \u06f7 \u0627\u0633\u0641\u0646\u062f(February 25) HW 3 Week 4 Advanced Methods \u06f1\u06f2 \u0627\u0633\u0641\u0646\u062f(March 2) \u06f1\u06f4 \u0627\u0633\u0641\u0646\u062f(March 4) HW 4 Week 5 Model-Based Methods \u06f1\u06f9 \u0627\u0633\u0641\u0646\u062f(March 9) \u06f2\u06f1 \u0627\u0633\u0641\u0646\u062f(March 11) HW 5 Week 6 Multi-Armed Bandits \u06f2\u06f6 \u0627\u0633\u0641\u0646\u062f(March 16) \u06f2\u06f8 \u0627\u0633\u0641\u0646\u062f(March 18) HW 6"},{"location":"#in-depththeoritical","title":"In Depth/Theoritical","text":"Week # Topic of the Week Lecture 1 Lecture 2 Homework Week 7 Value-Based Theory \u06f1\u06f7 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646(April 6) \u06f1\u06f9 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646(April 8) HW 7 Week 8 Policy-Based Theory \u06f2\u06f4 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646(April 13) \u06f2\u06f6 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646(April 15) HW 8 Week 9 Advanced Theory \u06f3\u06f1 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646(April 20) \u06f2 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(April 22) HW 9 Week 10 Exploration Methods \u06f7 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(April 27) \u06f9 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(April 29) HW 10 Week 11 Imitation &amp; Inverse RL \u06f1\u06f4 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(May 4) \u06f1\u06f6 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(May 6) HW 11 Week 12 Offline Methods \u06f2\u06f1 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(May 11) \u06f2\u06f3 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(May 13) HW 12 Week 13 Multi-Agent Methods \u06f2\u06f8 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(May 18) \u06f3\u06f0 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(May 20) HW 13 Week 14 Hierarchical &amp; Meta RL \u06f4 \u062e\u0631\u062f\u0627\u062f(May 25) \u06f6 \u062e\u0631\u062f\u0627\u062f(May 27) HW 14"},{"location":"#guest-lectures","title":"Guest Lectures","text":"Week # Topic of the Week Lecture 1 Lecture 2 Homework Week 15 Guest Lectures \u06f1\u06f1 \u062e\u0631\u062f\u0627\u062f(June 1) \u06f1\u06f3 \u062e\u0631\u062f\u0627\u062f(June 3) -"},{"location":"#logistics-policies","title":"Logistics &amp; Policies","text":"<ul> <li> <p>Lectures: Held on Sundays and Tuesdays from 1:30 PM to 3:00 PM in room 102 of the CE department.</p> </li> <li> <p>Recitation Classes: Weekly sessions where TAs review the last two lectures and solve related problems. These sessions will be held in person on Wednesdays, except for week 15 when there will be no recitation class.</p> </li> <li> <p>Homework: Will be released on Sunday. Due dates will be provided in the following table.</p> </li> </ul> Homework Release Date Due Date Details HW1-5 Sundayof the week Sundayof next week @ 11:59 PM HW6 \u06f2\u06f6 \u0627\u0633\u0641\u0646\u062f(March 16) \u06f1\u06f7 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646(April 6) @ 11:59 PM HW7-9 Sundayof the week \u06f2\u06f1 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(May 11) @ 11:59 PM HW10-11 Sundayof the week \u06f4 \u062e\u0631\u062f\u0627\u062f(May 25) @ 11:59 PM HW12-14 Sundayof the week \u06f2\u06f5 \u062e\u0631\u062f\u0627\u062f(June 15) @ 11:59 PM <ul> <li> <p>Homework Bonus: Some homeworks may have an optional bonus part that can earn you up to 0.75 bonus points.</p> </li> <li> <p>Slack Days: You have a total of 14 slack days throughout the course with no penalty for submitting your homework late. For each homework, you can use up to 7 slack days. After 7 days, the solution will be released, and no further submissions will be accepted. Any additional delays beyond the slack days will result in a 0.5% reduction in the assignment grade for every hour of delay. We have a flat reduction policy from 3 AM to 11 AM (for your convenience to rest peacefully!). The 7 days for submitting your work for each homework is a hard deadline, and after that, you will receive a 0 grade because we will release the solution to the homework.</p> <p>For the 6th homework and the last homeworks (12th, 13th, and 14th), due to the midterm exam and final exam, the solutions will be released on 1404/01/20 [\u06f2\u06f0 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646] (April 9) and 1404/03/30 [\u06f3\u06f0 \u062e\u0631\u062f\u0627\u062f] (June 20), respectively. You have a 3-day hard deadline for the 6th homework and a 5-day hard deadline for the last homeworks.</p> </li> <li> <p>Workshop Classes: Held for all weeks except weeks 7, 8, 9, and 15. These workshops will present practical implementations of the ideas covered in the lectures of the week. These sessions will be held online on Wednesdays.</p> </li> <li> <p>Lecture Summaries and Quizzes: Summaries of the previous lecture will be released at 8:00 AM on the day of the next lecture. You must participate in a quiz before the start of the lecture at 1:30 PM. Participation in quizzes will earn you 0.75 bonus points.</p> </li> <li> <p>Exams: Midterm questions will focus on conceptual understanding, while the final exam will be more theoretical.</p> </li> <li> <p>Poster Session: There will be a poster session at the end of the course. Presenting at the poster session can earn you 1 point of course credit, with the ability to get an additional 0.25 bonus credit for extra work.</p> </li> <li> <p>Feedback: Participation in all feedback sessions throughout the course will add up to 0.75 bonus points.</p> </li> <li> <p>Prerequisite: Prerequisite classes will be held based on demand. A form will be released for each session, and we will decide to hold it based on your responses.</p> </li> <li> <p>Journal Clubs: Journal clubs will be held weekly throughout the course. Their schedule and details will be announced. Participating in each of them can give you 0.1 bonus points, up to 0.5.</p> </li> <li> <p>Course Calendar: Office hours, lecture schedules, recitations, workshops, deadlines, and all important events can be found on the course calendar.</p> </li> <li> <p>Support: You can ask questions on Telegram Group or schedule office hours with a TA on the calender for additional guidance.</p> </li> <li> <p>Optional Activities: There will be an optional visit to Taarlab in the middle of the course, and maybe a few more fun and inspiring activities that we will announce throughout the course! We are full of surprises this semester \ud83d\ude80</p> </li> </ul>"},{"location":"#grading","title":"Grading","text":"<p>The grading for the Deep Reinforcement Learning course is structured as follows:</p>"},{"location":"#main-components","title":"Main Components","text":"<ul> <li>Homeworks: Gradual assessment through regular assignments</li> <li>Midterm: Conceptual understanding tested mid-course</li> <li>Final: Theoretical knowledge evaluated at the end of the course</li> <li>Poster Session: Presentation at the end of the course</li> </ul> Component Points Date Details Homeworks 7 - 14 HWs \\(\\times \\approx\\) 0.5 each Midterm 5 \u06f2\u06f1 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646(April 10) @ 9:00 AM Final 7 \u06f1 \u062a\u06cc\u0631(June 22) @ 8:00 AM Poster Session 1 End of course TBA"},{"location":"#bonus-components","title":"Bonus Components","text":"<p>Additional opportunities to earn bonus points:</p> Component Points Quizzes 0.75 Feedback 0.75 Homeworks Bonus 0.75 Poster Session Bonus 0.25 Journal Clubs 0.5 <p>Total possible points: 20 + 3 = 23</p>"},{"location":"#head-assistants","title":"Head Assistants","text":"<ul> <li> <p>Arash Alikhani</p> <p>Lead Head TA</p> <p>infinity2357@gmail.com</p> <p> </p> </li> </ul> <ul> <li> <p>Soroush VafaieTabar</p> <p>Head TA</p> <p>svafaiet@gmail.com</p> <p> </p> </li> <li> <p>Amir Mohammad Izadi</p> <p>Head TA</p> <p>amirmmdizady@gmail.com</p> <p> </p> </li> </ul>"},{"location":"#teaching-assistants","title":"Teaching Assistants","text":"<ul> <li> <p> <p>Abdollah Zohrabi</p> <p>Teaching Assistant</p> <p>abdollahzz1381@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Ahmad Karami</p> <p>Teaching Assistant</p> <p>ahmad.karami77@yahoo.com</p> <p> </p> </p> </li> <li> <p> <p>SeyyedAli MirGhasemi</p> <p>Teaching Assistant</p> <p>sam717269@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Alireza Nobakht</p> <p>Teaching Assistant</p> <p>a.nobakht13@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Amirabbas Afzali</p> <p>Teaching Assistant</p> <p>amir8afzali@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Amirhossein Asadi</p> <p>Teaching Assistant</p> <p>amirhossein.asadi1681@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Amirreza Velaei</p> <p>Teaching Assistant</p> <p>amirrezavelae@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Armin Saghafian</p> <p>Teaching Assistant</p> <p>armin.saghafian@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Arshia Gharooni</p> <p>Teaching Assistant</p> <p>arshiyagharoony@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Behnia Soleymani</p> <p>Teaching Assistant</p> <p>ibehnia.s@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Benyamin Naderi</p> <p>Teaching Assistant</p> <p>benjaminndr79@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Dariush Jamshidian</p> <p>Teaching Assistant</p> <p>drjm313@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Faezeh Sadeghi</p> <p>Teaching Assistant</p> <p>fz.saadeghi@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Ghazal Hosseini</p> <p>Teaching Assistant</p> <p>ghazaldesu@gmail.com</p> <p> </p> </p> </li> </ul> <ul> <li> <p> <p>Hamidreza Ebrahimpour</p> <p>Teaching Assistant</p> <p>ebrahimpour.7879@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Hesam Hosseini</p> <p>Teaching Assistant</p> <p>hesam138122@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Mahyar Afshinmehr</p> <p>Teaching Assistant</p> <p>mahyarafshinmehr@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Masoud Tahmasbi</p> <p>Teaching Assistant</p> <p>masoudtahmasbifard@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Milad Hosseini</p> <p>Teaching Assistant</p> <p>miladhoseini532@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Mohammad Mohammadi</p> <p>Teaching Assistant</p> <p>mohammadm97i@gmail.com</p> <p> </p> </p> </li> </ul> <ul> <li> <p> <p>MohammadHasan Abbasi</p> <p>Teaching Assistant</p> <p>mohasabbasi@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Naser Kazemi</p> <p>Teaching Assistant</p> <p>naserkazemi2002@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Nima Shirzady</p> <p>Teaching Assistant</p> <p>shirzady.1934@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Ramtin Moslemi</p> <p>Teaching Assistant</p> <p>ramtin.moslemi@yahoo.com</p> <p> </p> </p> </li> <li> <p> <p>Reza GhaderiZadeh</p> <p>Teaching Assistant</p> <p>r.ghaderi2001@gmail.com</p> <p> </p> </p> </li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>We would like to express our gratitude to the following individuals for their invaluable contributions to the Spring 2024 and 2023 offerings of this course. Their efforts have been instrumental in the development and success of this course.</p> Spring 2024 Alireza Ghahremani Alireza Sakhaei Rad Amirhossein Mohammadpour Azari Amirmohammad Izadi Arian Ahadinia Armin Behnamnia Armin Saghefian Behnia Soleimani Hossein Jafariniya Mahdi Ghaznavi Mohammadhassan Alikhani Ramtin Moslemi Spring 2023 Ali Kaheh Amirhossein Mesbah Ayda AfsharMohammadian Bardia Mohammadi Hossein Hassani Hossein Khalili Kiana Asgari Mohammad Mozaffari Negin Hashemi Parsa Haghighi Roozbeh Razavi Sepehr Ghabadi Seyed Abolfazl Rahimi Seyed Mohammad Hosseini Seyed MohammadHossein Mousavi Soroush Vafaie Tabar <p>This offering and all of these changes are thanks to their effort in starting this course.</p>"},{"location":"blog/","title":"Introduction","text":"<p>Welcome to the Deep Reinforcement Learning course blog! Stay tuned for updates, insights, tutorials, papers, and events related to our course.</p>"},{"location":"blog/tags/","title":"Tags","text":""},{"location":"blog/tags/#tag:introduction","title":"introduction","text":"<ul> <li>            Welcome to Deep RL Course!          </li> </ul>"},{"location":"blog/tags/#tag:welcome","title":"welcome","text":"<ul> <li>            Welcome to Deep RL Course!          </li> </ul>"},{"location":"blog/2025/02/08/welcome-to-deep-rl-course/","title":"Welcome to Deep RL Course!","text":"<p>Welcome to the Spring 2025 offering of the Deep Reinforcement Learning course at Sharif University of Technology! This week, we aim to introduce you to the exciting world of Reinforcement Learning. We've planned an engaging and informative week filled with lectures, hands-on sessions, and fun activities to get you started on your RL journey!</p>","tags":["welcome","introduction"]},{"location":"blog/2025/02/08/welcome-to-deep-rl-course/#lectures","title":"Lectures","text":"<p>Get ready for two exciting lectures that will introduce you to the fundamentals of RL.</p> <p>To kick things off, we recommend watching this award-winning documentary before class:</p> <p>And don't miss this talk on the history of reinforcement learning by Professor Andrew Barto himself:</p>","tags":["welcome","introduction"]},{"location":"blog/2025/02/08/welcome-to-deep-rl-course/#recitation","title":"Recitation","text":"<p>We have a recitation session where we will go through the lecture content with clarifying examples.</p>","tags":["welcome","introduction"]},{"location":"blog/2025/02/08/welcome-to-deep-rl-course/#workshop","title":"Workshop","text":"<p>Join us for a fun workshop session where we will get hands-on and implement most of the important ideas covered in the lectures and more!</p>","tags":["welcome","introduction"]},{"location":"blog/2025/02/08/welcome-to-deep-rl-course/#homework","title":"Homework","text":"<p>We've designed a fun homework assignment for you! You'll model some interesting problems in RL and solve them using the StableBaseline framework.</p>","tags":["welcome","introduction"]},{"location":"blog/2025/02/08/welcome-to-deep-rl-course/#guest-lecture","title":"Guest Lecture","text":"<p>We also have a very exciting and surprising guest for this week that we will announce tomorrow!</p> <p>Wish you all the best for this semester!</p>","tags":["welcome","introduction"]},{"location":"course_notes/","title":"Introduction","text":""},{"location":"course_notes/advanced/","title":"Week 4: Advanced Methods","text":""},{"location":"course_notes/advanced/#introduction","title":"Introduction","text":"<p>Reinforcement Learning (RL) has significantly evolved with the development of actor-critic methods, which combine the benefits of policy-based and value-based approaches. These methods utilize policy gradients for optimizing the agent\u2019s actions while employing a critic network to estimate value functions, leading to improved stability.</p> <p>As we explored last week, key concepts like Reward-to-Go and Advantage Estimation lay the foundation for understanding and enhancing these methods. Building on that conversation, this document revisits these ideas, emphasizing their role in refining policy updates and stabilizing training.</p> <p>In actor-critic methods, we explore key concepts that enhance learning efficiency:</p> <ul> <li>Reward-to-Go: A method for computing future. </li> <li>Advantage Estimation: A technique to quantify how much better an action is compared to the expected return.  </li> <li>Generalized Advantage Estimation (GAE): A framework that balances bias and variance in advantage computation, making training more stable.  </li> </ul> <p>This document covers three widely used actor-critic algorithms:</p> <ul> <li>Proximal Policy Optimization (PPO): A popular on-policy algorithm that stabilizes policy updates through a clipped objective function.  </li> <li>Deep Deterministic Policy Gradient (DDPG): An off-policy algorithm designed for continuous action spaces, incorporating experience replay and target networks.  </li> <li>Soft Actor-Critic (SAC): A state-of-the-art off-policy method that introduces entropy maximization to improve exploration and robustness.  </li> </ul> <p>Each section delves into these algorithms, their theoretical foundations, and their practical advantages in reinforcement learning tasks.</p>"},{"location":"course_notes/advanced/#actor-critic","title":"Actor-Critic","text":"<p>The variance of policy methods can originate from two sources:</p> <ol> <li>high variance in the cumulative reward estimate</li> <li>high variance in the gradient estimate. </li> </ol> <p>For both problems, a solution has been developed: bootstrapping for better reward estimates and baseline subtraction to lower the variance of gradient estimates.</p> <p>In the next seciton we will review the concepts bootstrapping (reward to go), using baseline (Advantage value) and Generalized Advantage Estimation (GAE).</p>"},{"location":"course_notes/advanced/#reward-to-go","title":"Reward to Go:","text":"<p>A cumulative reward from state \\(s_t\\) to the end of the episode by applying policy \\(\\pi_\\theta\\).</p> <p>As mentioned earlier, in the policy gradient method, we update our policy weights with the learning rate \\(\\alpha\\) as follows:</p> \\[ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta), \\] <p>where</p> \\[ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t})\\cdot r(s_{i,t},a_{i,t}). \\] <p>In this equation, the term \\(r(s_{i,t}, a_{i,t})\\) is the primary source of variance and noise. We use the causality trick to mitigate this issue by multiplying the policy gradient at state \\(s_t\\) with its future rewards. It is important to note that the policy at state \\(s_t\\) can only affect future rewards, not past ones. The causality trick is represented as follows:</p> \\[ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1}\\bigg( \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t})\\bigg) \\bigg( \\sum^T_{t=1}r(s_{i,t},a_{i,t}) \\bigg) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t})\\bigg( \\sum^T_{t'=t}r(s_{i,t'},a_{i,t'}) \\bigg). \\] <p>The term \\(\\sum^T_{t'=t}r(s_{i,t'},a_{i,t'})\\) is known as reward to go, which is calculated in a Monte Carlo manner. It represents the total expected reward from a given state by applying policy \\(\\pi_\\theta\\), starting from time \\(t\\) to the end of the episode.</p> <p>To further reduce variance, we can approximate the reward to go with the Q-value, which conveys a similar meaning. Thus, we can rewrite \\(\\nabla_\\theta J(\\theta)\\) as:</p> \\[ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t}) Q(s_{i,t},a_{i,t}). \\]"},{"location":"course_notes/advanced/#advantage-value","title":"Advantage Value:","text":"<p>Measures how much an action is better than the average of other actions in a given state.</p>"},{"location":"course_notes/advanced/#why-use-the-advantage-value","title":"Why Use the Advantage Value?","text":"<p>We can further reduce variance by subtracting a baseline from \\(Q(s_{i,t}, a_{i,t})\\) without altering the expectation of \\(\\nabla_\\theta J(\\theta)\\), making it an unbiased estimator:</p> \\[ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t}) \\bigg( Q(s_{i,t},a_{i,t}) - b_t \\bigg). \\] <p>A reasonable choice for the baseline is the expected reward. Although it is not optimal, it significantly reduces variance.</p> <p>We define:</p> \\[ Q(s_{i,t},a_{i,t}) = \\sum_{t'=t}^T E_{\\pi_\\theta}[r(s_{t'}, a_{t'})|s_t,a_t]. \\] <p>To ensure the baseline is independent of the action taken, we compute the expectation of \\(Q(s_{i,t}, a_{i,t})\\) over all actions sampled from the policy:</p> \\[ E_{a_t \\sim \\pi_\\theta(a_{i,t}|s_{i,t})} [Q(s_{i,t},a_{i,t})] = V(s_t) = b_t. \\] <p>Thus, the variance-reduced policy gradient equation becomes:</p> \\[ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t}) \\bigg( Q(s_{i,t},a_{i,t}) - V(s_t) \\bigg). \\] <p>We define the advantage function as:</p> \\[ A(s_t,a_t) = Q(s_{i,t},a_{i,t}) - V(s_t). \\] <p>Example of Understanding the Advantage Function</p> <p>Consider a penalty shootout game to illustrate the concept of the advantage function and Q-values in reinforcement learning.</p> <p> </p> <ul> <li>Game Setup:<ol> <li>Goalie Strategy:  A goalie always jumps to the right to block the shot.</li> <li>Kicker Strategy: A kicker can shoot either left or right with equal probability (0.5 each), defining the kicker's policy \\(\\pi_k\\).</li> </ol> </li> </ul> <p>The reward matrix for the game is:</p> Kicker / Goalie Right (jumps right) Left (jumps left) Right (shoots right) 0,1 1,0 Left (shoots left) 1,0 0,1 <ul> <li> <p>Expected Reward:</p> <p>Since the kicker selects left and right with equal probability, the expected reward is:</p> \\[ V^{\\pi_k}(s_t) = 0.5 \\times 1 + 0.5 \\times 0 = 0.5. \\] </li> <li> <p>Q-Value Calculation:     The Q-value is expressed as:</p> \\[ Q^{\\pi_k}(s_{i,t},a_{i,t}) = V^{\\pi_k}(s_t) + A^{\\pi_k}(s_t,a_t). \\] <ul> <li>If the kicker shoots right, the shot is always saved (\\(Q^{\\pi_k}(s_{i,t},r) = 0\\)).</li> <li>If the kicker shoots left, the shot is always successful (\\(Q^{\\pi_k}(s_{i,t},l) = 1\\)).</li> </ul> </li> <li> <p>Advantage Calculation:</p> <p>The advantage function \\(A^{\\pi_k}(s_t,a_t)\\) measures how much better or worse an action is compared to the expected reward.</p> <ul> <li>If the kicker shoots left, he scores (reward = 1), which is 0.5 more than the expected reward \\(V^{\\pi_k}(s_t)\\). Thus, the advantage of shooting left is:</li> </ul> \\[ 1 = 0.5 + A^{\\pi_k}(s_t,l) \\Rightarrow A^{\\pi_k}(s_t,l) = 0.5. \\] <ul> <li>If the kicker shoots right, he fails (reward = 0), which is 0.5 less than the expected reward. Thus, the advantage of shooting right is:</li> </ul> \\[ 0 = 0.5 + A^{\\pi_k}(s_t,r) \\Rightarrow A^{\\pi_k}(s_t,r) = -0.5. \\] </li> </ul>"},{"location":"course_notes/advanced/#estimating-the-advantage-value","title":"Estimating the Advantage Value","text":"<p>Instead of maintaining separate networks for estimating \\(V(s_t)\\) and \\(Q(s_{i,t}, a_{i,t})\\), we approximate \\(Q(s_{i,t}, a_{i,t})\\) using \\(V(s_t)\\):</p> \\[ Q(s_{i,t},a_{i,t}) = r(s_t, a_t) + \\sum_{t'=t+1}^T E_{\\pi_\\theta}[r(s_{t'}, a_{t'})|s_t,a_t] \\approx r(s_t, a_t) + V(s_{t+1}). \\] <p>Thus, we estimate the advantage function as:</p> \\[ A(s_{i,t},a_{i,t}) \\approx r(s_t, a_t) + V(s_{t+1}) - V(s_t). \\] <p>We can also, consider the advantage function with discount factor as:</p> \\[ A(s_{i,t},a_{i,t}) \\approx r(s_t, a_t) + \\gamma V(s_{t+1}) - V(s_t). \\] <p>To train the value estimator, we use Monte Carlo estimation.</p>"},{"location":"course_notes/advanced/#generalized-advantage-estimation-gae","title":"Generalized Advantage Estimation (GAE)","text":"<p>To have a good  balance between variance and bias, we can use the concept of GAE, which is firstly introduced in High-Dimensional Continuous Control Using Generalized Advantage Estimation. </p> <p>At the first, we define \\(\\hat{A}^{(k)}(s_{i,t},a_{i,t})\\) to understand this the GAE concept.</p> \\[ \\hat{A}^{(k)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \\dots + \\gamma^{k-1}r(s_{t+k-1}, a_{t+k-1}) + \\gamma^k V(s_{t+k})- V(s_t). \\] <p>So, we can write the \\(\\hat{A}^{(k)}(s_{i,t},a_{i,t})\\) for \\(k \\in \\{1, \\infty\\}\\) as:</p> \\[ \\hat{A}^{(1)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \\gamma V(s_{t+1}) - V(s_t) \\] \\[ \\hat{A}^{(2)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \\gamma r(s_{t+1}, a_{t+1}) + \\gamma^2 V(s_{t+2}) - V(s_t) \\] \\[ .\\\\ .\\\\ .\\\\ \\] \\[ \\hat{A}^{(\\infty)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \\gamma r(s_{t+1}, a_{t+1}) + \\gamma^2 r(s_{t+2}, a_{t+2})+ \\dots - V(s_t)\\\\ \\] <p>\\(\\hat{A}^{(1)}(s_{i,t},a_{i,t})\\) is high bias, low variance, whilst \\(\\hat{A}^{(\\infty)}(s_{i,t},a_{i,t})\\) is unbiased, high variance.</p> <p>We take a weighted average of all \\(\\hat{A}^{(k)}(s_{i,t},a_{i,t})\\) for \\(k \\in \\{1, \\infty\\}\\) with weight \\(w_k = \\lambda^{k-1}\\) to balance bias and variance. This is called Generalized Advantage Estimation (GAE). </p> \\[ \\hat{A}^{(GAE)}(s_{i,t},a_{i,t}) = \\frac{\\sum_{k =1}^T  w_k \\hat{A}^{(k)}(s_{i,t},a_{i,t})}{\\sum_k w_k}= \\frac{\\sum_{k =1}^T \\lambda^{k-1} \\hat{A}^{(k)}(s_{i,t},a_{i,t})}{\\sum_k w_k} \\]"},{"location":"course_notes/advanced/#actor-critic-algorihtms","title":"Actor-Critic Algorihtms","text":""},{"location":"course_notes/advanced/#batch-actor-critic-algorithm","title":"Batch actor-critic algorithm","text":"<p>The first algorithm is Actor-Critic with Bootstrapping and Baseline Subtraction. In this algorithm, the simulator runs for an entire episode before updating the policy.</p> <p>Batch actor-critic algorithm:</p> <ol> <li>for each episode do:</li> <li> for each step do:</li> <li>\u2003\u2003Take action \\(a_t \\sim \\pi_{\\theta}(a_t | s_t)\\), get \\((s_t,a_t,s'_t,r_t)\\).</li> <li>\u2003Fit \\(\\hat{V}(s_t)\\) with sampled rewards.</li> <li>\u2003Evaluate the advantage function: \\(A({s_t, a_t})\\)</li> <li>\u2003Compute the policy gradient: \\(\\nabla_{\\theta} J(\\theta) \\approx \\sum_{i} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_i | s_i) A({s_t})\\)</li> <li>\u2003Update the policy parameters:  \\(\\theta \\gets \\theta + \\alpha \\nabla_{\\theta} J(\\theta)\\)</li> </ol> <p>Running full episodes for a single update is inefficient as it requires a significant amount of time. To address this issue, the online actor-critic algorithm is proposed.</p>"},{"location":"course_notes/advanced/#online-actor-critic-algorithm","title":"Online actor-critic algorithm","text":"<p>In this algorithm, we take an action in the environment and immediately apply an update using that action.</p> <p>Online actor-critic algorithm</p> <ol> <li>for each episode do:</li> <li> for each step do:</li> <li>\u2003\u2003Take action \\(a_t \\sim \\pi_{\\theta}(a_t | s_t)\\), get \\((s_t,a_t,s'_t,r_t)\\).</li> <li>\u2003\u2003Fit \\(\\hat{V}(s_t)\\) with the sampled reward.</li> <li>\u2003\u2003Evaluate the advantage function: \\(A({s,a})\\)</li> <li>\u2003\u2003Compute the policy gradient: \\(\\nabla_{\\theta} J(\\theta) \\approx  \\nabla_{\\theta} \\log \\pi_{\\theta}(a | s) A({s,a})\\)</li> <li>\u2003\u2003Update the policy parameters: \\(\\theta \\gets \\theta + \\alpha \\nabla_{\\theta} J(\\theta)\\)</li> </ol> <p>Training neural networks with a batch size of 1 leads to high variance, making the training process unstable.</p> <p>To mitigate this issue, two main solutions are commonly used:</p> <ol> <li>Parallel Actor-Critic (Online)</li> <li>Off-Policy Actor-Critic</li> </ol>"},{"location":"course_notes/advanced/#parallel-actor-critic-online","title":"Parallel Actor-Critic (Online)","text":"<p>Many high-performance implementations are based on the actor critic approach. For large problems, the algorithm is typically parallelized and implemented on a large cluster computer.</p> <p>To reduce variance, multiple actors are used to update the policy. There are two main approaches:</p> <ul> <li>Synchronized Parallel Actor-Critic: All actors run synchronously, and updates are applied simultaneously. However, this introduces synchronization overhead, making it impractical in many cases.</li> <li>Asynchronous Parallel Actor-Critic: Each actor applies its updates independently, reducing synchronization constraints and improving computational efficiency. It also, uses asynchronous (parallel and distributed) gradient descent for optimization of deep neural network controllers.</li> </ul> Resources &amp; Links <p>Asynchronous Methods for Deep Reinforcement Learning</p> <p>Actor-Critic Methods: A3C and A2C</p> <p>The idea behind Actor-Critics and how A2C and A3C improve them</p>"},{"location":"course_notes/advanced/#off-policy-actor-critic-algorithm","title":"Off-Policy Actor-Critic Algorithm","text":"<p>In the off-policy approach, we maintain a replay buffer to store past experiences, allowing us to train the model using previously collected data rather than relying solely on the most recent experience.</p> <p>Off-policy actor-critic algorithm:</p> <ol> <li>for each episode do:</li> <li> for multiple steps do:</li> <li>\u2003\u2003Take action \\(a \\sim \\pi_{\\theta}(a | s)\\), get \\((s,a,s',r)\\), store in \\(\\mathcal{R}\\).</li> <li>\u2003Sample a batch \\(\\{s_i, a_i, r_i, s'_i \\}\\) for buffer \\(\\mathcal{R}\\).</li> <li>\u2003Fit \\(\\hat{Q}^{\\pi}(s_i, a_i)\\) for each \\(s_i, a_i\\).</li> <li>\u2003Compute the policy gradient: \\(\\nabla_{\\theta} J(\\theta) \\approx \\frac{1}{N} \\sum_{i} \\nabla_{\\theta} \\log \\pi_{\\theta}(a^{\\pi}_i | s_i) \\hat{Q}^{\\pi}(s_i, a^{\\pi}_i)\\)</li> <li>\u2003Update the policy parameters: \\(\\theta \\gets \\theta + \\alpha \\nabla_{\\theta} J(\\theta)\\)</li> </ol> <p>To work with off-policy methods, we use the Q-value instead of the V-value in step 3. In step 4, rather than using the advantage function, we directly use \\(\\hat{Q}^{\\pi}(s_i, a^{\\pi}_i)\\), where \\(a^{\\pi}_i\\)  is sampled from the policy \\(\\pi\\). By using the Q-value instead of the advantage function, we do not encounter the high-variance problem typically associated with single-step updates. This is because we sample a batch from the replay buffer, which inherently reduces variance. As a result, there is no need to compute an explicit advantage function for variance reduction.</p>"},{"location":"course_notes/advanced/#issues-with-standard-policy-gradient-methods","title":"Issues with Standard Policy Gradient Methods","text":"<p>Earlier policy gradient methods, such as Vanilla Policy Gradient (VPG) or REINFORCE, suffer from high variance and instability in training. A key problem is that large updates to the policy can lead to drastic performance degradation.</p> <p>To address these issues, Trust Region Policy Optimization (TRPO) was introduced, enforcing a constraint on how much the policy can change in a single update. However, TRPO is computationally expensive because it requires solving a constrained optimization problem. PPO is a simpler and more efficient alternative to TRPO, designed to ensure stable policy updates without requiring complex constraints.</p>"},{"location":"course_notes/advanced/#proximal-policy-optimization-ppo","title":"Proximal Policy Optimization (PPO)","text":""},{"location":"course_notes/advanced/#how-ppo-enhances-on-policy-actor-critic-methods","title":"How PPO Enhances On-Policy Actor-Critic Methods","text":"<p>PPO (Proximal Policy Optimization) is an on-policy actor-critic algorithm. It combines a policy network (the actor) and a value network (the critic) and employs a clipped surrogate objective to restrict excessive policy updates, thereby promoting training stability.</p> <p>PPO addresses several problems inherent in on-policy actor-critic methods in the following ways:</p> <ul> <li> <p>Stabilizing Policy Updates:   PPO introduces a clipping mechanism in its objective function that constrains the policy update by ensuring the new policy doesn't deviate too far from the old policy.This clipping prevents overly large updates, thereby stabilizing the learning process and reducing sensitivity.</p> </li> <li> <p>Improving Sample Efficiency:   Although PPO is an on-policy algorithm, it reuses a fixed batch of data for multiple epochs of mini-batch updates. This means that each set of interactions with the environment can contribute more to learning, mitigating the need for excessive sampling.</p> </li> <li> <p>Reducing Variance in Gradient Estimates:   By incorporating advanced advantage estimation techniques (e.g., Generalized Advantage Estimation), PPO reduces the high variance typically associated with policy gradients, leading to more reliable and stable updates.</p> </li> <li> <p>Implicitly Maintaining a Trust Region:   The clipping mechanism acts like an implicit trust region constraint. It ensures that updates remain within a safe boundary around the current policy, which helps in achieving monotonic improvements and prevents performance collapse.</p> </li> </ul>"},{"location":"course_notes/advanced/#the-intuition-behind-ppo","title":"The intuition behind PPO","text":"<p>The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability of the policy by limiting the change you make to the policy at each training epoch: we want to avoid having too large policy updates.  why?</p> <ol> <li>We know empirically that smaller policy updates during training are more likely to converge to an optimal solution.</li> <li>If we change the policy too much, we may end up with a bad policy that cannot be improved.</li> </ol> <p>soruce: Unit 8, of the Deep Reinforcement Learning Class with Hugging Face</p> <p>Therefore, in order not to allow the current policy to change much compared to the previous policy, we limit the ratio of these two policies to  \\([1 - \\epsilon, 1 + \\epsilon]\\).</p>"},{"location":"course_notes/advanced/#the-clipped-surrogate-objective","title":"the Clipped Surrogate Objective","text":"\\[ L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip} \\left( r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_t \\right) \\right] \\]"},{"location":"course_notes/advanced/#the-ratio-function","title":"The ratio Function","text":"\\[ r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\] <p>\\(r_{\\theta}\\) denotes the probability ratio between the current and old policy. if \\(r_{\\theta} &gt; 1\\), then the probability of doing action \\(a_t\\) at \\(s_t\\) in current policy is higher than the old policy and vice versa.</p> <p>So this probability ratio is an easy way to estimate the divergence between old and current policy.</p>"},{"location":"course_notes/advanced/#the-clipped-part","title":"The clipped part","text":"\\[ \\text{clip} \\left( r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_t \\] <p>If the current policy is updated significantly, such that the new policy parameters \\(\\theta'\\)  diverge greatly from the previous ones, the probability ratio between the new and old policies is clipped to the bounds  \\(1 - \\epsilon\\), \\(1 + \\epsilon\\). At this point, the derivative of the objective function becomes zero, effectively preventing further updates. </p>"},{"location":"course_notes/advanced/#the-unclipped-part","title":"The unclipped part","text":"\\[ r_t(\\theta) \\hat{A}_t \\] <p>In the context of optimization, if the initial starting point is not ideal\u2014i.e., if the probability ratio between the new and old policies is outside the range of \\(1 - \\epsilon\\) and \\(1 + \\epsilon\\)\u2014the ratio is clipped to these bounds. This clipping results in the derivative of the objective function becoming zero, meaning no gradient is available for updates. </p> <p>In this formulation, the optimization is performed with respect to the new policy parameters \\(\\theta'\\), and \\(A\\) represents the advantage function, which indicates how much better or worse the action performed is compared to the average return.</p> <p>Example of PPO objective function</p> <ul> <li>Case 1: Positive Advantage (\\(A &gt; 0\\))</li> </ul> <p>if the Advantage \\(A\\) is positive (indicating that the action taken has a higher return than the expected return), and \\(\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} &lt; 1-\\epsilon\\) , the unclipped part is less than the clipped part and then it is minimized, so we have gradient to update the policy. This allows the policy to increase the probability of the action, aiming for the ratio to reach \\(1 + \\epsilon\\) without violating the clipping constraint.</p> <ul> <li>Case 2: Negative Advantage (\\(A &lt; 0\\))</li> </ul> <p>On the other hand, if the Advantage \\(A\\) is negative (meaning the action taken is worse than the average return), and \\(\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} &gt; 1+\\epsilon\\), the unclipped objective is again minimized and the gradient is non-zero, leading to an update. In this case, since the Advantage is negative, the policy is adjusted to reduce the probability of selecting that action, bringing the ratio closer to the boundary (\\(1-\\epsilon\\)), while ensuring that the new policy does not deviate too much from the old one.</p>"},{"location":"course_notes/advanced/#visualize-the-clipped-surrogate-objective","title":"Visualize the Clipped Surrogate Objective","text":"<p>Algorithm: PPO-Clip </p> <ol> <li>Input: initial policy parameters \\(\\theta_0\\), initial value function parameters \\(\\phi_0\\) </li> <li>for \\(k = 0, 1, 2, \\dots\\) do </li> <li>\u2003 Collect set of trajectories \\(\\mathcal{D}_k = \\{\\tau_i\\}\\) by running policy \\(\\pi_k = \\pi(\\theta_k)\\) in the environment.  </li> <li>\u2003 Compute rewards-to-go \\(\\hat{R}_t\\).  </li> <li>\u2003 Compute advantage estimates, \\(\\hat{A}_t\\) (using any method of advantage estimation) based on the current value function \\(V_{\\phi_k}\\).  </li> <li> <p>\u2003 Update the policy by maximizing the PPO-Clip objective:  </p> \\[ \\theta_{k+1} = \\arg \\max_{\\theta} \\frac{1}{|\\mathcal{D}_k| T} \\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^{T} \\min \\left( \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_k}(a_t | s_t)} A^{\\pi_{\\theta_k}}(s_t, a_t), \\, g(\\epsilon, A^{\\pi_{\\theta_k}}(s_t, a_t)) \\right) \\] <p>typically via stochastic gradient ascent with Adam.  </p> </li> <li> <p>\u2003 Fit value function by regression on mean-squared error:  </p> \\[ \\phi_{k+1} = \\arg \\min_{\\phi} \\frac{1}{|\\mathcal{D}_k| T} \\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^{T} \\left( V_{\\phi}(s_t) - \\hat{R}_t \\right)^2 \\] </li> <li> <p>end for</p> </li> </ol>"},{"location":"course_notes/advanced/#challenges-of-ppo-algorithm","title":"Challenges of PPO algorithm","text":"<p>PPO requires a significant amount of interactions with the environment to converge. This can be problematic in real-world applications where data is expensive or difficult to collect. In fact it is a sample inefficient algorithm.</p> <ol> <li> <p>Hyperparameter Sensitivity:   PPO requires careful tuning of hyperparameters such as the clipping parameter (epsilon), learning rate, and the number of updates per iteration. Poorly chosen hyperparameters can lead to suboptimal performance or even failure to converge.</p> </li> <li> <p>Sample Efficiency: Although PPO is more sample-efficient than some other policy gradient methods, it still requires a significant amount of data to achieve good performance. This can be problematic in environments where data collection is expensive or time-consuming.</p> </li> <li> <p>Exploration-Exploitation Tradeoff: PPO, like other policy gradient methods, can struggle with balancing exploration and exploitation. It may prematurely converge to suboptimal policies if it fails to explore sufficiently.</p> </li> </ol> Helpful links <p>Unit 8, of the Deep Reinforcement Learning Class with Hugging Face</p> <p>Proximal Policy Optimization (PPO) Explained</p> <p>Proximal Policy Optimization (PPO) - How to train Large Language Models</p> <p>Proximal Policy Optimization</p> <p>Understanding PPO: A Game-Changer in AI Decision-Making Explained for RL Newcomers</p> <p>Proximal Policy Optimization (PPO) - Explained</p>"},{"location":"course_notes/advanced/#deep-deterministic-policy-gradients-ddpg","title":"Deep Deterministic Policy Gradients (DDPG)","text":""},{"location":"course_notes/advanced/#handling-continuous-action-spaces","title":"Handling Continuous Action Spaces","text":""},{"location":"course_notes/advanced/#why-is-this-a-problem","title":"Why is this a problem?","text":"<p>In discrete action spaces, methods like DQN (Deep Q-Networks) can use an action-value function \\(Q(s, a)\\) to select the best action. However, in continuous action spaces, selecting the optimal action requires solving a high-dimensional optimization problem at every step, which is computationally expensive.</p>"},{"location":"course_notes/advanced/#how-does-ddpg-solve-it","title":"How does DDPG solve it?","text":"<p>DDPG uses a deterministic policy network \\(\\pi(s)\\), which directly maps states to actions, eliminating the need for iterative optimization over action values.</p>"},{"location":"course_notes/advanced/#ddpg-architecture","title":"DDPG Architecture","text":"<p>DDPG uses four neural networks:</p> <ul> <li>A Q network </li> <li>A deterministic policy network </li> <li>A target Q network </li> <li>A target policy network </li> </ul> <p>The Q network and policy network are similar to Advantage Actor-Critic (A2C), but in DDPG, the Actor directly maps states to actions (the output of the network directly represents the action) instead of outputting a probability distribution over a discrete action space.</p> <p>The target networks are time-delayed copies of their original networks that slowly track the learned networks. Using these target value networks greatly improves stability in learning.  </p>"},{"location":"course_notes/advanced/#why-use-target-networks","title":"Why Use Target Networks?","text":"<p>In methods without target networks, the update equations of the network depend on the network's own calculated values, making it prone to divergence.  </p> <p>For example, if we update the Q-values directly using the current network, errors can compound, leading to instability. The target networks help mitigate this issue by providing more stable targets for updates. </p> \\[     Q(s_t, a_t) \\leftarrow r_t + \\gamma Q(s_{t+1}, \\arg\\max_{a'} Q(s_{t+1}, a')) \\]"},{"location":"course_notes/advanced/#breakdown-of-ddpg-components","title":"Breakdown of DDPG Components","text":"<ol> <li>Experience Replay </li> <li>Actor &amp; Critic Network Updates </li> <li>Target Network Updates </li> <li>Exploration </li> </ol>"},{"location":"course_notes/advanced/#replay-buffer","title":"Replay Buffer","text":"<p>As used in Deep Q-Learning and other RL algorithms, DDPG also utilizes a replay buffer to store experience tuples:  </p> \\[ (state, action, reward, next\\_state) \\] <p>These tuples are stored in a finite-sized cache (replay buffer). During training, random mini-batches are sampled from this buffer to update the value and policy networks.</p> Why Use Experience Replay? <p>In optimization tasks, we want data to be independently distributed. However, in an on-policy learning process, the collected data is highly correlated.  </p> <p>By storing experience in a replay buffer and sampling random mini-batches for training, we break correlations and improve learning stability.</p>"},{"location":"course_notes/advanced/#actor-policy-critic-value-network-updates","title":"Actor (Policy) &amp; Critic (Value) Network Updates","text":"<p>The value network is updated similarly to Q-learning. The updated Q-value is obtained using the Bellman equation:</p> \\[ y_i = r_i + \\gamma Q' \\left (s_{i+1}, \\mu' (s_{i+1}|\\theta^{\\mu'})|\\theta^{Q'} \\right) \\] <p>However, in DDPG, the next-state Q-values are calculated using the target Q network and target policy network.  </p> <p>Then, we minimize the mean squared error (MSE) loss between the updated Q-value and the original Q-value:</p> \\[ \\mathcal{L} = \\frac{1}{N} \\sum_{i} \\left(y_i -Q(s_i, a_i|\\theta^Q) \\right)^2 \\] <ul> <li>Note: The original Q-value is calculated using the learned Q-network, not the target Q-network.</li> </ul> <p>The policy function aims to maximize the expected return:</p> \\[ J(\\theta) = \\mathbb{E} \\left[ Q(s, a) \\mid s = s_t, a_t = \\mu(s_t) \\right] \\] <p>The policy loss is computed by differentiating the objective function with respect to the policy parameters:</p> \\[ \\nabla_{\\theta^\\mu} J(\\theta) = \\nabla_a Q(s, a) |_{a = \\mu(s)} \\nabla_{\\theta^\\mu} \\mu(s|\\theta^\\mu) \\] <p>But since we are updating the policy in an off-policy way with batches of experience, we take the mean of the sum of gradients calculated from the mini-batch:</p> \\[ \\nabla_{\\theta_\\mu} J(\\theta) \\approx \\frac{1}{N} \\sum_i \\left[  \\left. \\nabla_a Q(s, a \\mid \\theta^Q) \\right|_{s = s_i, a = \\mu(s_i)}  \\nabla_{\\theta_\\mu} \\mu(s \\mid \\theta^\\mu) \\Big|_{s = s_i}  \\right] \\]"},{"location":"course_notes/advanced/#target-network-updates","title":"Target Network Updates","text":"<p>The target networks are updated via soft updates instead of direct copying:</p> \\[ \\theta^{Q'} \\leftarrow \\tau \\theta^{Q} + (1 - \\tau) \\theta^{Q'} \\] \\[ \\theta^{\\mu'} \\leftarrow \\tau \\theta^{\\mu} + (1 - \\tau) \\theta^{\\mu'} \\] <p>where \\(\\tau\\) is a small value , ensuring smooth updates that prevent instability.</p>"},{"location":"course_notes/advanced/#exploration","title":"Exploration","text":"<p>In RL for discrete action spaces, exploration is often done using epsilon-greedy or Boltzmann exploration.  </p> <p>However, in continuous action spaces, exploration is done by adding noise to the action itself.  </p> <ul> <li>Ornstein-Uhlenbeck Process The DDPG paper proposes adding Ornstein-Uhlenbeck (OU) noise to the actions.  </li> </ul> <p>The OU Process generates temporally correlated noise, preventing the noise from canceling out or \"freezing\" the action dynamics.</p> \\[ \\mu^{'}(s_t) = \\mu(s_t|\\theta^\\mu_t) + \\mathcal{N} \\] Diagram Of DDPG Algorithms <p> </p>"},{"location":"course_notes/advanced/#algorithm-ddpg-algorithm","title":"Algorithm: DDPG Algorithm","text":"<p>Randomly initialize critic network \\( Q(s, a | \\theta^Q) \\) and actor \\( \\mu(s | \\theta^\\mu) \\) with weights \\( \\theta^Q \\) and \\( \\theta^\\mu \\). Initialize target networks \\( Q' \\) and \\( \\mu' \\) with weights \\( \\theta^{Q'} \\gets \\theta^Q, \\quad \\theta^{\\mu'} \\gets \\theta^\\mu \\). Initialize replay buffer \\( R \\).  </p> <p>for episode = 1 to \\( M \\) do \u00a0\u00a0Initialize a random process \\( \\mathcal{N} \\) for action exploration. \u00a0\u00a0Receive initial observation state \\( s_1 \\). for \\( t = 1 \\) to \\( T \\) do \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Select action \\( a_t = \\mu(s_t | \\theta^\\mu) + \\mathcal{N}_t \\) according to the current policy and exploration noise. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Execute action \\( a_t \\) and observe reward \\( r_t \\) and new state \\( s_{t+1} \\). \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Store transition \\( (s_t, a_t, r_t, s_{t+1}) \\) in \\( R \\). \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Sample a random minibatch of \\( N \\) transitions \\( (s_i, a_i, r_i, s_{i+1}) \\) from \\( R \\). \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Set:  </p> \\[   y_i = r_i + \\gamma Q'(s_{i+1}, \\mu'(s_{i+1} | \\theta^{\\mu'}) | \\theta^{Q'})   \\] <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Update critic by minimizing the loss:  </p> \\[   L = \\frac{1}{N} \\sum_i (y_i - Q(s_i, a_i | \\theta^Q))^2   \\] <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Update actor policy using the sampled policy gradient: </p> \\[   \\nabla_{\\theta^\\mu} J \\approx \\frac{1}{N} \\sum_i \\nabla_a Q(s, a | \\theta^Q) |_{s=s_i, a=\\mu(s_i)} \\nabla_{\\theta^\\mu} \\mu(s | \\theta^\\mu) |_{s_i}   \\] <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Update the target networks:  </p> \\[   \\theta^{Q'} \\gets \\tau \\theta^Q + (1 - \\tau) \\theta^{Q'}   \\] \\[   \\theta^{\\mu'} \\gets \\tau \\theta^\\mu + (1 - \\tau) \\theta^{\\mu'}   \\] <p> end for end for </p>"},{"location":"course_notes/advanced/#soft-actor-critic-sac","title":"Soft Actor-Critic (SAC)","text":""},{"location":"course_notes/advanced/#challenges-and-motivation-of-sac","title":"Challenges and motivation of SAC","text":"<ol> <li>Previous Off-policy methods like DDPG often struggle with exploration , leading to suboptimal policies. SAC overcomes this by introducing entropy maximization, which encourages the agent to explore more efficiently.</li> <li>Sample inefficiency is a major issue in on-policy algorithms like Proximal Policy Optimization (PPO), which require a large number of interactions with the environment. SAC, being an off-policy algorithm, reuses past experiences stored in a replay buffer, making it significantly more sample-efficient.</li> <li>Another challenge is instability in learning, as methods like DDPG can suffer from overestimation of Q-values. SAC mitigates this by employing twin Q-functions (similar to TD3) and incorporating entropy regularization, leading to more stable and robust learning.</li> </ol> <p>In essence, SAC seeks to maximize the entropy in policy, in addition to the expected reward from the environment. The entropy in policy can be interpreted as randomness in the policy.</p> what is entropy? <p>We can think of entropy as how unpredictable a random variable is. If a random variable always takes a single value then it has zero entropy because it\u2019s not unpredictable at all. If a random variable can be any Real Number with equal probability then it has very high entropy as it is very unpredictable.  </p> <p>probability distributions with low entropy have a tendency to greedily sample certain values, as the probability mass is distributed relatively unevenly.</p>"},{"location":"course_notes/advanced/#maximum-entropy-reinforcement-learning","title":"Maximum Entropy Reinforcement Learning","text":"<p>In Maximum Entropy RL, the agent tries to optimize the policy to choose the right action that can receive the highest sum of reward and long term sum of entropy. This enables the agent to explore more and avoid converging to local optima.</p> <p>reason</p> <p>We want a high entropy in our policy to encourage the policy to assign equal probabilities to actions that have same or nearly equal Q-values(allow the policy to capture multiple modes of good policies), and also to ensure that it does not collapse into repeatedly selecting a particular action that could exploit some inconsistency in the approximated Q function. Therefore, SAC overcomes the  problem by encouraging the policy network to explore and not assign a very high probability to any one part of the range of actions.</p> <p>The objective function of the Maximum entropy RL is as shown below:</p> \\[ J(\\pi_{\\theta}) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) + \\alpha H(\\pi(\\cdot | s_t)) \\right] \\] <p>and the optimal policy is:</p> \\[ \\pi^* = \\arg\\max_{\\pi_{\\theta}} \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) + \\alpha H(\\pi(\\cdot | s_t)) \\right] \\] <p>\\(\\alpha\\) is the temperature parameter that balances between exploration and exploitation.</p>"},{"location":"course_notes/advanced/#overcoming-exploration-bias-in-multimodal-q-functions","title":"Overcoming Exploration Bias in Multimodal Q-Functions","text":"<p>Here we want to explain the concept of a multimodal Q-function in reinforcement learning (RL), where the Q-value function, \\(Q(s, a)\\), represents the expected cumulative reward for taking action \\(a\\) in state \\(s\\). </p> <p>In this context, the robot is in an initial state and has two possible passages to follow, which result in a bimodal Q-function (a function with two peaks). These peaks correspond to two different action choices, each leading to different potential outcomes.</p>"},{"location":"course_notes/advanced/#standard-rl-approach","title":"Standard RL Approach","text":"<p>The grey curve represents the Q-function, which has two peaks, indicating two promising actions.A conventional RL approach typically assumes a unimodal (single-peaked) policy distribution, represented by the red curve.</p> <p>This policy distribution is modeled as a Gaussian \\(\\mathcal{N}(\\mu(s_t), \\Sigma)\\) centered around the highest Q-value peak.</p> <p>This setup results in exploration bias where the agent primarily explores around the highest peak and ignores the lower peak entirely.</p>"},{"location":"course_notes/advanced/#improved-exploration-strategy","title":"Improved Exploration Strategy","text":"<p>Instead of using a unimodal Gaussian policy, a Boltzmann-weighted policy is used. The policy distribution (green shaded area) is proportional to \\(\\exp(Q(s_t, a_t))\\), meaning actions are sampled based on their Q-values. </p> <p>This approach allows the agent to explore multiple high-reward actions and avoids the bias of ignoring one passage. As a result, the agent recognizes both options, increasing the chance of finding the optimal path.</p> <p>This concept is relevant for actor-critic RL methods like Soft Actor-Critic (SAC), which uses entropy to encourage diverse exploration.</p>"},{"location":"course_notes/advanced/#soft-policy","title":"Soft Policy","text":"<ul> <li>Soft policy </li> </ul> \\[ J(\\pi) = \\sum_{t=0}^{T} \\mathbb{E}_{(s_t, a_t)\\sim\\rho_{\\pi}} \\left[ r(s_t, a_t) + \\alpha\\mathcal{H}(\\pi(\\cdot | s_t)) \\right] \\] <p>With new objective function we need to define Value funciton and Q-value funciton again. </p> <ul> <li>Soft Q-value funciton</li> </ul> \\[ Q(s_t, a_t) = r(s_t, a_t) + \\gamma\\mathbb{E}_{s_{t+1}\\sim p}\\left[ V(s_{t+1}) \\right] \\] <ul> <li>Soft Value function</li> </ul> \\[ V(s_t) = \\mathbb{E}_{a_t\\sim \\pi} \\left[Q(s_t, a_t) - \\text{log}\\space\\pi(a_t|s_t)\\right] \\]"},{"location":"course_notes/advanced/#soft-policy-iteration","title":"Soft Policy Iteration","text":"<p>Soft Policy Iteration is an entropy-regularized version of classical policy iteration, which consists of:</p> <ol> <li>Soft Policy Evaluation: Estimating the soft Q-value function under the current policy.</li> <li>Soft Policy Improvement:  Updating the policy to maximize the soft Q-value function,incorporating entropy regularization.</li> </ol> <p>This process iteratively improves the policy while balancing exploration and exploitation.</p>"},{"location":"course_notes/advanced/#soft-policy-evaluation-critic-update","title":"Soft Policy Evaluation (Critic Update)","text":"<p>The goal of soft policy evaluation is to compute the expected return of a given policy \\(\\pi\\) under the maximum entropy objective, which modifies the standard Bellman equation by adding an entropy term. (SAC explicitly learns the Q-function for the current policy)</p> <p>The soft Q-value function for a policy \\(\\pi\\) is updated using a modified Bellman operator \\(T^{\\pi}\\):</p> \\[ T^\\pi Q(s_t, a_t) = r(s_t, a_t) + \\gamma \\mathbb{E}_{s_{t+1} \\sim p} [V(s_{t+1})] \\] <p>with substitution of \\(V\\) we have :</p> \\[ T^\\pi Q(s_t, a_t) = r(s_t, a_t) + \\gamma \\mathbb{E}_{\\substack{s_{t+1} \\sim p \\\\ a_{t+1} \\sim \\pi}} [Q(s_{t+1,}, a_{t+1}) - \\text{log}\\space\\pi(a_{t+1}|s_{t+1})] \\] <p>Key Result: Soft Bellman Backup Convergence </p> <p>Theorem: By repeatedly applying the operator \\(T^\\pi\\), the Q-value function converges to the true soft Q-value function for policy \\(\\pi\\):  </p> \\[ Q_k \\to Q^\\pi \\text{ as } k \\to \\infty \\] <p>Thus, we can estimate \\(Q^\\pi\\) iteratively.</p>"},{"location":"course_notes/advanced/#soft-policy-improvement-actor-update","title":"Soft Policy Improvement (Actor Update)","text":"<p>Once the Q-function is learned, we need to improve the policy using a gradient-based update. This means:</p> <ul> <li>Instead of directly maximizing Q-values, the policy is updated to optimize a modified objective that balances reward maximization and exploration.</li> <li>The update is off-policy, meaning the policy can be trained using past experiences stored in a replay buffer, rather than requiring fresh samples like on-policy methods (e.g., PPO).</li> </ul> <p>The update is based on an exponential function of the Q-values:</p> \\[ \\pi^*(a | s) \\propto \\exp (Q^\\pi(s, a)) \\] <p>which means the optimal policy is obtained by normalizing \\(\\exp (Q^\\pi(s, a))\\) over all actions:</p> \\[ \\pi^*(a | s) = \\frac{\\exp (Q^\\pi(s, a))}{Z^\\pi(s)} \\] <p>where \\(Z^\\pi(s)\\) is the partition function that ensures the distribution sums to 1.</p> <p>For the policy improvement step, we update the policy distribution towards the softmax distribution for the current Q function.</p> \\[ \\pi_{\\text{new}} = \\arg \\min_{\\pi' \\in \\Pi} D_{\\text{KL}} \\left( \\pi'(\\cdot | s) \\, \\bigg|\\bigg| \\, \\frac{\\exp (Q^\\pi(s, \\cdot))}{Z^\\pi(s)} \\right) \\] <p>Key Result: Soft Policy Improvement Theorem The new policy \\(\\pi_{\\text{new}}\\) obtained via this update improves the expected soft return:</p> \\[ Q^{\\pi_{\\text{new}}}(s, a) \\geq Q^{\\pi_{\\text{old}}}(s, a) \\quad \\forall (s, a) \\] <p>Thus, iterating this process leads to a better policy.</p>"},{"location":"course_notes/advanced/#convergence-of-soft-policy-iteration","title":"Convergence of Soft Policy Iteration","text":"<p>By alternating between soft policy evaluation and soft policy improvement, soft policy iteration converges to an optimal maximum entropy policy within the policy class \\(\\Pi\\):</p> \\[ \\pi^* = \\arg \\max_{\\pi \\in \\Pi} \\sum_t \\mathbb{E}[r_t + \\alpha H(\\pi(\\cdot | s_t))] \\] <p>However, this exact method is only feasible in the tabular setting. For continuous control, we approximate it using function approximators.</p>"},{"location":"course_notes/advanced/#soft-actor-critic","title":"Soft Actor-Critic","text":"<p>For complex learning domains with high-dimensional and/or continuous state-action spaces, it is mostly impossible to find exact solutions for the MDP. Thus, we must leverage function approximation (i.e. neural networks) to find a practical approximation to soft policy iteration. then we use stochastic gradient descent (SGD) to update parameters of these networks.</p> <p>we model the value functions as expressive neural networks, and the policy as a Gaussian distribution over the action space with the mean and covariance given as neural network outputs with the current state as input.</p>"},{"location":"course_notes/advanced/#soft-value-function-v_psis","title":"Soft Value function (\\(V_{\\psi}(s)\\))","text":"<p>A separate soft value function which helps in stabilising the training process. The soft value function approximator minimizes the squared residual error as follows:</p> \\[ J_V(\\psi) = \\mathbb{E}_{s_{t} \\sim \\mathcal{D}} \\left[ \\frac{1}{2} \\left( V_{\\psi}(s_t) - \\mathbb{E}_{a \\sim \\pi_{\\phi}} [Q_{\\theta}(s_t, a_t) - \\log \\pi_{\\phi}(a_t | s_t)] \\right)^2 \\right] \\] <p>It means the learning of the state-value function \\(V\\), is done by minimizing the squared difference between the prediction of the value network and expected prediction of Q-function with the entropy of the policy, \\(\\pi\\).</p> <ul> <li>\\(D\\) is the distribution of previously sampled states and actions, or a replay buffer.</li> </ul> <p>Gradient Update for \\(V_{\\psi}(s)\\)</p> \\[ \\hat{\\nabla}_{\\psi} J_V(\\psi) = \\nabla_{\\psi} V_{\\psi}(s_t) \\left( V_{\\psi}(s_t) - Q_{\\theta}(s_t, a_t) + \\log \\pi_{\\phi}(a_t | s_t) \\right) \\] <p>where the actions are sampled according to the current policy, instead of the replay buffer.</p>"},{"location":"course_notes/advanced/#soft-q-funciton-q_thetas-a","title":"Soft Q-funciton (\\(Q_{\\theta}(s, a)\\))","text":"<p>We minimize the soft Q-function parameters by using the soft Bellman residual provided here:</p> \\[ J_Q(\\theta) = \\mathbb{E}_{(s_{t}, a_t) \\sim \\mathcal{D}} \\left[ \\frac{1}{2} \\left( Q_{\\theta}(s_t, a_t) - \\hat{Q}(s_t, a_t)\\right)^2 \\right] \\] <p>with : </p> \\[ \\hat{Q}(s_t, a_t) = r(s_t, a_t) + \\gamma \\space \\mathbb{E}_{s_{t+1} \\sim p} [V_{\\bar{\\psi}}(s_{t+1})] \\] <p>Gradient Update for \\(Q_{\\theta}\\):</p> \\[ \\hat{\\nabla}_\\theta J_Q(\\theta) = \\nabla_{\\theta} Q_{\\theta}(s_t, a_t) \\left( Q_{\\theta}(s_t, a_t) - r(s_t, a_t) - \\gamma V_{\\bar{\\psi}}(s_{t+1}) \\right) \\] <p>A target value function \\(V_{\\bar{\\psi}}\\) (exponentially moving average of \\(V_{\\psi}\\)) is used to stabilize training.</p> More Explanation About Target Networks <p>The use of target networks is motivated by a problem in training \\(V\\) network. If you go back to the objective functions in the Theory section, you will find that the target for the \\(Q\\) network training depends on the \\(V\\) Network and the target for the \\(V\\) Network depends on the \\(Q\\) network (this makes sense because we are trying to enforce Bellman Consistency between the two functions). Because of this, the \\(V\\) network has a target that\u2019s indirectly dependent on itself which means that the \\(V\\) network\u2019s target depends on the same parameters we are trying to train. This makes training very unstable.</p> <p>The solution is to use a set of parameters which comes close to the parameters of the main \\(V\\) network, but with a time delay. Thus we create a second network which lags the main network called the target network. There are two ways to go about this.</p> <ol> <li> <p>Periodic Hard Update</p> <p>This method involves completely overwriting the target network\u2019s parameters (\\(\\theta^{-}\\)) with the main network\u2019s parameters (\\(\\theta\\)) at regular intervals (after a fixed number of steps).</p> <ul> <li> <p>Purpose: The periodic hard update ensures the target network aligns closely with the main network, preventing significant divergence between them.</p> </li> <li> <p>Key Characteristics:  </p> <ul> <li>Sudden updates: The target network parameters are replaced entirely at specific intervals.</li> <li>Simplicity: Easy to implement without requiring complex calculations.</li> <li>Stability: Reduces computational overhead during training.</li> </ul> </li> <li> <p>Equation: </p> </li> </ul> \\[ \\theta^{-} \\leftarrow \\theta \\] <ul> <li>Drawback: The abrupt change can lead to instability in learning if the main network's parameters shift significantly during training. It may result in fluctuations in performance for some environments.</li> </ul> </li> <li> <p>Soft Update </p> <p>Soft updates use Polyak averaging (a kind of moving averaging), a method where the target network\u2019s parameters (\\(\\theta^{-}\\)) are updated gradually based on a weighted combination of the current main network\u2019s parameters (\\(\\theta\\)) and the existing target network\u2019s parameters.</p> <ul> <li> <p>Purpose: This smooth transition avoids abrupt shifts and allows the target network to slowly converge towards the main network\u2019s parameters, promoting stability in learning.</p> </li> <li> <p>Key Characteristics:  </p> <ul> <li>Incremental updates: Parameters are updated gradually at each training step.</li> <li>Flexibility: The weighting factor \\(\\tau\\) (a small constant, e.g., 0.001) controls the speed of convergence.(\\(\\tau \\ll 1\\))</li> <li>Stability: Ensures smooth transitions and minimizes sudden changes.</li> </ul> </li> <li> <p>Equation: </p> </li> </ul> \\[ \\theta^{-} \\leftarrow \\tau \\theta + (1-\\tau) \\theta^{-} \\] <ul> <li> <p>Advantages:  </p> <ul> <li>Gradual updates reduce instability in learning.</li> <li>Ideal for environments requiring smooth and stable convergence.</li> </ul> </li> <li> <p>Trade-off: Slower adaptation to the main network\u2019s parameters compared to hard updates, but the added stability usually outweighs this drawback.</p> </li> </ul> </li> </ol> <p> </p> <p>source: concept target network in category reinforcement learning</p> <p>other links:</p> <p>Deep Q-Network -- Tips, Tricks, and Implementation</p> <p>How and when should we update the Q-target in deep Q-learning?</p>"},{"location":"course_notes/advanced/#policy-network-pi_phias","title":"Policy network (\\(\\pi_{\\phi}(a|s)\\))","text":"<p>The policy \\(\\pi_{\\phi}(a | s)\\) is updated using the soft policy improvement step, minimizing the KL-divergence:</p> \\[ J_{\\pi}(\\phi) = \\mathbb{E}_{s_t \\sim \\mathcal{D}} \\left[ D_{\\text{KL}} \\left( \\pi_{\\phi}(\\cdot | s_t) \\bigg\\| \\frac{\\exp(Q_{\\theta}(s_t, \\cdot))}{Z_{\\theta}(s_t)} \\right) \\right] \\] <p>Instead of solving this directly, SAC reparameterizes the policy using:</p> \\[ a_t = f_{\\phi}(\\epsilon_t; s_t) \\] <p>This trick is used to make sure that sampling from the policy is a differentiable process so that there are no problems in backpropagating the errors.  \\(\\epsilon_t\\) is random noise vector sampled from fixed distribution (e.g., Spherical Gaussian).</p> Why Reparameterization is Needed? <p>In reinforcement learning, the policy \\(\\pi(a | s)\\) often outputs a probability distribution over actions rather than deterministic actions. The standard way to sample an action is:</p> \\[ a_t \\sim \\pi_{\\phi}(a_t | s_t) \\] <p>However, this sampling operation blocks gradient flow during backpropagation, preventing efficient training using stochastic gradient descent (SGD).</p> <p>Instead of directly sampling \\(a_t\\) from \\(\\pi_{\\phi}(a_t | s_t)\\), we transform a simple noise variable into an action:</p> \\[ a_t = f_{\\phi}(\\epsilon_t, s_t) \\] <ul> <li>\\(\\epsilon_t \\sim \\mathcal{N}(0, I)\\) is sampled from a fixed noise distribution (e.g., a Gaussian).</li> <li>\\(f_{\\phi}(\\epsilon_t, s_t)\\) is a differentiable function (often a neural network) that maps noise to an action.</li> </ul> <p>For a Gaussian policy in SAC, the action is computed as:</p> \\[ a_t = \\mu_{\\phi}(s_t) + \\sigma_{\\phi}(s_t) \\cdot \\epsilon_t \\] <p>So instead of sampling from \\(\\mathcal{N}(\\mu, \\sigma^2)\\) directly, we sample from a fixed standard normal and transform it using a differentiable function.This makes the policy differentiable, allowing gradients to flow through \\(\\mu_{\\phi}\\) and \\(\\sigma_{\\phi}\\).</p> <p> </p> <ul> <li>Continuous Action Generation</li> </ul> <p>In a continuous action space soft actor-critic agent, the neural network in the actor takes the current observation and generates two outputs, one for the mean and the other for the standard deviation. To select an action, the actor randomly selects an unbounded action from this Gaussian distribution. If the soft actor-critic agent needs to generate bounded actions, the actor applies tanh and scaling operations to the action sampled from the Gaussian distribution.</p> <p>During training, the agent uses the unbounded Gaussian distribution to calculate the entropy of the policy for the given observation.  </p> <ul> <li>Discrete Action Generation</li> </ul> <p>In a discrete action space soft actor-critic agent, the actor takes the current observation and generates a categorical distribution, in which each possible action is associated with a probability. Since each action that belongs to the finite set is already assumed feasible, no bounding is needed.</p> <p>During training, the agent uses the categorical distribution to calculate the entropy of the policy for the given observation.</p> <p>if we rewrite the equation we have:</p> \\[ J_{\\pi}(\\phi) = \\mathbb{E}_{s_t \\sim \\mathcal{D}, \\epsilon_t \\sim \\mathcal{N}} \\left[ \\text{log}\\space\\pi_{\\phi} \\left(f_{\\phi}(\\epsilon_t; s_t) | s_t \\right) - Q_{\\theta}(s_t,f_{\\phi}(\\epsilon_t; s_t) \\right] \\] <p>where \\(\\pi_{\\phi}\\) is defined implicitly in terms of \\(f_{\\phi}\\), and we have noted that the partition function is independent of \\(\\phi\\) and can thus be omitted.</p> <p>Policy Gradient Update</p> \\[ \\hat{\\nabla}_{\\phi} J_{\\pi}(\\phi) = \\nabla_{\\phi} \\log \\pi_{\\phi}(a_t | s_t) + \\left( \\nabla_{a_t} \\log \\pi_{\\phi}(a_t | s_t)- \\nabla_{a_t} Q_{\\theta}(s_t, a_t) \\right) \\nabla_{\\phi} f_{\\phi}(\\epsilon_t; s_t) \\] <p>SAC main steps</p> <ol> <li> <p>Q-function Update:</p> \\[ Q(s, a) \\leftarrow r(s, a) + \\mathbb{E}_{s' \\sim p, a' \\sim \\pi} \\left[ Q(s', a') - \\log \\pi(a' | s') \\right]. \\] <p>This update converges to \\(Q^\\pi\\), the Q-function under the current policy \\(\\pi\\).</p> </li> <li> <p>Policy Update:</p> \\[ \\pi_{\\text{new}} = \\arg\\min_{\\pi'} D_{KL} \\left( \\pi'(\\cdot | s) \\Bigg\\| \\frac{1}{Z} \\exp Q^{\\pi_{\\text{old}}}(s, \\cdot) \\right). \\] <p>In practice, only one gradient step is taken on this objective to ensure stability.</p> </li> <li> <p>Interaction with the Environment:     Collect more data by interacting with the environment using the updated policy.</p> </li> </ol> <p>Algorithm: Soft Actor-Critic</p> <p>Initialize parameter vectors \\(\\psi, \\bar{\\psi}, \\theta, \\phi\\)</p> <p>for each iteration do for each environment step do \\(a_t \\sim \\pi_{\\phi}(a_t | s_t)\\) \\(s_{t+1} \\sim p(s_{t+1} | s_t, a_t)\\) \\(\\mathcal{D} \\gets \\mathcal{D} \\cup \\{(s_t, a_t, r(s_t, a_t), s_{t+1})\\}\\) end for </p> <p> for each gradient step do \\(\\psi \\gets \\psi - \\lambda \\hat{\\nabla}_{\\psi} J_V(\\psi)\\) \\(\\theta_i \\gets \\theta_i - \\lambda_Q \\hat{\\nabla}_{\\theta_i} J_Q(\\theta_i) \\quad \\text{for } i \\in \\{1,2\\}\\) \\(\\phi \\gets \\phi - \\lambda_{\\pi} \\hat{\\nabla}_{\\phi} J_{\\pi}(\\phi)\\) \\(\\bar{\\psi} \\gets \\tau \\psi + (1 - \\tau) \\bar{\\psi}\\) end for end for</p>"},{"location":"course_notes/advanced/#conclusion","title":"Conclusion","text":""},{"location":"course_notes/advanced/#reward-to-go_1","title":"Reward-to-Go","text":"<p>Reward-to-Go computes the sum of future rewards starting from a specific timestep, ensuring that the agent optimizes actions based on expected future returns rather than full trajectory rewards.</p> \\[ R_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k \\] <p>where \\(\\gamma\\) is the discount factor.</p> <ul> <li> <p>Advantages: </p> <ul> <li>More efficient than using complete trajectory returns.  </li> <li>Enhances learning by assigning appropriate importance to past actions based on their future impact.</li> </ul> </li> <li> <p>Disadvantages: </p> <ul> <li>Still introduces variance in training.  </li> <li>Can be unstable if the reward structure is sparse or highly delayed.  </li> </ul> </li> </ul>"},{"location":"course_notes/advanced/#advantage-estimation","title":"Advantage Estimation","text":"<p>The advantage function quantifies how much better a specific action is compared to the expected return under the current policy. It is defined as:  </p> \\[ A(s_t, a_t) = Q(s_t, a_t) - V(s_t) \\] <ul> <li> <p>Advantages: </p> <ul> <li>Reduces variance in policy gradient updates compared to directly using return estimates.  </li> <li>Helps improve stability in training by distinguishing between good and bad actions.  </li> </ul> </li> <li> <p>Disadvantages: </p> <ul> <li>Requires an accurate value function estimation.  </li> <li>Can still introduce bias if the value function is not well-trained.  </li> </ul> </li> </ul>"},{"location":"course_notes/advanced/#generalized-advantage-estimation-gae_1","title":"Generalized Advantage Estimation (GAE)","text":"<p>GAE improves advantage estimation by introducing a trade-off between bias and variance, using an exponentially-weighted sum of temporal-difference (TD) residuals:</p> \\[ A_t^{GAE(\\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l} \\] <p>where \\(\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\\). The parameter \\(\\lambda\\) determines the trade-off:  </p> <ul> <li>Low \\(\\lambda\\) (close to 0) \u2192 More bias, less variance (TD-learning).  </li> <li> <p>High \\(\\lambda\\) (close to 1) \u2192 Less bias, more variance (Monte Carlo estimation).  </p> </li> <li> <p>Advantages: </p> <ul> <li>Provides a tunable balance between bias and variance.  </li> <li>Improves sample efficiency by reducing variance in advantage estimates.  </li> </ul> </li> <li> <p>Disadvantages: </p> <ul> <li>Requires careful tuning of \\(\\lambda\\) for optimal performance.  </li> <li>Slightly increases computational overhead due to extra calculations.  </li> </ul> </li> </ul>"},{"location":"course_notes/advanced/#comparison-of-reward-to-go-advantage-estimation-and-gae","title":"Comparison of Reward-to-Go, Advantage Estimation, and GAE","text":"Method Variance Reduction Bias Stability Computational Cost Reward-to-Go Moderate No bias Moderate Low Advantage Estimation High Some bias High Moderate GAE Adjustable (via \\(\\lambda\\)) Adjustable High Higher"},{"location":"course_notes/advanced/#proximal-policy-optimization-ppo_1","title":"Proximal Policy Optimization (PPO)","text":"<p>PPO is an on-policy actor-critic algorithm that improves stability by constraining policy updates with a clipped objective function. It builds upon Trust Region Policy Optimization (TRPO) while being simpler to implement.  </p> <ul> <li> <p>Advantages: </p> <ul> <li>Ensures stable updates by limiting drastic policy changes.  </li> <li>Works well in large-scale reinforcement learning problems.  </li> <li>Simple to implement compared to TRPO.  </li> <li>Effective for environments with discrete and continuous action spaces.  </li> </ul> </li> <li> <p>Disadvantages: </p> <ul> <li>As an on-policy method, it requires more samples for training.  </li> <li>Less sample-efficient than off-policy methods like DDPG and SAC.  </li> <li>Clipping can sometimes slow down convergence if not tuned properly.  </li> </ul> </li> </ul>"},{"location":"course_notes/advanced/#deep-deterministic-policy-gradient-ddpg","title":"Deep Deterministic Policy Gradient (DDPG)","text":"<p>DDPG is an off-policy, model-free algorithm designed for continuous action spaces. It extends Deterministic Policy Gradient (DPG) by incorporating experience replay and target networks, inspired by Deep Q-Networks (DQN).  </p> <ul> <li> <p>Advantages: </p> <ul> <li>More sample-efficient than on-policy methods like PPO.  </li> <li>Can handle high-dimensional continuous action spaces effectively.  </li> <li>Uses replay buffers to break correlation in training data.  </li> </ul> </li> <li> <p>Disadvantages: </p> <ul> <li>Highly sensitive to hyperparameters like learning rates and noise scaling.  </li> <li>Poor exploration due to its deterministic policy, requiring techniques like Ornstein-Uhlenbeck (OU) noise.  </li> <li>Prone to instability due to function approximation errors in Q-learning.  </li> </ul> </li> </ul>"},{"location":"course_notes/advanced/#soft-actor-critic-sac_1","title":"Soft Actor-Critic (SAC)","text":"<p>SAC is an off-policy algorithm that improves upon DDPG by introducing entropy regularization, which encourages exploration and robustness. It uses two Q-networks (double Q-learning) to mitigate overestimation bias and a stochastic policy for improved exploration.  </p> <ul> <li> <p>Advantages: </p> <ul> <li>Better exploration due to entropy regularization.  </li> <li>More stable than DDPG because of double Q-learning.  </li> <li>Handles high-dimensional continuous control tasks efficiently.  </li> <li>More sample-efficient than PPO.  </li> </ul> </li> <li> <p>Disadvantages: </p> <ul> <li>Higher computational cost due to multiple Q-function updates.  </li> <li>Requires careful tuning of entropy coefficient \\(\\alpha\\).  </li> <li>Slower than DDPG in deterministic settings where exploration is not an issue.  </li> </ul> </li> </ul>"},{"location":"course_notes/advanced/#comparison-table-ppo-vs-ddpg-vs-sac","title":"Comparison Table: PPO vs. DDPG vs. SAC","text":"Algorithm Type Sample Efficiency Stability Exploration PPO On-policy Low High Moderate DDPG Off-policy High Moderate Weak (deterministic) SAC Off-policy High High Strong (entropy regularization)"},{"location":"course_notes/advanced/#final-thoughts","title":"Final Thoughts","text":"<p>Actor-critic methods such as PPO, DDPG, and SAC have significantly improved reinforcement learning, making it more scalable and sample-efficient.  </p> <ul> <li>PPO is widely used for its stability and ease of implementation but is less sample-efficient.  </li> <li>DDPG works well in continuous control but suffers from poor exploration and instability.  </li> <li>SAC improves upon DDPG by adding entropy regularization, leading to better exploration and stability.  </li> </ul>"},{"location":"course_notes/advanced/#authors","title":"Author(s)","text":"<ul> <li> <p>Ahmad Karami</p> <p>Teaching Assistant</p> <p>ahmad.karami77@yahoo.com</p> <p> </p> </li> <li> <p>Hamidreza Ebrahimpour</p> <p>Teaching Assistant</p> <p>ebrahimpour.7879@gmail.com</p> <p> </p> </li> <li> <p>Hesam Hosseini</p> <p>Teaching Assistant</p> <p>hesam138122@gmail.com</p> <p> </p> </li> </ul>"},{"location":"course_notes/advanced/#references","title":"References","text":"<ol> <li> <p>Reinforcement Learning Explained</p> </li> <li> <p>An Introduction to Deep Reinforcement Learning</p> </li> <li> <p>Deep Reinforcement Learning Processor Design for Mobile Applications</p> </li> <li> <p>Policy Gradient Algorithms</p> </li> <li> <p>Deep Reinforcement Learning</p> </li> <li> <p>Reinforcement Learning (BartoSutton)</p> </li> <li> <p>Spinning Up in Deep RL!</p> </li> <li> <p>CleanRL Algorithms</p> </li> </ol>"},{"location":"course_notes/bandits/","title":"Week 6: Multi-Armed Bandits","text":""},{"location":"course_notes/bandits/#introduction","title":"Introduction","text":"<p>The multi-armed bandit (MAB) problem represents one of the simplest yet profoundly insightful frameworks for analyzing the fundamental dilemma known as the exploration-exploitation tradeoff in decision-making under uncertainty. This tradeoff arises naturally whenever an agent faces multiple choices whose outcomes are uncertain, requiring it to continually balance between exploring unknown actions to discover their potential rewards and exploiting known actions to maximize immediate returns. The elegance and simplicity of the MAB setup enable rigorous theoretical analysis while maintaining relevance to numerous practical scenarios.</p>"},{"location":"course_notes/bandits/#formal-problem-statement","title":"Formal Problem Statement","text":"<p>Formally, a multi-armed bandit problem can be modeled as a simplified form of a Markov Decision Process (MDP) characterized solely by an action set and reward functions, without state dynamics. Specifically, the bandit setup is represented by the tuple \\((\\mathcal{A}, \\mathcal{R})\\), where:</p> <ul> <li> <p>Action Set: \\(\\mathcal{A}\\) is a finite set of discrete actions, often referred to as \"bandit arms,\" indexed by \\(a = 1, 2, \\dots, k\\). Here, \\(k\\) denotes the total number of available actions.</p> </li> <li> <p>Reward Distributions: Each action \\(a \\in \\mathcal{A}\\) is associated with a distinct probability distribution over rewards, denoted by \\(\\mathcal{R}^a\\). Formally, the reward obtained from action \\(a\\) at time step \\(t\\), represented as \\(R_t\\), is sampled from this distribution:</p> </li> </ul> \\[ R_t \\sim \\mathcal{R}^{A_t}, \\quad \\text{where } A_t \\in \\mathcal{A} \\] <p>This means the reward for choosing action \\(a\\) is a random variable with a specific but unknown probability distribution.</p> <ul> <li>Objective: The goal of the agent in this setting is explicitly to maximize the cumulative reward collected over a finite horizon of \\(T\\) steps:</li> </ul> \\[ G_T = \\sum_{t=1}^{T} R_t \\]"},{"location":"course_notes/bandits/#action-value-functions-q-values","title":"Action-Value Functions (Q-values)","text":"<p>To formally analyze and optimize decisions in the multi-armed bandit problem, we define an essential concept known as the action-value or Q-value of an action. The Q-value of an action represents its expected or average reward:</p> \\[ q(a) = \\mathbb{E}[R \\mid A = a] = \\int_{-\\infty}^{\\infty} r \\cdot \\mathcal{R}^{a}(r) \\,dr \\] <p>In simpler terms, the action-value \\(q(a)\\) captures the average reward the agent can expect if it repeatedly selects action \\(a\\). Estimating these action-values accurately is central to solving bandit problems, as optimal actions will naturally correspond to those with higher Q-values.</p>"},{"location":"course_notes/bandits/#optimal-action-and-optimal-value","title":"Optimal Action and Optimal Value","text":"<p>Within the multi-armed bandit framework, there always exists at least one optimal action, denoted by \\(a^\\star\\), that maximizes the expected reward. The corresponding maximum Q-value, known as the optimal value, is defined as:</p> \\[ v_\\star = q(a^\\star) = \\max_{a \\in \\mathcal{A}} q(a) \\] <p>Identifying the optimal action is the primary challenge, as the agent initially lacks knowledge about the reward distributions and must learn through interaction.</p>"},{"location":"course_notes/bandits/#exploration-vs-exploitation-core-difficulty","title":"Exploration vs. Exploitation: Core Difficulty","text":"<p>The fundamental difficulty faced by agents in the MAB scenario arises precisely from the lack of initial knowledge about the underlying reward distributions. The agent must simultaneously accomplish two conflicting tasks:</p> <ul> <li> <p>Exploration: By choosing less-understood or infrequently selected arms, the agent gathers crucial information about their reward structures. Exploration can yield long-term benefits by identifying potentially superior actions.</p> </li> <li> <p>Exploitation: By selecting the actions known to yield high rewards, the agent maximizes immediate returns. Excessive exploitation, however, risks prematurely converging to suboptimal actions due to inadequate exploration.</p> </li> </ul> <p>Balancing these two aspects to maximize cumulative reward over time forms the crux of solving any bandit problem effectively.</p>"},{"location":"course_notes/bandits/#non-associativity-property","title":"Non-Associativity Property","text":"<p>One unique characteristic of the multi-armed bandit setting, which significantly simplifies its theoretical analysis compared to general MDPs, is the property of non-associativity. Formally:</p> <ul> <li> <p>Non-associativity means the optimal action does not depend on any notion of \"state\" or previous actions. In other words, the bandit problem does not include state transitions\u2014each action choice is independent of any past or future decision.</p> </li> <li> <p>Therefore, the optimal action \\(a^\\star\\) remains constant for all time steps, unaffected by previously selected actions. Mathematically, no state-based transition probabilities or value functions conditioned on states are necessary, making the bandit problem a purely action-oriented optimization scenario.</p> </li> </ul> <p>This non-associativity greatly simplifies both theoretical and practical treatment, allowing researchers to isolate the core exploration-exploitation dynamics from more complex temporal or state-dependent phenomena.</p>"},{"location":"course_notes/bandits/#real-world-applications","title":"Real-World Applications","text":"<p>Despite its apparent simplicity, the multi-armed bandit framework finds extensive applications across diverse fields, where efficient decision-making under uncertainty directly influences outcomes. Some key areas include:</p> <ul> <li> <p>Medical Trials: Clinical research often faces the challenge of testing multiple treatments while minimizing patient risk. MAB strategies help researchers adaptively assign treatments, effectively balancing learning (exploring treatment efficacy) and optimizing patient outcomes (exploiting effective treatments).</p> </li> <li> <p>Online Advertising: Digital platforms utilize MAB algorithms to dynamically select advertisements that maximize user engagement and revenue. By continuously balancing exploration of new ads and exploitation of proven performers, businesses optimize long-term profits.</p> </li> <li> <p>Recommendation Systems: Platforms like streaming services or e-commerce websites employ MAB methods to personalize content delivery. Adaptive recommendation algorithms efficiently learn user preferences by experimenting with various content while maintaining user satisfaction.</p> </li> <li> <p>Financial Investment: Asset allocation and portfolio management tasks naturally map onto bandit problems, where investment decisions must balance immediate financial returns against uncertainty about future asset performance. Using MAB-based decision frameworks, investors systematically explore financial instruments to identify strategies that yield superior long-term returns.</p> </li> </ul> <p>In all these applications, the fundamental logic of balancing exploration and exploitation captured by the multi-armed bandit problem remains central to achieving optimal performance under uncertainty.</p>"},{"location":"course_notes/bandits/#action-value-methods-and-types","title":"Action-Value Methods and Types","text":"<p>To effectively approach and solve the Multi-Armed Bandit (MAB) problem, we require a method for accurately estimating the value associated with each action. This value, commonly referred to as the action-value function, denoted by \\(Q_t(a)\\), represents the estimated expected reward of choosing a particular action \\(a\\) at time step \\(t\\). Formally, the goal is for \\(Q_t(a)\\) to approximate the true expected reward \\(q_*(a)\\), as closely as possible:</p> \\[ Q_t(a) \\approx q_*(a). \\] <p>In practice, the exact values \\(q_*(a)\\) are unknown and must be estimated through experience.</p>"},{"location":"course_notes/bandits/#sample-average-estimation","title":"Sample-Average Estimation","text":"<p>A straightforward approach for estimating the action-value is known as sample-average estimation. Under this method, the value of an action \\(a\\) is estimated by averaging all the observed rewards obtained from selecting action \\(a\\) up to time step \\(t\\). The sample-average estimator is formally defined as:</p> \\[ Q_t(a) = \\frac{1}{N_t(a)} \\sum_{i=1}^{N_t(a)} R_i, \\] <p>where: - \\(N_t(a)\\) is the total number of times action \\(a\\) has been selected up to time step \\(t\\).</p> <ul> <li>\\(R_i\\) is the reward received at the \\(i^{th}\\) time action \\(a\\) was selected.</li> </ul> Intuition <p>This method relies on the Law of Large Numbers, where averaging a large number of observations converges to the true expected reward. Initially, the estimates are inaccurate due to limited observations, but as the action is repeatedly selected, the estimate    \\(Q_t(a)\\) increasingly stabilizes and converges towards the true mean reward \\(q_*(a)\\).</p>"},{"location":"course_notes/bandits/#incremental-update-rule-for-efficient-computation","title":"Incremental Update Rule for Efficient Computation","text":"<p>While computing the action-value through sample-average estimation, it would be computationally inefficient and memory-intensive to store and sum all previous rewards each time a new reward is obtained. Instead, an efficient, incremental update rule can be derived, allowing the estimate \\(Q_t(a)\\) to be updated using only the previously calculated estimate and the most recent reward.</p> <p>This incremental rule is given by:</p> \\[ Q_{t+1}(a) = Q_t(a) + \\frac{1}{N_t(a)} \\left(R_t - Q_t(a)\\right). \\] Derivation of the Incremental Update Rule <p>Starting from the definition of the sample-average estimate at the next time step \\(t+1\\), we have:</p> \\[ Q_{t+1}(a) = \\frac{1}{N_{t+1}(a)} \\sum_{i=1}^{N_{t+1}(a)} R_i. \\] <p>Breaking this down into the previous \\(N_t(a)\\) rewards plus the most recent reward \\(R_t\\), we have:</p> \\[ Q_{t+1}(a) = \\frac{1}{N_{t+1}(a)} \\left(\\sum_{i=1}^{N_t(a)} R_i + R_t\\right). \\] <p>We already have from the previous step:</p> \\[ Q_t(a) = \\frac{1}{N_t(a)} \\sum_{i=1}^{N_t(a)} R_i \\quad \\Rightarrow \\quad \\sum_{i=1}^{N_t(a)} R_i = N_t(a)Q_t(a). \\] <p>Substituting this into the equation above gives:</p> \\[ Q_{t+1}(a) = \\frac{1}{N_{t+1}(a)} \\left(N_t(a)Q_t(a) + R_t\\right). \\] <p>Recognizing that \\(N_{t+1}(a) = N_t(a) + 1\\), we can rewrite this as:</p> \\[ Q_{t+1}(a) = Q_t(a) + \\frac{1}{N_t(a) + 1}\\left(R_t - Q_t(a)\\right), \\] <p>which is precisely the incremental update rule. This formulation clearly demonstrates that updating action-value estimates does not require retaining all historical  rewards\u2014only the current estimate, \\(Q_t(a)\\), and the most recent observation, \\(R_t\\), are needed.</p>"},{"location":"course_notes/bandits/#constant-step-size-update-for-nonstationary-problems","title":"Constant Step-Size Update for Nonstationary Problems","text":"<p>The previously discussed sample-average estimation assumes the reward distributions are stationary (constant over time). However, many practical problems involve nonstationary environments, where the true action values can change over time. To handle such scenarios, we introduce a modified update rule that uses a constant step-size parameter \\(\\alpha\\) instead of the diminishing factor \\(\\frac{1}{N_t(a)}\\):</p> \\[ Q_{t+1}(a) = Q_t(a) + \\alpha(R_t - Q_t(a)), \\] <p>where \\(0 &lt; \\alpha \\leq 1\\) determines how much emphasis is placed on recent rewards.</p> <ul> <li>If \\(\\alpha = \\frac{1}{N_t(a)}\\), this formulation reverts back to the sample-average method.</li> <li>If \\(\\alpha\\) is constant and fixed, recent rewards have greater influence, making the estimates more responsive to changes in the environment.</li> </ul> Exponential Weighted Averaging <p>When employing a constant step-size, the estimate effectively becomes an exponentially weighted average of past rewards, giving exponentially decreasing weights to older observations. This becomes clear by expanding the incremental update recursively:</p> \\[ Q_{t+1}(a) = (1 - \\alpha)Q_t(a) + \\alpha R_t \\] <p>Continuing recursively for additional steps, we have:</p> \\[ Q_{t+2}(a) = (1 - \\alpha)^2 Q_t(a) + \\alpha(1 - \\alpha) R_t + \\alpha R_{t+1}. \\] <p>Generalizing this recursive expansion, the influence of the initial estimate \\(Q_0(a)\\) decreases exponentially, and we have the general form:</p> \\[ Q_t(a) = (1 - \\alpha)^t Q_0(a) + \\sum_{i=0}^{t-1} \\alpha(1 - \\alpha)^i R_{t-i}. \\] <p>This explicitly illustrates the exponential weighting mechanism: recent rewards (closer to the current time \\(t\\)) exert a higher influence on the current estimate, while older rewards have their influence gradually diminished by a factor of \\((1 - \\alpha)\\) per time step.</p> <p>This exponential weighting characteristic makes the constant step-size update particularly well-suited for dynamic, nonstationary environments, where quickly adapting to changes in action-value distributions is critical.</p>"},{"location":"course_notes/bandits/#regret-measuring-suboptimality","title":"Regret: Measuring Suboptimality","text":""},{"location":"course_notes/bandits/#concept-of-regret","title":"Concept of Regret","text":"<p>In sequential decision-making, especially within reinforcement learning and multi-armed bandit frameworks, a central concept is the regret. Regret quantifies the notion of lost opportunity incurred by choosing suboptimal actions over optimal ones. Intuitively, regret measures how much better the agent could have performed had it always selected the best possible action available, denoted by \\(a^\\star\\). Formally, we define the instantaneous regret at iteration \\(t\\) as the expected difference between the reward from the optimal action and the reward received from the chosen action \\(A_t\\):</p> \\[ I_t = \\mathbb{E}[v_\\star - q(A_t)], \\] <p>where \\(v_\\star\\) represents the expected reward of the optimal action \\(a^\\star\\), and \\(q(A_t)\\) represents the expected reward from the action actually taken at step \\(t\\).</p>"},{"location":"course_notes/bandits/#total-regret","title":"Total Regret","text":"<p>To evaluate the performance of an agent over a sequence of decisions, we typically consider the cumulative effect of these instantaneous regrets. The total regret over a horizon of \\(t\\) steps is thus:</p> \\[ L_t = \\mathbb{E}\\left[\\sum_{\\tau=1}^{t} (v_\\star - q(A_\\tau))\\right]. \\] <p>Minimizing total regret is directly equivalent to maximizing cumulative reward, making regret a natural performance metric for learning algorithms in reinforcement learning contexts.</p>"},{"location":"course_notes/bandits/#3-regret-gap-and-action-counts","title":"3. Regret, Gap, and Action Counts","text":"<p>To analyze regret in greater detail, we introduce two important concepts:</p> <ul> <li>The action-count \\(N_t(a)\\), which denotes the expected number of times an action \\(a\\) has been selected up to iteration \\(t\\).</li> <li>The gap \\(\\Delta_a\\), defined as the difference between the optimal action's expected value and the expected value of action \\(a\\):</li> </ul> \\[ \\Delta_a = v_\\star - q(a). \\] <p>Given these definitions, the total regret \\(L_t\\) can also be expressed in terms of the gaps and action counts. Specifically, by decomposing the regret according to how often each suboptimal action is chosen, we have:</p> \\[ \\begin{aligned} L_t &amp;= \\mathbb{E}\\left[\\sum_{\\tau=1}^{t} (v_\\star - q(A_\\tau))\\right] \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\mathbb{E}[N_t(a)](v_\\star - q(a)) \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\mathbb{E}[N_t(a)] \\Delta_a. \\end{aligned} \\] <p>Thus, the problem of regret minimization reduces to minimizing the expected count of suboptimal actions chosen, particularly those with large gaps.</p>"},{"location":"course_notes/bandits/#regret-dynamics-and-algorithmic-insights","title":"Regret Dynamics and Algorithmic Insights","text":"<p>An important insight about regret is how it grows as a function of time \\(t\\) under various strategies. For instance, a purely greedy algorithm\u2014one that selects actions solely based on current value estimates\u2014will exhibit linear regret. This linear growth occurs because the algorithm might prematurely \"lock in\" on a suboptimal action indefinitely, accruing constant regret at each step.</p> <p>One powerful mitigation strategy is known as optimistic initialization, where we deliberately overestimate initial action values. Formally, the action-value estimates \\(Q(a)\\) are updated using an averaging process:</p> \\[ Q(a) = \\frac{1}{N_t(a)} \\sum_{\\tau=1}^{t} \\mathbf{1}(A_\\tau = a) R_\\tau. \\] <p>This optimistic approach incentivizes initial exploration, reducing the chance of permanently settling on a suboptimal action, thereby improving long-term regret performance.</p>"},{"location":"course_notes/bandits/#lower-bound-on-regret-lai-robbins-bound-this-topic-is-beyond-the-scope-of-this-course","title":"Lower Bound on Regret (Lai-Robbins Bound) (This topic is beyond the scope of this course.)","text":"<p>An essential theoretical result by Lai and Robbins (1985) provides a fundamental lower bound on achievable regret growth for any \"consistent\" algorithm\u2014that is, any algorithm whose regret grows sublinearly for all problem instances. Formally, the Lai-Robbins bound is stated as:</p> \\[ \\liminf_{t \\to \\infty} \\frac{L_t}{\\ln t} \\geq \\sum_{a \\mid \\Delta_a &gt; 0} \\frac{\\Delta_a}{D_{\\text{KL}}(\\mathcal{R}^a \\|\\| \\mathcal{R}^{a^\\star})}, \\] <p>where \\(D_{\\text{KL}}(\\mathcal{R}^a || \\mathcal{R}^{a^\\star})\\) is the Kullback\u2013Leibler (KL) divergence between the reward distributions of a suboptimal arm \\(a\\) and the optimal arm \\(a^\\star\\). Intuitively, this bound indicates that arms with smaller gaps (\\(\\Delta_a\\) close to zero) or similar reward distributions to the optimal arm (small KL divergence) inherently require more exploration, resulting in greater regret.</p> liminf <p>In mathematics, the limit inferior (or liminf) of a sequence \\(\\{a_n\\}\\) is defined as:</p> \\[ \\liminf_{n \\to \\infty} a_n = \\lim_{n \\to \\infty} \\left( \\inf \\{a_k : k \\geq n\\} \\right) \\] <p>This expression represents the greatest lower bound of the tail of the sequence, effectively capturing the \"largest eventual minimum\" of the sequence.   </p>"},{"location":"course_notes/bandits/#bernoulli-bandit-case","title":"Bernoulli Bandit Case","text":"<p>In practical scenarios such as Bernoulli bandits, where each action's reward distribution is Bernoulli(\\(\\mu_a\\)), the KL divergence has a closed-form expression:</p> \\[ D_{\\text{KL}}(\\text{Bern}(\\mu_a) \\|\\| \\text{Bern}(\\mu^\\star)) = \\mu^\\star \\ln \\frac{\\mu^\\star}{\\mu_a} + (1 - \\mu^\\star) \\ln \\frac{1 - \\mu^\\star}{1 - \\mu_a}. \\] <p>For large \\(t\\), the Lai-Robbins bound simplifies approximately to:</p> \\[ L_t \\gtrsim \\sum_{a \\mid \\mu_a &lt; \\mu^\\star} \\frac{\\ln t}{\\mu^\\star - \\mu_a}, \\] <p>clearly demonstrating the logarithmic lower bound on regret growth. Thus, no algorithm can improve beyond a logarithmic rate of regret growth for these problem instances.</p>"},{"location":"course_notes/bandits/#problem-dependent-versus-minimax-regret","title":"Problem-Dependent versus Minimax Regret","text":"<p>The regret bounds discussed so far are problem-dependent, reflecting intrinsic characteristics of specific problem instances (such as gaps between arms). Another view is the minimax regret, which considers the worst-case regret across all possible problem instances. For stochastic bandits with fixed reward distributions, the problem-dependent bound (\\(\\Theta(\\ln t)\\)) is generally more informative and achievable compared to the minimax bound, which typically scales as \\(\\Theta(\\sqrt{Kt})\\) in adversarial settings.</p> <p>Several algorithms, including Upper Confidence Bound (UCB) and Thompson Sampling, have been shown to achieve regret growth matching the logarithmic Lai-Robbins lower bound, both asymptotically and in some cases even in constant factors. This optimal performance contrasts starkly with naive strategies such as fixed \\(\\varepsilon\\)-greedy methods, which incur linear regret due to continual exploration.</p>"},{"location":"course_notes/bandits/#the-explorationexploitation-trade-off","title":"The Exploration\u2013Exploitation Trade-off","text":"<p>In sequential decision-making tasks, particularly in the multi-armed bandit problem, maintaining accurate estimates of the value or expected reward of each available action (often called an \"arm\") is essential. However, accurate estimation alone is insufficient. A fundamental and challenging issue emerges naturally from this setting: the exploration\u2013exploitation trade-off. This trade-off encapsulates a strategic decision every agent must confront repeatedly: should it exploit its current knowledge by choosing actions known (or estimated) to yield high rewards, or should it explore uncertain options to gather more information and potentially discover even more rewarding choices?</p>"},{"location":"course_notes/bandits/#formal-definition-and-intuition","title":"Formal Definition and Intuition","text":"<p>Formally, the exploration\u2013exploitation trade-off can be characterized as follows. Consider a bandit problem with a set of arms \\(\\mathcal{A} = \\{1, 2, \\dots, K\\}\\), each arm \\(i\\) associated with a fixed but unknown reward distribution characterized by a mean reward \\(\\mu_i\\). At any time step \\(t\\), the agent selects an arm \\(A_t \\in \\mathcal{A}\\), observes a reward \\(R_t \\sim \\text{distribution}(\\mu_{A_t})\\), and updates its value estimates accordingly. If we denote the agent's estimate of the expected reward of arm \\(i\\) at time \\(t\\) by \\(\\hat{Q}_t(i)\\), the decision about which arm to pull next becomes critical.</p> <p>The key tension arises because of incomplete knowledge: the agent does not initially know the true mean rewards \\(\\mu_i\\). Thus, it must decide between:</p> <ul> <li>Exploitation: Selecting the arm with the highest current estimated value \\(\\hat{Q}_t(i)\\) (greedy choice) to maximize immediate reward.</li> <li>Exploration: Selecting a less certain arm to refine value estimates and possibly discover a superior arm for future exploitation.</li> </ul> <p>Intuitively, excessive exploitation risks converging prematurely to a suboptimal arm due to misleading early observations. On the other hand, excessive exploration continuously incurs opportunity costs by potentially sacrificing immediate rewards. Hence, effective strategies must delicately balance these competing objectives to achieve minimal long-term regret.</p>"},{"location":"course_notes/bandits/#risks-of-pure-exploitation-and-exploration-strategies","title":"Risks of Pure Exploitation and Exploration Strategies","text":"<p>Consider first a purely exploitative approach\u2014commonly referred to as the greedy strategy. Under this policy, at every step after an initial brief exploration period, the agent always selects the arm that currently has the highest empirical mean reward:</p> \\[ A_t = \\arg\\max_{i \\in \\mathcal{A}} \\hat{Q}_t(i) \\] <p>At first glance, this might seem optimal, as the agent is consistently choosing the \"best\" known option. However, such a strategy is vulnerable to early randomness. For example, if the true best arm \\(i^*\\) initially yields a low reward due to chance, while an inferior arm \\(j\\) provides unusually high early rewards, the greedy policy will mistakenly favor the suboptimal arm \\(j\\) indefinitely. Consequently, the agent fails to discover the superior reward potential of arm \\(i^*\\), causing substantial long-term regret. Formally, it can be rigorously proven that the purely greedy strategy incurs linear regret:</p> \\[ R(T) = \\Theta(T), \\quad \\text{as } T \\rightarrow \\infty \\] <p>In contrast, a purely exploratory strategy, which chooses arms uniformly at random or with constant probability regardless of their past performance, guarantees discovery of each arm\u2019s true expected value but at an excessive cost. Because exploration is indiscriminate, the agent continues to select suboptimal arms frequently, incurring unnecessary losses. This continuous exploration also leads to linear regret in expectation:</p> \\[ R(T) = \\Theta(T) \\quad \\text{(pure exploration strategy)} \\] <p>Therefore, neither extreme\u2014pure exploitation nor pure exploration\u2014is desirable. A systematic, controlled approach is required to reduce regret from linear to sublinear growth.</p>"},{"location":"course_notes/bandits/#regret-minimization-and-the-concept-of-optimism","title":"Regret Minimization and the Concept of Optimism","text":"<p>Regret, denoted \\(R(T)\\), measures the cumulative loss of reward compared to always choosing the best possible arm \\(i^*\\) with mean reward \\(\\mu^* = \\max_i \\mu_i\\). Mathematically, regret after \\(T\\) steps is defined as:</p> \\[ R(T) = T \\mu^* - \\sum_{t=1}^{T} \\mu_{A_t} \\] <p>Strategies addressing the exploration\u2013exploitation trade-off aim for sublinear regret growth, typically logarithmic or polynomial, ensuring the average regret per step diminishes as time progresses. This goal motivates the concept of optimism in the face of uncertainty, a foundational principle guiding many effective algorithms. Optimism assumes uncertain actions potentially hold better rewards than currently estimated, encouraging exploration of less well-known arms. As the uncertainty around an arm's estimated value decreases (through repeated selection and reward observation), the optimism naturally decreases, favoring exploitation once the uncertainty sufficiently narrows.</p>"},{"location":"course_notes/bandits/#common-approaches-to-balancing-exploration-and-exploitation","title":"Common Approaches to Balancing Exploration and Exploitation","text":"<p>Several classic strategies systematically manage exploration and exploitation, each embodying optimism in a different way. We'll now skim through them briefly and then dive deeper into each one.</p>"},{"location":"course_notes/bandits/#1-epsilon-greedy-strategy","title":"1. \\(\\epsilon\\)-Greedy Strategy","text":"<p>The \\(\\epsilon\\)-greedy algorithm selects the greedy action with probability \\(1-\\epsilon\\), and explores randomly chosen arms uniformly with probability \\(\\epsilon\\). This approach guarantees continuous exploration but at a simple and fixed rate, allowing eventual convergence toward the optimal arm. The key limitation is the fixed exploration rate, which may remain unnecessarily high as uncertainty decreases, leading to avoidable regret.</p>"},{"location":"course_notes/bandits/#2-optimistic-initial-values","title":"2. Optimistic Initial Values","text":"<p>This approach deliberately initializes all arms\u2019 value estimates \\(\\hat{Q}_0(i)\\) optimistically high. The optimism encourages initial exploration since arms must be repeatedly tested to reduce inflated estimates toward their true values. Eventually, as real performance emerges clearly, exploitation naturally takes over. While effective initially, this method relies heavily on appropriate initial values and may lack flexibility at later stages.</p>"},{"location":"course_notes/bandits/#3-upper-confidence-bound-ucb-algorithms","title":"3. Upper Confidence Bound (UCB) Algorithms","text":"<p>UCB methods use statistical confidence intervals around the estimated values of arms. The algorithm selects actions by:</p> \\[ A_t = \\arg\\max_{i} \\left[ \\hat{Q}_t(i) + \\sqrt{\\frac{2\\ln t}{N_t(i)}} \\right] \\] <p>where \\(N_t(i)\\) denotes the number of times arm \\(i\\) has been chosen by time \\(t\\). The term added to the estimate is larger when arm \\(i\\) is less explored (small \\(N_t(i)\\)), creating optimism toward uncertain arms. This systematic exploration results in a provably logarithmic regret bound, making UCB highly appealing from a theoretical perspective.</p>"},{"location":"course_notes/bandits/#4-thompson-sampling-bayesian-probability-matching","title":"4. Thompson Sampling (Bayesian Probability Matching)","text":"<p>Thompson Sampling employs a Bayesian framework. At each step, the agent draws random samples from posterior distributions representing its belief about arm values and chooses the arm associated with the highest sampled value. This probabilistic matching naturally balances exploration and exploitation, with uncertainty directly encoded in the posterior distributions. Thompson sampling frequently demonstrates excellent empirical and theoretical performance, often achieving state-of-the-art regret bounds.</p>"},{"location":"course_notes/bandits/#exploration-strategies-for-multi-armed-bandits","title":"Exploration Strategies for Multi-Armed Bandits","text":"<p>Multi-armed bandit (MAB) problems embody the fundamental challenge of balancing exploration (gathering information about the uncertain environment) and exploitation (leveraging existing knowledge to maximize rewards). Several exploration strategies have emerged, each employing distinct mechanisms to navigate this critical trade-off. Below, we elaborate on two common strategies\u2014the \u03b5-Greedy algorithm and Optimistic Initial Values\u2014examining their theoretical underpinnings, implementation specifics, and intuitive rationale.</p>"},{"location":"course_notes/bandits/#the-epsilon-greedy-algorithm","title":"The \\(\\epsilon\\)-Greedy Algorithm","text":""},{"location":"course_notes/bandits/#overview-and-motivation","title":"Overview and Motivation","text":"<p>The \\(\\epsilon\\)-greedy algorithm is one of the most fundamental and widely used strategies for balancing the exploration-exploitation trade-off in sequential decision-making problems, particularly in the context of the stochastic multi-armed bandit problem. Its appeal lies in its simplicity and intuitive structure: the agent typically selects what appears to be the best action according to its current knowledge (exploitation), but occasionally takes a random action to gather more information about alternatives (exploration).</p> <p>Formally, consider a \\(K\\)-armed bandit problem, where each arm \\(i \\in \\{1, \\dots, K\\}\\) provides i.i.d. rewards drawn from an unknown distribution with mean \\(\\mu_i\\). The goal is to maximize the cumulative reward over time, or equivalently, minimize the regret with respect to always playing the optimal arm \\(i^* = \\arg\\max_i \\mu_i\\).</p> <p>The \\(\\epsilon\\)-greedy algorithm addresses this by injecting randomness into the action selection process. At each time step \\(t\\), it behaves as follows:</p> \\[ A_t =  \\begin{cases} \\text{random arm from } \\{1,\\dots,K\\}, &amp; \\text{with probability } \\epsilon, \\\\ \\arg\\max_i \\hat{Q}_{t-1}(i), &amp; \\text{with probability } 1 - \\epsilon. \\end{cases} \\] <p>Here, \\(\\hat{Q}_{t-1}(i)\\) is the estimated mean reward of arm \\(i\\) based on observations up to time \\(t-1\\).</p>"},{"location":"course_notes/bandits/#formal-definition","title":"Formal Definition","text":"<p>Let us define the following notation:</p> <ul> <li>Let \\(K\\) be the number of arms.</li> <li>Let \\(X_{i, s}\\) be the \\(s\\)-th observed reward from arm \\(i\\).</li> <li>Let \\(N_t(i)\\) denote the number of times arm \\(i\\) has been selected up to time \\(t\\).</li> <li>Let \\(\\hat{Q}_t(i) = \\frac{1}{N_t(i)} \\sum_{s=1}^{N_t(i)} X_{i,s}\\) denote the empirical mean reward for arm \\(i\\) at time \\(t\\).</li> </ul> <p>At time \\(t+1\\), the \\(\\epsilon\\)-greedy algorithm proceeds as:</p> \\[ A_{t+1} = \\begin{cases} \\text{randomly select an arm from } \\{1, \\dots, K\\}, &amp; \\text{with probability } \\epsilon, \\\\ \\arg\\max_i \\hat{Q}_t(i), &amp; \\text{with probability } 1 - \\epsilon. \\end{cases} \\] <p>The value of \\(\\epsilon \\in [0,1]\\) is typically a small constant (e.g., \\(\\epsilon = 0.1\\)), ensuring occasional exploration while primarily exploiting the current knowledge.</p> Intuition Behind the Algorithm <p>The core idea of \\(\\epsilon\\)-greedy is to ensure that all arms are explored with non-zero probability. This addresses the fundamental problem of uncertainty in estimating the rewards of each arm. Initially, all estimates \\(\\hat{Q}_t(i)\\) are inaccurate due to limited samples. If the algorithm only exploits the current maximum, it risks becoming overconfident in suboptimal arms and permanently ignoring better alternatives.</p> <p>Exploration allows the algorithm to collect more data about all arms, improving the estimates and preventing premature convergence to a suboptimal policy. Exploitation ensures that we are using the best known option most of the time, thus maximizing the expected reward in the short term.</p> <p>The balance is governed by \\(\\epsilon\\): high \\(\\epsilon\\) means more exploration (potentially higher short-term regret), while low \\(\\epsilon\\) means more exploitation (potentially poor long-term performance if the optimal arm is missed early).</p>"},{"location":"course_notes/bandits/#regret-analysis-with-constant-epsilon","title":"Regret Analysis with Constant \\(\\epsilon\\)","text":"<p>The regret of a bandit algorithm at time \\(T\\) is defined as:</p> \\[ R(T) = T \\mu^* - \\mathbb{E} \\left[ \\sum_{t=1}^T \\mu_{A_t} \\right], \\] <p>where \\(\\mu^* = \\max_i \\mu_i\\) is the mean reward of the optimal arm.</p> <p>For constant \\(\\epsilon\\), the agent explores with probability \\(\\epsilon\\) in each round. During exploration, it selects a random arm uniformly from the \\(K\\) options. Hence, even as time progresses and the estimate \\(\\hat{Q}_t\\) of the optimal arm improves, the algorithm will still spend \\(\\epsilon T\\) steps (in expectation) exploring randomly.</p> <p>Let \\(\\Delta_i = \\mu^* - \\mu_i\\) denote the expected reward gap between arm \\(i\\) and the optimal arm. Then, the expected regret due to exploration is roughly:</p> \\[ R_{\\text{explore}}(T) \\approx \\epsilon T \\cdot \\Delta_{\\text{avg}}, \\] <p>where \\(\\Delta_{\\text{avg}} = \\frac{1}{K} \\sum_{i=1}^K \\Delta_i\\) is the average regret per random pull.</p> <p>The expected regret due to exploitation is smaller. With enough exploration, the agent will learn to identify the optimal arm, and during the \\((1 - \\epsilon)T\\) exploitation steps, it will mostly choose the correct arm. Hence, the dominant contribution to regret comes from exploration.</p> <p>Thus, for constant \\(\\epsilon\\), we have:</p> \\[ R(T) = \\Theta(T), \\quad \\text{(linear regret)} \\] <p>and the average regret \\(\\frac{R(T)}{T} \\to \\epsilon \\Delta_{\\text{avg}}\\) as \\(T \\to \\infty\\). Therefore, constant-\\(\\epsilon\\) greedy is not asymptotically optimal.</p>"},{"location":"course_notes/bandits/#convergence-of-hatq_t-estimates","title":"Convergence of \\(\\hat{Q}_t\\) Estimates","text":"<p>Despite its linear regret, constant-\\(\\epsilon\\) greedy does guarantee convergence of estimates. Since there is a fixed, non-zero probability of selecting each arm at every time step, the number of times any given arm \\(i\\) is selected satisfies:</p> \\[ \\mathbb{E}[N_T(i)] \\geq \\epsilon \\cdot \\frac{T}{K}. \\] <p>By the Law of Large Numbers, this ensures:</p> \\[ \\hat{Q}_T(i) \\xrightarrow{a.s.} \\mu_i \\quad \\text{as } T \\to \\infty, \\] <p>for all \\(i\\). Thus, in the limit, the algorithm identifies the optimal arm correctly, but continues to explore forever at a constant rate \u2014 causing the regret to grow linearly over time.</p>"},{"location":"course_notes/bandits/#decaying-epsilon_t-and-sublinear-regret","title":"Decaying \\(\\epsilon_t\\) and Sublinear Regret","text":"<p>To improve performance, one can use a time-dependent exploration rate \\(\\epsilon_t\\) that decreases with \\(t\\). The motivation is that early on, when little is known, exploration should be frequent. As estimates improve, less exploration is needed, and exploitation becomes safer.</p> <p>Common choices for decaying schedules include:</p> <ul> <li>Inverse time decay: \\(\\epsilon_t = \\frac{1}{t}\\)</li> <li>Logarithmic decay: \\(\\epsilon_t = \\frac{c \\ln t}{t}\\), for some constant \\(c &gt; 0\\)</li> <li>Gap-aware decay: \\(\\epsilon_t = \\min\\left\\{1, \\frac{K}{t \\Delta^2}\\right\\}\\) (requires knowledge of gap \\(\\Delta\\))</li> </ul> <p>Under such schedules, we can show that:</p> \\[ R(T) = O(\\ln T), \\] <p>i.e., the regret grows logarithmically, which is the best one can hope for in the stochastic setting (matching the lower bound of Lai and Robbins for the asymptotic regret of consistent algorithms).</p> <p>Sketch of proof (informal intuition): With \\(\\epsilon_t = \\frac{c \\ln t}{t}\\), the total number of exploration steps up to time \\(T\\) is approximately:</p> \\[ \\sum_{t=1}^T \\epsilon_t = \\sum_{t=1}^T \\frac{c \\ln t}{t} \\leq c \\ln^2 T. \\] <p>This means that suboptimal arms are chosen much less frequently over time, and the cumulative regret remains sublinear.</p>"},{"location":"course_notes/bandits/#optimistic-initial-values","title":"Optimistic Initial Values","text":"<p>In the context of multi-armed bandit problems, where an agent must choose among several actions (or \"arms\") to maximize cumulative reward, one of the core challenges is managing the exploration-exploitation trade-off. That is, the agent must balance the need to explore lesser-known actions to gather information with the desire to exploit currently believed-to-be optimal actions to maximize rewards.</p> <p>While a common strategy like \\(\\epsilon\\)-greedy addresses this by injecting randomness into action selection, another elegant and deterministic alternative is optimistic initialization, or optimistic initial values. This approach leverages prior optimism to naturally induce exploration, without relying on explicit stochasticity.</p>"},{"location":"course_notes/bandits/#motivating-intuition","title":"Motivating Intuition","text":"<p>The intuition behind optimistic initial values stems from a simple psychological metaphor: the agent begins with overly optimistic beliefs about the potential payoff of every action. It \u201cbelieves\u201d every arm is excellent \u2014 better than it realistically could be \u2014 and therefore is compelled to try each arm at least once to confirm or refute this belief. Upon playing an arm and observing actual rewards (which are, on average, lower than the initial belief), the agent adjusts its estimate downward. Thus, exploration arises not from randomness, but from disappointment in inflated expectations.</p> <p>This method of optimistic bias is especially powerful in stationary environments, where the underlying reward distributions do not change over time. In such settings, an intense burst of early exploration can suffice, after which the agent can greedily exploit the best-known option based on refined value estimates.</p>"},{"location":"course_notes/bandits/#formal-definition-and-mathematical-formulation","title":"Formal Definition and Mathematical Formulation","text":"<p>Let us consider the standard \\(k\\)-armed bandit problem. Each arm \\(i \\in \\{1, 2, \\dots, k\\}\\) yields stochastic rewards drawn from an unknown and stationary distribution with true mean \\(\\mu_i\\).</p> <p>The agent maintains an estimate \\(\\hat{Q}_t(i)\\) of the mean reward for each arm \\(i\\) at time \\(t\\), which is updated incrementally as rewards are observed. The standard sample average update rule is:</p> \\[ \\hat{Q}_{t+1}(i) = \\hat{Q}_t(i) + \\alpha_t(i) \\left( R_t(i) - \\hat{Q}_t(i) \\right), \\] <p>where:</p> <ul> <li>\\(R_t(i)\\) is the reward observed after playing arm \\(i\\) at time \\(t\\),</li> <li>\\(\\alpha_t(i)\\) is the step size, typically set to \\(1/N_t(i)\\) where \\(N_t(i)\\) is the number of times arm \\(i\\) has been selected by time \\(t\\).</li> </ul> <p>In optimistic initialization, we initialize the estimates as follows:</p> \\[ \\hat{Q}_0(i) = Q^+ \\quad \\text{for all } i, \\] <p>where \\(Q^+\\) is a constant such that \\(Q^+ &gt; \\max_i \\mu_i\\), i.e., it exceeds all plausible true reward means. For example, if rewards are bounded in the interval \\([0, 1]\\), a typical choice is \\(Q^+ = 1\\) or even slightly higher.</p> <p>At each time step \\(t\\), the agent selects the arm with the highest estimated value:</p> \\[ A_t = \\arg\\max_i \\hat{Q}_t(i), \\] <p>which is a purely greedy policy.</p>"},{"location":"course_notes/bandits/#behavioral-dynamics","title":"Behavioral Dynamics","text":"<p>Initially, all estimates \\(\\hat{Q}_0(i) = Q^+\\) are equal and maximal, so the agent arbitrarily picks one. Upon selecting an arm, the estimate is updated based on the reward received. Because actual rewards are typically lower than \\(Q^+\\), the new estimate will decrease. Since all other arms still retain their high initial estimates, the agent is then drawn to try those next. This cyclic effect continues until all arms have been sampled and their estimates revised downward, in proportion to their observed performance.</p> <p>Once each arm has been sampled sufficiently to provide reliable estimates of their true means, the arm with the highest empirical average is selected going forward. At this point, the agent effectively switches from exploration to exploitation \u2014 but crucially, without any explicit exploration parameter.</p>"},{"location":"course_notes/bandits/#comparison-to-epsilon-greedy","title":"Comparison to \\(\\epsilon\\)-Greedy","text":"<p>In contrast to \\(\\epsilon\\)-greedy \u2014 where the agent continues to explore with fixed probability \\(\\epsilon\\) indefinitely \u2014 optimistic initialization focuses exploration into the early stage of learning. Once the overly optimistic estimates are corrected, the algorithm becomes purely greedy. This concentrated exploration phase often leads to faster convergence to optimal behavior, especially when the environment is stationary.</p> <p>Furthermore, optimistic initialization avoids persistent exploration of obviously suboptimal arms, a common downside of \\(\\epsilon\\)-greedy. This leads to reduced long-term regret in many practical scenarios.</p>"},{"location":"course_notes/bandits/#regret-analysis","title":"Regret Analysis","text":"<p>Let us now examine the regret of optimistic initialization from a theoretical standpoint. Define the regret at time \\(T\\) as:</p> \\[ \\text{Regret}(T) = T\\mu^* - \\sum_{t=1}^T \\mathbb{E}[\\mu_{A_t}], \\] <p>where \\(\\mu^* = \\max_i \\mu_i\\) is the mean reward of the optimal arm.</p> <p>Even though the policy is greedy after a short initial phase, optimistic initialization ensures that every arm is sampled sufficiently many times to detect suboptimality. Suppose that \\(Q^+\\) is set such that each suboptimal arm \\(i\\) is pulled at most \\(O\\left(\\frac{1}{\\Delta_i^2} \\log T\\right)\\) times, where \\(\\Delta_i = \\mu^* - \\mu_i\\) is the suboptimality gap. This yields:</p> \\[ \\text{Regret}(T) = O\\left( \\sum_{i: \\Delta_i &gt; 0} \\frac{\\log T}{\\Delta_i} \\right), \\] <p>which matches the asymptotic regret bound of more sophisticated algorithms like Upper Confidence Bound (UCB). Hence, optimistic initialization, despite its simplicity, can achieve logarithmic regret.</p> <p>However, if \\(Q^+\\) is set too optimistically, the agent may spend unnecessary time validating even poor arms. Conversely, if it is set insufficiently optimistically, some arms may not be explored at all. Therefore, the choice of \\(Q^+\\) must be made carefully, ideally based on prior knowledge of the reward bounds.</p>"},{"location":"course_notes/bandits/#upper-confidence-bound-ucb-algorithms-a-detailed-exploration","title":"Upper Confidence Bound (UCB) Algorithms: A Detailed Exploration","text":""},{"location":"course_notes/bandits/#introduction-and-motivation","title":"Introduction and Motivation","text":"<p>In the study of the exploration-exploitation dilemma in stochastic multi-armed bandit (MAB) problems, one of the most elegant and foundational strategies is the Upper Confidence Bound (UCB) algorithm. UCB embodies a principle of decision-making known as optimism in the face of uncertainty. This heuristic encourages an agent to behave optimistically about less-explored actions by constructing upper confidence bounds for their expected rewards and then selecting actions as if these bounds were true estimates of the actual value.</p> <p>The UCB framework is based on a rigorous statistical foundation: if we can form a high-probability upper bound on the true reward of each arm, then choosing the arm with the largest such bound encourages both exploitation of arms that are empirically promising and exploration of arms about which we remain uncertain. </p> <p>To put it simply: the algorithm behaves as if the true reward of each arm is the most optimistic plausible value consistent with the observed data. This naturally balances the dual needs of exploration (gathering information about uncertain arms) and exploitation (using the current best knowledge to make good decisions).</p>"},{"location":"course_notes/bandits/#problem-setup","title":"Problem Setup","text":"<p>We formalize the stochastic K-armed bandit setting as follows:</p> <ul> <li>Let \\(\\mathcal{A} = \\{1, 2, \\dots, K\\}\\) denote the set of \\(K\\) actions or arms.</li> <li>At each time step \\(t = 1, 2, \\dots, T\\), the agent selects an arm \\(A_t \\in \\mathcal{A}\\) and receives a reward \\(R_t \\in [0,1]\\) drawn from a fixed, unknown distribution associated with that arm.</li> <li>Let \\(\\mu_i = \\mathbb{E}[R_t \\mid A_t = i]\\) denote the true expected reward of arm \\(i\\).</li> <li>Let \\(\\mu^* = \\max_i \\mu_i\\) denote the value of the optimal arm.</li> <li>The goal is to minimize regret, defined as the expected difference between the reward accumulated by always playing the optimal arm and the reward collected by the algorithm:</li> </ul> \\[ R(T) = T\\mu^* - \\mathbb{E}\\left[\\sum_{t=1}^T R_t\\right] = \\sum_{i=1}^K \\Delta_i \\mathbb{E}[N_i(T)], \\] <p>where \\(\\Delta_i = \\mu^* - \\mu_i\\) and \\(N_i(T)\\) is the number of times arm \\(i\\) was selected up to time \\(T\\).</p>"},{"location":"course_notes/bandits/#optimism-in-the-face-of-uncertainty","title":"Optimism in the Face of Uncertainty","text":"<p>At the heart of UCB algorithms lies the idea of confidence intervals. Suppose for each arm \\(i\\), we maintain an estimate \\(\\hat{Q}_{t-1}(i)\\) of its true mean \\(\\mu_i\\), based on observed rewards. Alongside this estimate, we compute an upper confidence term \\(U_{t-1}(i)\\), such that with high probability:</p> \\[ \\mu_i \\leq \\hat{Q}_{t-1}(i) + U_{t-1}(i). \\] <p>Rather than just selecting the arm with the highest empirical mean, we select the arm with the highest upper bound, i.e.,</p> \\[ A_t = \\arg\\max_{i \\in \\mathcal{A}} \\hat{Q}_{t-1}(i) + U_{t-1}(i). \\] <p>This choice reflects optimism: we act as if each arm is as good as it could plausibly be, given the data so far. The key insight is that this encourages exploration of less-frequently pulled arms because their confidence intervals are wider (i.e., larger \\(U_{t-1}(i)\\)), and this naturally decreases as more data accumulates.</p>"},{"location":"course_notes/bandits/#derivation-via-hoeffdings-inequality","title":"Derivation via Hoeffding's Inequality","text":"<p>To construct the upper confidence term \\(U_{t-1}(i)\\), we rely on Hoeffding's inequality, a concentration bound for bounded random variables.</p>"},{"location":"course_notes/bandits/#hoeffdings-inequality","title":"Hoeffding\u2019s Inequality:","text":"<p>Let \\(X_1, ..., X_n\\) be i.i.d. random variables with values in \\([0, 1]\\), and let \\(\\bar{X}_n = \\frac{1}{n} \\sum_{j=1}^n X_j\\) be their sample mean. Then for any \\(u &gt; 0\\),</p> \\[ \\Pr\\left\\{ \\mathbb{E}[X] &gt; \\bar{X}_n + u \\right\\} \\leq e^{-2nu^2}. \\] <p>This inequality bounds the probability that the true mean exceeds the empirical mean by more than \\(u\\). Rearranging it gives us a confidence interval: with probability at least \\(1 - \\delta\\),</p> \\[ \\mathbb{E}[X] \\leq \\bar{X}_n + \\sqrt{\\frac{\\ln(1/\\delta)}{2n}}. \\] <p>We apply this inequality to each arm \\(i\\), where \\(X_j\\) is the reward from the \\(j\\)-th time we pulled arm \\(i\\), \\(\\bar{X}_n = \\hat{Q}_{t-1}(i)\\), and \\(n = N_{t-1}(i)\\) is the number of times we\u2019ve pulled arm \\(i\\).</p> <p>This leads us to define:</p> \\[ U_{t-1}(i) = \\sqrt{\\frac{\\ln(1/\\delta_{t-1})}{2N_{t-1}(i)}}, \\] <p>so that with high probability,</p> \\[ \\mu_i \\leq \\hat{Q}_{t-1}(i) + U_{t-1}(i). \\]"},{"location":"course_notes/bandits/#the-ucb1-algorithm","title":"The UCB1 Algorithm","text":"<p>To ensure the confidence holds for all time steps (so that the overall regret is bounded), we define a schedule for \\(\\delta_t\\) that decays with \\(t\\), e.g., \\(\\delta_t = 1/t^2\\) or \\(\\delta_t = 1/t^4\\). Plugging this into our formula gives the well-known UCB1 index:</p> \\[ \\text{UCB}_t(i) = \\hat{Q}_{t-1}(i) + \\sqrt{\\frac{2 \\ln t}{N_{t-1}(i)}}. \\] <p>Then the action selection rule is:</p> \\[ A_t = \\arg\\max_{i \\in \\mathcal{A}} \\left[ \\hat{Q}_{t-1}(i) + \\sqrt{\\frac{2 \\ln t}{N_{t-1}(i)}} \\right]. \\] <p>This algorithm guarantees that each arm is explored enough to maintain confidence, while also converging to exploiting the optimal arm.</p>"},{"location":"course_notes/bandits/#interpretation-and-intuition","title":"Interpretation and Intuition","text":"<p>This formula can be interpreted in two parts:</p> <ul> <li>Exploitation: \\(\\hat{Q}_{t-1}(i)\\) is the empirical mean of rewards \u2014 it represents our current best estimate.</li> <li>Exploration Bonus: \\(\\sqrt{\\frac{2 \\ln t}{N_{t-1}(i)}}\\) is large when the arm has been pulled only a few times (small \\(N\\)) or early in time (small \\(t\\)), and shrinks as \\(N\\) grows.</li> </ul> <p>Crucially, \\(\\ln t\\) grows slowly (logarithmically), so even at large time steps, the algorithm still occasionally re-explores arms with low \\(N_i\\). This ensures that no arm is neglected forever, and yet the frequency of exploration diminishes as the confidence in \\(\\hat{Q}\\) grows.</p>"},{"location":"course_notes/bandits/#regret-analysis-of-ucb1","title":"Regret Analysis of UCB1","text":"<p>The strength of UCB1 lies in its theoretical guarantees. Let \\(\\Delta_i = \\mu^* - \\mu_i\\) denote the suboptimality gap of arm \\(i\\).</p> <p>Auer et al. (2002) proved that:</p> \\[ \\mathbb{E}[N_i(T)] \\leq \\frac{8 \\ln T}{\\Delta_i^2} + O(1), \\] <p>meaning the number of times a suboptimal arm is pulled grows logarithmically with \\(T\\). This leads to the following bound on expected cumulative regret:</p> \\[ \\mathbb{E}[R(T)] = \\sum_{i: \\Delta_i &gt; 0} \\Delta_i \\mathbb{E}[N_i(T)] \\leq \\sum_{i: \\Delta_i &gt; 0} \\left( \\frac{8 \\ln T}{\\Delta_i} + O(\\Delta_i) \\right), \\] <p>which simplifies to:</p> \\[ \\mathbb{E}[R(T)] = O\\left( \\sum_{i: \\Delta_i &gt; 0} \\frac{\\ln T}{\\Delta_i} \\right) = O(\\ln T). \\] <p>This is order-optimal, matching the lower bound for regret in stochastic bandits up to constant factors. Importantly, this bound is problem dependent: larger suboptimality gaps \\(\\Delta_i\\) lead to fewer required explorations, and hence lower regret.</p>"},{"location":"course_notes/bandits/#thompson-sampling-bayesian-probability-matching","title":"Thompson Sampling (Bayesian Probability Matching)","text":"<p>Thompson Sampling (TS), originally proposed by William R. Thompson in 1933, is a foundational algorithm in the domain of sequential decision-making under uncertainty, particularly within the multi-armed bandit (MAB) framework. It embodies a Bayesian philosophy, maintaining a probabilistic belief about the reward-generating distribution of each arm and using this belief to guide arm selection.</p> <p>At its core, Thompson Sampling follows the principle of probability matching: it selects actions (i.e., arms) in proportion to the probability that each is the optimal choice, conditioned on observed data. This leads to a dynamic and adaptive strategy that naturally balances exploration (gathering information about uncertain arms) and exploitation (leveraging the current best guess to maximize reward).</p>"},{"location":"course_notes/bandits/#bayesian-framework","title":"Bayesian Framework","text":"<p>Suppose we have a stochastic K-armed bandit problem with arms indexed by \\(i=1,2,\\dots,K\\). Each arm \\(i\\) is associated with an unknown reward distribution parameterized by \\(\\theta_i\\), and our goal is to maximize cumulative reward over \\(T\\) rounds. At each time \\(t\\), the learner chooses an arm \\(A_t\\), observes a reward \\(R_t\\sim\\mathcal{D}_{A_t}\\), and updates their belief about the corresponding \\(\\theta_{A_t}\\).</p> <p>From a Bayesian standpoint, we place a prior distribution \\(p(\\theta_i)\\) over each \\(\\theta_i\\), and after observing data \\(\\mathcal{H}_t\\) (history up to time \\(t\\)), we update the posterior \\(p(\\theta_i|\\mathcal{H}_t)\\) via Bayes' theorem:</p> \\[ p(\\theta_i|\\mathcal{H}_t)=\\frac{p(\\mathcal{H}_t|\\theta_i)p(\\theta_i)}{p(\\mathcal{H}_t)} \\] <p>Thompson Sampling samples \\(\\tilde{\\theta}_i^{(t)}\\sim p(\\theta_i|\\mathcal{H}_t)\\) for each arm and chooses the arm with the highest sampled expected reward.</p> <p>Formally:</p> \\[ A_t=\\arg\\max_{i\\in\\{1,\\dots,K\\}}\\mathbb{E}[R_t|\\tilde{\\theta}_i^{(t)}] \\]"},{"location":"course_notes/bandits/#bernoulli-bandits-beta-bernoulli-model","title":"Bernoulli Bandits: Beta-Bernoulli Model","text":"<p>To make these ideas concrete, consider the special case where each arm yields Bernoulli rewards:</p> \\[ R_t\\in\\{0,1\\} \\quad \\text{with} \\quad R_t\\sim\\text{Bernoulli}(\\theta_i) \\] <p>We assume that the success probability \\(\\theta_i\\in[0,1]\\) is unknown. The natural conjugate prior for the Bernoulli distribution is the Beta distribution, defined as:</p> \\[ \\theta_i\\sim\\text{Beta}(\\alpha_i,\\beta_i), \\quad \\text{with density:} \\quad p(\\theta_i)=\\frac{\\theta_i^{\\alpha_i-1}(1-\\theta_i)^{\\beta_i-1}}{B(\\alpha_i,\\beta_i)} \\] <p>where \\(B(\\alpha,\\beta)\\) is the beta function (a normalization constant). The Beta distribution is flexible and allows us to express varying degrees of prior belief. For instance, the uninformative uniform prior is \\(\\text{Beta}(1,1)\\).</p>"},{"location":"course_notes/bandits/#algorithm-steps","title":"Algorithm Steps","text":"<p>At each round \\(t\\), the Thompson Sampling algorithm proceeds as follows:</p> <ol> <li>Posterior Sampling:    For each arm \\(i=1,\\dots,K\\), draw:</li> </ol> \\[ \\tilde{\\theta}_i^{(t)}\\sim\\text{Beta}(\\alpha_i,\\beta_i) \\] <ol> <li>Action Selection:    Choose the arm with the highest sampled value:</li> </ol> \\[ A_t=\\arg\\max_{i}\\tilde{\\theta}_i^{(t)} \\] <ol> <li> <p>Reward Observation:    Pull arm \\(A_t\\), observe reward \\(R_t\\in\\{0,1\\}\\)</p> </li> <li> <p>Posterior Update:    Update the Beta parameters:</p> </li> </ol> \\[ \\alpha_{A_t}\\leftarrow\\alpha_{A_t}+R_t, \\quad \\beta_{A_t}\\leftarrow\\beta_{A_t}+(1-R_t) \\] <p>The rest of the arms' parameters remain unchanged.</p> Intuition Behind Exploration and Exploitation <p>This process allows the algorithm to explore uncertain arms and exploit promising ones in a naturally balanced way. Consider an arm \\(i\\) with a high mean estimate but low certainty (wide posterior). There's a non-negligible chance that its sampled \\(\\tilde{\\theta}_i\\) will be large, leading to selection. Conversely, an arm with high empirical reward but tight posterior still occasionally gets out-sampled by a more uncertain one.</p> <p>This phenomenon is called randomized optimism: sometimes, by chance, an uncertain arm is sampled optimistically, leading to its exploration. The more we pull an arm, the narrower its posterior becomes, reducing unnecessary exploration over time.</p>"},{"location":"course_notes/bandits/#extension-beyond-bernoulli-rewards","title":"Extension Beyond Bernoulli Rewards","text":"<p>While the Beta-Bernoulli setup is particularly elegant due to its conjugacy (posterior is analytically tractable), Thompson Sampling extends naturally to other reward models:</p> <ul> <li>Gaussian rewards with unknown mean (known variance): Use a normal prior \\(\\theta_i\\sim\\mathcal{N}(\\mu_i,\\sigma_i^2)\\) </li> <li>Poisson rewards: Use a Gamma prior on the rate parameter \\(\\lambda_i\\) </li> <li>General likelihoods: Use approximate inference (e.g., Monte Carlo methods, variational inference)</li> </ul> <p>In non-conjugate or complex settings, one often resorts to sampling-based approximations of the posterior, such as particle filters or MCMC methods.</p>"},{"location":"course_notes/bandits/#regret-analysis_1","title":"Regret Analysis","text":"<p>Thompson Sampling was initially justified from a Bayesian perspective \u2014 minimizing expected regret under a prior over reward distributions. However, rigorous analysis has shown that TS also achieves strong frequentist guarantees.</p>"},{"location":"course_notes/bandits/#regret-definition","title":"Regret Definition","text":"<p>Let \\(\\mu_i=\\mathbb{E}[R_t|A_t=i]\\) be the expected reward of arm \\(i\\), and let \\(\\mu^*=\\max_i\\mu_i\\) be the optimal reward. Then the cumulative regret over \\(T\\) rounds is:</p> \\[ R(T)=T\\mu^*-\\sum_{t=1}^T\\mathbb{E}[\\mu_{A_t}] \\] <p>For Bernoulli bandits, let \\(\\Delta_i=\\mu^*-\\mu_i\\). Then under mild conditions, the expected regret of Thompson Sampling satisfies:</p> \\[ \\mathbb{E}[R(T)]=O\\left(\\sum_{i:\\Delta_i&gt;0}\\frac{\\ln T}{\\Delta_i^2}\\right) \\] <p>This bound is only slightly looser than the regret of UCB algorithms, which have regret scaling as \\(\\sum_i\\frac{\\ln T}{\\Delta_i}\\). Despite this, Thompson Sampling often outperforms UCB in practice due to better constant factors and more flexible adaptation.</p> <p>In fact, for many distributions, it has been shown that TS asymptotically matches the Lai\u2013Robbins lower bound:</p> \\[ \\liminf_{T\\to\\infty}\\frac{\\mathbb{E}[R(T)]}{\\ln T}\\geq\\sum_{i:\\Delta_i&gt;0}\\frac{\\Delta_i}{D(\\mu_i\\|\\mu^*)} \\] <p>where \\(D(p\\|q)\\) is the Kullback\u2013Leibler divergence between the reward distributions of arms \\(i\\) and the optimal arm.</p>"},{"location":"course_notes/bandits/#contextual-bandits","title":"Contextual Bandits","text":"<p>While standard multi-armed bandits assume no additional data or \u201ccontext\u201d is available when selecting an arm, many real-world applications present extra information\u2014sometimes called features or context\u2014that can help guide the choice of action. This setting is known as a contextual bandit or bandit with side information.</p>"},{"location":"course_notes/bandits/#motivation-and-setup","title":"Motivation and Setup","text":"<p>In a contextual bandit problem, at each time step \\(t\\):</p> <ol> <li>The environment reveals a context \\(x_t \\in \\mathcal{X}\\).  </li> <li>Based on this context, the agent chooses an action (arm) \\(A_t \\in \\{1, \\dots, K\\}\\).  </li> <li>The chosen action yields a reward \\(R_t\\), drawn from a distribution that can depend on both the action and the context.</li> </ol> <p>Formally, we might write:</p> \\[ R_t \\sim \\mathcal{R}\\bigl(a = A_t, x = x_t\\bigr) \\] <p>Here, \\(\\mathcal{X}\\) is a (possibly high-dimensional) space of contexts. The agent\u2019s goal remains to maximize cumulative reward (or minimize regret), but now it can exploit the relationship between (context, action) and reward.</p>"},{"location":"course_notes/bandits/#distinction-from-standard-mab","title":"Distinction from Standard MAB","text":"<ul> <li>In standard MAB, the same arms are offered in every round, with no side information, and each arm has a single reward distribution.  </li> <li>In contextual bandits, each arm\u2019s reward distribution changes depending on the context \\(\\(x\\)\\). The agent must learn a context-to-action mapping (a policy) that predicts which arm will perform best in each situation.</li> </ul>"},{"location":"course_notes/bandits/#example-use-cases","title":"Example Use Cases","text":"<ul> <li>News Article Recommendation </li> <li>Personalized Medicine </li> <li>Targeted Advertising</li> </ul>"},{"location":"course_notes/bandits/#linucb-algorithm","title":"LinUCB Algorithm","text":"<p>One of the canonical and most influential approaches to contextual bandits is the LinUCB algorithm. LinUCB is designed for problems where the reward can be assumed (or approximated) to be a linear function of the context. This approach was popularized by Li et al. (2010), where it was used for news article recommendation; see also the original UCB framework by Auer et al. (2002) for the theoretical underpinnings of confidence-bound methods.</p>"},{"location":"course_notes/bandits/#linear-contextual-model","title":"Linear Contextual Model","text":"<p>Assume the reward from arm \\(i\\) when context \\(x_t \\in \\mathbb{R}^d\\) is presented has an expected value of the form:</p> \\[ \\mathbb{E}[R_t \\mid x_t, A_t = i] = x_t^\\top \\theta_i, \\] <p>where \\(\\theta_i \\in \\mathbb{R}^d\\) is an unknown weight vector for arm \\(i\\). Each arm \\(i\\) thus corresponds to a particular linear relationship between context and reward. In practice, this means if you have a \\(d\\)-dimensional feature vector \\(x_t\\) representing the context at time \\(t\\), the arm\u2019s expected payoff is captured by the dot product between \\(x_t\\) and the parameter vector \\(\\theta_i\\).</p> <ul> <li>Why linearity? The assumption of linearity often arises from modeling each component of \\(x_t\\) as contributing additively to the reward. While real-world relationships may be more complex, linear approximations can be quite effective in high-dimensional settings, especially when combined with feature engineering.</li> </ul>"},{"location":"course_notes/bandits/#algorithm-structure","title":"Algorithm Structure","text":"<p>At a high level, LinUCB maintains an estimate \\(\\hat{\\theta}_i\\) for each arm \\(i\\). To account for uncertainty in \\(\\hat{\\theta}_i\\), it constructs an upper confidence bound for the expected reward of each arm, thereby balancing exploration and exploitation (see Abbasi-Yadkori et al. (2011) for in-depth theoretical analysis of this confidence set approach).</p> <ol> <li>Initialization (for each arm \\(i\\)):</li> <li>\\(A_i = I_{d\\times d}\\) (identity matrix)  </li> <li>\\(b_i = 0\\) (zero vector in \\(\\mathbb{R}^d\\))</li> </ol> <p>Here, \\(A_i\\) and \\(b_i\\) can be understood in terms of ridge regression: they will accumulate the contextual data and observed rewards for arm \\(i\\), respectively.</p> <ol> <li>At time \\(t\\), upon receiving context \\(x_t\\):</li> <li>For each arm \\(i\\):</li> </ol> \\[ \\hat{\\theta}_i = A_i^{-1} \\, b_i \\] \\[ p_i(t) = x_t^\\top \\hat{\\theta}_i + \\alpha \\sqrt{x_t^\\top A_i^{-1} x_t} \\] <p>where $ \\alpha $ is an exploration parameter controlling how \u201coptimistic\u201d the estimate is, and $ \\sqrt{x_t^\\top A_i^{-1} x_t} $ measures uncertainty in the linear reward estimate. The larger this term, the less data we have for arm \\(i\\) under similar contexts, so the algorithm encourages exploration of that arm.</p> <ul> <li>Select arm \\(A_t = \\arg\\max_i \\; p_i(t)\\).</li> </ul> <p>Intuitively, \\(p_i(t)\\) combines the current best guess (\\(x_t^\\top \\hat{\\theta}_i\\)) with a statistical bonus (\\(\\alpha \\sqrt{x_t^\\top A_i^{-1} x_t}\\)). This reflects the principle of optimism in the face of uncertainty: an action with limited data is given a higher \u201coptimistic\u201d estimate, prompting additional exploration.</p> <ol> <li>Observe reward \\(R_t\\). Update:</li> </ol> \\[ A_{A_t} \\leftarrow A_{A_t} + x_t x_t^\\top,  \\quad b_{A_t} \\leftarrow b_{A_t} + R_t \\, x_t. \\] <p>These updates are analogous to incrementally solving a regularized least-squares problem for each arm\u2019s parameters. After enough pulls, \\(A_i\\) becomes well-conditioned, shrinking the confidence interval in \\(p_i(t)\\) for arm \\(i\\).</p>"},{"location":"course_notes/bandits/#usage-of-linucb-in-contextual-bandits","title":"Usage of LinUCB in Contextual Bandits","text":"<p>LinUCB is particularly effective when the context-reward relationship is (or is close to) linear. It scales well to large time horizons so long as the context dimension \\(d\\) is not too large, because the key matrix inverse \\(A_i^{-1}\\) is only \\(d \\times d\\).</p> <ul> <li>Feature Construction: If the raw context is not linear, one can often use polynomial or kernel feature mappings to approximate non-linear relationships within a higher-dimensional linear model.  </li> <li>Hyperparameter Tuning: The exploration parameter $ \\alpha $ often requires careful tuning (or theoretically derived values) to ensure a good balance of exploration and exploitation.  </li> <li>Practical Extensions: Variants of LinUCB can incorporate regularization parameters, discount old data for nonstationary environments, and use approximate matrix updates for very large \\(d\\).</li> </ul> <p>For more details and practical insights, see: - Li et al. (2010) for the original application to personalized news recommendation. - Chapelle &amp; Li (2011) for empirical comparisons of bandit algorithms (including LinUCB). - Auer (2002) for the foundational UCB concept.</p>"},{"location":"course_notes/bandits/#regret-analysis-of-linucb","title":"Regret Analysis of LinUCB","text":"<p>Under standard assumptions (linear rewards, bounded noise), LinUCB achieves sublinear regret in the order of \\(O(d \\sqrt{T} \\ln T)\\). As \\(T\\) grows, average per-step regret goes to zero, indicating the algorithm efficiently balances exploration and exploitation in a theoretically rigorous manner.</p> <ul> <li>High-Level Idea: The quantity \\(x_t^\\top A_i^{-1} x_t\\) can be interpreted as capturing how much \u201cnew information\u201d the context \\(x_t\\) provides about arm \\(i\\). Once \\(A_i\\) becomes large and well-inverted, the algorithm is confident in its parameter estimates, reducing the exploration term.  </li> <li>Practical Interpretation: In simpler terms, each arm \\(i\\) fits a linear predictor \\(\\hat{\\theta}_i\\) by \u201ccollecting\u201d relevant \\((x_t, R_t)\\) pairs. With enough data, the algorithm zeroes in on the optimal linear function.</li> </ul> <p>For a formal derivation of these regret bounds, one can consult:</p> <ul> <li>Abbasi-Yadkori et al. (2011) \u2014 detailed proofs for linear bandits\u2019 regret bounds.  </li> <li>Bouneffouf et al. (2020) \u2014 comprehensive survey on bandits, including contextual and linear settings.</li> </ul>"},{"location":"course_notes/bandits/#thompson-sampling-in-contextual-bandits","title":"Thompson Sampling in Contextual Bandits","text":""},{"location":"course_notes/bandits/#overview","title":"Overview","text":"<p>Thompson Sampling (TS) can also be extended to contextual bandits by placing a prior over each arm\u2019s parameter vector and updating that posterior after each interaction. Similar to standard TS, it selects arms by sampling from this posterior and picking the arm whose sampled parameter suggests the highest reward given the current context.</p>"},{"location":"course_notes/bandits/#usage-of-thompson-sampling-algorithm-in-contextual-bandits","title":"Usage of Thompson Sampling Algorithm in Contextual Bandits","text":"<ol> <li>Model Specification: Assume a prior distribution over each arm\u2019s parameter \\(\\theta_i\\) (e.g., Gaussian for linear models).  </li> <li>At Each Round \\(t\\):</li> <li>Observe context \\(x_t\\) .  </li> <li>Sample \\(\\tilde{\\theta}_i\\) from the posterior for each arm \\(i\\) .  </li> <li>Compute \\(\\tilde{r}_i(t) = x_t^\\top \\tilde{\\theta}_i\\) .  </li> <li>Select \\(A_t = \\arg\\max_i \\tilde{r}_i(t)\\) .  </li> <li>Observe reward \\(R_t\\) .  </li> <li>Update the posterior of \\(\\theta_{A_t}\\) .</li> </ol>"},{"location":"course_notes/bandits/#regret-analysis-of-thompson-sampling-in-contextual-bandits","title":"Regret Analysis of Thompson Sampling in Contextual Bandits","text":"<p>With similar assumptions to LinUCB, contextual Thompson Sampling attains comparable \\(O(\\sqrt{T})\\) -type regret bounds, often with good empirical results due to its Bayesian \u201cprobability matching\u201d mechanism.</p>"},{"location":"course_notes/bandits/#conclusion","title":"Conclusion","text":"<p>The multi-armed bandit (MAB) problem encapsulates a fundamental tension inherent to sequential decision-making under uncertainty: the exploration\u2013exploitation trade-off. Despite its conceptual simplicity\u2014consisting solely of a set of actions with unknown reward distributions\u2014the MAB framework reveals rich theoretical structures and remains deeply relevant across a wide array of real-world applications.</p> <p>This chapter began with a formalization of the stochastic bandit setting, introducing key constructs such as action-value functions, sample-average estimation, and the non-associativity property, which distinguishes MABs from general Markov Decision Processes (MDPs) by eliminating the influence of state transitions. The core objective was established as the maximization of cumulative reward, equivalently viewed through the lens of regret minimization.</p> <p>To this end, various algorithmic strategies were introduced for estimating action values and managing the trade-off between exploration and exploitation:</p> <ul> <li> <p>Sample-average and incremental update rules form the foundation for value estimation in stationary environments, while constant step-size updates extend applicability to nonstationary settings through exponential weighting of recent observations.</p> </li> <li> <p>The notion of regret, both instantaneous and cumulative, provides a principled metric for evaluating the performance of bandit algorithms. Analytical decompositions reveal that total regret depends critically on the gap between suboptimal and optimal actions and the frequency with which suboptimal actions are selected.</p> </li> <li> <p>Baseline strategies such as \u03b5-greedy and optimistic initial values offer intuitive approaches to exploration, though \u03b5-greedy with a constant exploration rate incurs linear regret. Improvements can be achieved through decaying exploration schedules or more principled algorithms.</p> </li> <li> <p>Upper Confidence Bound (UCB) methods exemplify the \"optimism in the face of uncertainty\" paradigm by using high-probability confidence intervals to balance learning and exploitation. These methods offer logarithmic regret bounds, matching the Lai\u2013Robbins lower bound for stochastic settings.</p> </li> <li> <p>Thompson Sampling, rooted in Bayesian inference and probability matching, introduces a powerful and flexible framework for balancing exploration and exploitation. It often performs competitively with UCB both in theory and practice, and generalizes well across reward models.</p> </li> <li> <p>Extensions to the contextual bandit setting further elevate the practical relevance of MABs. By incorporating side information or features, algorithms such as LinUCB and contextual Thompson Sampling dynamically adapt action choices based on observed context, effectively learning context-to-action policies with provably sublinear regret.</p> </li> </ul> <p>In summary, the MAB framework offers a minimal yet powerful model that lies at the heart of many online learning and reinforcement learning scenarios. The theoretical underpinnings, from regret analysis to optimal exploration policies, provide valuable tools for designing adaptive systems. Simultaneously, the algorithmic developments discussed herein continue to form the basis of modern intelligent agents operating in uncertain, real-time environments.</p>"},{"location":"course_notes/bandits/#authors","title":"Author(s)","text":"<ul> <li> <p>Arshia Gharooni</p> <p>Teaching Assistant</p> <p>arshiyagharoony@gmail.com</p> <p> </p> </li> <li> <p>Mohammad Mohammadi</p> <p>Teaching Assistant</p> <p>mohammadm97i@gmail.com</p> <p> </p> </li> <li> <p>Hesam Hosseini</p> <p>Teaching Assistant</p> <p>hesam138122@gmail.com</p> <p> </p> </li> </ul>"},{"location":"course_notes/bandits/#references","title":"References","text":"<ol> <li> <p>The RL Hub MAB Chapters</p> </li> <li> <p>The Multi-Armed Bandit Problem and Its Solutions</p> </li> <li> <p>How to make decisions in a bandit game?</p> </li> <li> <p>Lower bounds on regret for multi-armed bandits.</p> </li> <li> <p>Continuous Intelligence with Contextual Bandits</p> </li> <li> <p>Reinforcement Learning (BartoSutton)</p> </li> </ol>"},{"location":"course_notes/exploration/","title":"Exploration Methods","text":""},{"location":"course_notes/hierarchical/","title":"Hierarchical RL","text":""},{"location":"course_notes/imitation/","title":"Imitation Learning","text":""},{"location":"course_notes/intro-to-phase2/","title":"Introduction to RL in Depth","text":""},{"location":"course_notes/intro-to-phase2/#goal","title":"goal","text":"<p>get deeper into the concepts. learn new math that are usfull for RL </p>"},{"location":"course_notes/intro-to-phase2/#overview","title":"overview","text":""},{"location":"course_notes/intro-to-phase2/#value-iteration","title":"value iteration","text":""},{"location":"course_notes/intro-to-phase2/#policy-iterration","title":"policy iterration","text":""},{"location":"course_notes/intro-to-phase2/#liptishtness","title":"liptishtness","text":""},{"location":"course_notes/intro-to-phase2/#contraction-mapping","title":"contraction mapping","text":""},{"location":"course_notes/intro-to-phase2/#ddpg","title":"DDPG","text":""},{"location":"course_notes/intro-to-phase2/#natural-gradient","title":"natural gradient","text":""},{"location":"course_notes/intro-to-phase2/#policy-gradient","title":"policy gradient","text":""},{"location":"course_notes/intro-to-phase2/#kl","title":"kl","text":""},{"location":"course_notes/intro-to-phase2/#trpo-theory","title":"TRPO theory","text":""},{"location":"course_notes/intro-to-phase2/#sac-theory","title":"SAC theory","text":""},{"location":"course_notes/intro-to-phase2/#conceteration-bound","title":"conceteration bound","text":""},{"location":"course_notes/intro-to-phase2/#hofdding","title":"hofdding","text":""},{"location":"course_notes/intro-to-phase2/#regret-bound","title":"regret bound","text":""},{"location":"course_notes/intro-to-phase2/#ucb","title":"ucb","text":""},{"location":"course_notes/intro-to-phase2/#notaion","title":"notaion","text":"<p>here is a the notation we will use thgoht this part:</p>"},{"location":"course_notes/intro-to-phase2/#prequestions","title":"prequestions","text":"<p>you may need to know the follwing concepts two better undestad this part:</p>"},{"location":"course_notes/intro-to-rl/","title":"Week 1: Introduction to RL","text":""},{"location":"course_notes/intro-to-rl/#why-do-we-need-reinforcement-learning","title":"Why Do We Need Reinforcement Learning?","text":"<p>Reinforcement Learning (RL) is a subfield of artificial intelligence (AI) that focuses on training agents to make sequences of decisions by interacting with an environment. Unlike other AI methods, RL is particularly well-suited for problems where the agent must learn optimal behavior through trial and error, often in dynamic and uncertain environments.</p>"},{"location":"course_notes/intro-to-rl/#limitations-of-other-ai-methods","title":"Limitations of Other AI Methods","text":"<ol> <li>Supervised Learning: </li> <li>What it does: Supervised learning requires a labeled dataset where the correct output (label) is provided for each input. The model learns to map inputs to outputs based on this data.</li> <li> <p>Limitation: In many real-world scenarios, obtaining a labeled dataset is impractical or impossible. For example, consider training a robot to walk. It's not feasible to provide a labeled dataset of all possible states and actions the robot might encounter.</p> </li> <li> <p>Unsupervised Learning:</p> </li> <li>What it does: Unsupervised learning deals with unlabeled data and tries to find hidden patterns or intrinsic structures within the data.</li> <li> <p>Limitation: While unsupervised learning can identify patterns, it doesn't provide a way to make decisions or take actions based on those patterns. For instance, clustering similar states in a game doesn't tell the agent how to win the game.</p> </li> <li> <p>Traditional Control Methods:</p> </li> <li>What it does: Traditional control methods are designed to maintain a system's state within a desired range using predefined rules.</li> <li>Limitation: These methods require a precise model of the environment and are not adaptable to complex, changing environments.</li> </ol>"},{"location":"course_notes/intro-to-rl/#where-reinforcement-learning-shines","title":"Where Reinforcement Learning Shines","text":"<p>Reinforcement Learning excels in scenarios where:</p> <ul> <li> <p>No Labeled Data is Available: RL agents learn by interacting with the environment and receiving feedback in the form of rewards or penalties. This eliminates the need for a pre-labeled dataset.</p> </li> <li> <p>Sequential Decision-Making is Required: RL is designed to handle problems where decisions are made in a sequence, and each decision affects the future state of the environment. For example, in a game like Go or Chess, each move affects the board's state and influences future moves.</p> </li> <li> <p>The Environment is Dynamic and Uncertain: RL agents can adapt to changing environments and learn optimal policies even when the environment is not fully known or is stochastic. For instance, an RL agent can learn to navigate a maze even if the maze's layout changes over time.</p> </li> <li> <p>Non-i.i.d. Data: RL is capable of handling non-independent and identically distributed (non-i.i.d.) data. In many real-world scenarios, data points are not independent (e.g., the state of the environment at one time step depends on previous states) and may not be identically distributed (e.g., the distribution of states changes over time). A notable example is robotic control, where an autonomous robot learns to walk. Each movement directly affects the next state, and the terrain may change dynamically, requiring the RL agent to adapt its policy based on sequential dependencies. RL agents can learn from such data by considering the temporal dependencies and adapting to the evolving data distribution.</p> </li> <li> <p>Lack of Input Data: In supervised learning, the model is provided with input-output pairs \\((x, y)\\), where \\(x\\) is the input data and \\(y\\) is the corresponding label. In RL, not only are the labels (correct actions) not provided, but the \"\\(x\\)\"s (input states) are also not explicitly given to the model. The agent must actively interact with the environment to observe and collect these states.</p> </li> <li> <p>Example: Consider training an RL agent to play a video game. In supervised learning, you would need a dataset of game states (\\(x\\)) and the corresponding optimal actions (\\(y\\)). In RL, the agent starts with no prior knowledge of the game states or actions. It must explore the game environment, observe the states, and learn which actions lead to higher rewards. This process of discovering and learning from the environment is what makes RL uniquely powerful for tasks where the input data is not readily available.</p> </li> </ul>"},{"location":"course_notes/intro-to-rl/#key-difference-between-rl-and-supervised-learning","title":"Key Difference Between RL and Supervised Learning","text":"<p>The primary distinction between RL and supervised learning is that, while feedback is provided in RL, the exact correct answer or action is not explicitly given. Let's delve deeper into this concept and explore the importance of exploration in RL, along with its challenges.</p>"},{"location":"course_notes/intro-to-rl/#supervised-learning","title":"Supervised Learning:","text":"<ul> <li>In supervised learning, you are presented with a bunch of data and told exactly what the answer for each data point is. Your model adjusts its parameters to get the prediction right for the data you trained on, and the goal is to generalize to unseen data, but is doesn't try to do better than the data.</li> <li> <p>Example: In a image classification task, the model is given images along with their correct labels (e.g., \"cat\" or \"dog\"). The model learns to predict the label for new, unseen images based on this labeled data.</p> </li> <li> <p>Imitation Learning: A specialized form of supervised learning used in decision-making problems is Imitation Learning (IL). In IL, the model learns by mimicking expert demonstrations. The training data consists of state-action pairs provided by an expert, and the model learns to replicate the expert's behavior. Unlike traditional supervised learning, IL is applied to sequential decision-making tasks, making it a bridge between supervised learning and RL.</p> </li> </ul>"},{"location":"course_notes/intro-to-rl/#reinforcement-learning","title":"Reinforcement Learning","text":"<ul> <li> <p>Agent-Environment Interaction: In RL, the agent interacts with the environment, takes actions, receives rewards, and adapts its policy based on these rewards. However, unlike supervised learning, the agent is never told which action was the right one for a given state or what the correct policy is for a given task. In other words, there are no labels! The agent must learn the optimal actions using the learning signals provided by the reward.</p> </li> <li> <p>Exploration: Exploration is a critical component of RL. Since the agent is not provided with labeled data or explicit instructions, it must explore the environment to discover which actions yield the highest rewards. This exploration allows the agent to learn from its experiences and improve its policy over time. Without exploration, the agent might get stuck in suboptimal behaviors, never discovering better strategies.</p> </li> <li> <p>Drawbacks and Difficulties of Exploration: While exploration is essential, it comes with its own set of challenges. One major difficulty is that wrong exploration can lead to poor learning outcomes. If the agent explores suboptimal or harmful actions excessively, it may reinforce bad behaviors, leading to a failure in learning the optimal policy. This is often summarized by the phrase \"garbage in, garbage out\"\u2014if the agent explores poorly and collects low-quality data, the resulting policy will also be of low quality.</p> </li> <li> <p>Example: Consider an RL agent learning to navigate a maze. If the agent spends too much time exploring dead ends or repeatedly taking wrong turns, it may fail to discover the correct path to the goal. The agent's policy will be based on these poor explorations, resulting in a suboptimal or even failing strategy.</p> </li> <li> <p>Balancing Exploration and Exploitation: A key challenge in RL is balancing exploration (trying new actions to discover their effects) and exploitation (using known actions that yield high rewards). Too much exploration can lead to inefficiency, while too much exploitation can cause the agent to miss out on better strategies. Techniques like epsilon-greedy policies and Thompson sampling are often used to strike this balance.</p> </li> <li> <p>Outperforming Human Intelligence: Despite the challenges, when exploration is done effectively, the agent can discover novel strategies and solutions that humans might not consider. Since the data is collected by the agent itself through exploration, an RL agent can even outperform human intelligence and execute impressive actions that no one has thought of before.</p> </li> <li> <p>Example: In a game of chess, the RL agent might receive a reward for winning the game but won't be told which specific move led to the victory. It must figure out the sequence of optimal moves through trial and error. By exploring different moves and learning from the outcomes, the agent can develop a strategy that maximizes its chances of winning. However, if the agent explores ineffective moves too often, it may fail to learn a winning strategy.</p> </li> </ul>"},{"location":"course_notes/intro-to-rl/#example-autonomous-driving","title":"Example: Autonomous Driving","text":"<p>Consider the task of autonomous driving. </p> <ul> <li>Supervised Learning Approach: You would need a massive labeled dataset of all possible driving scenarios, including rare events like a child running into the street. This is impractical.</li> <li> <p>Imitation Learning: In autonomous driving, IL can be used to train a model by observing human drivers. The model is provided with data on how human drivers react in various driving scenarios (e.g., steering, braking, accelerating). The model learns to mimic these actions, effectively reducing the problem to a supervised learning task. However, the model's performance is limited by the quality of the expert demonstrations and may not discover strategies that outperform the expert.</p> </li> <li> <p>Reinforcement Learning Approach: An RL agent can learn to drive by interacting with a simulated environment. It receives rewards for safe driving and penalties for accidents or traffic violations. Over time, the agent learns an optimal policy for driving without needing a labeled dataset.</p> </li> </ul>"},{"location":"course_notes/intro-to-rl/#key-concepts-in-reinforcement-learning","title":"Key Concepts in Reinforcement Learning","text":"<p>To understand RL, it's essential to grasp some fundamental concepts: state, action, reward, and policy. Let's explore each of these concepts in detail, using an example to illustrate their roles.</p>"},{"location":"course_notes/intro-to-rl/#1-state-mathbfs_t","title":"1. State (\\(\\mathbf{s}_t\\))","text":"<ul> <li>Definition: A state represents the current situation or configuration of the environment at a given time. It encapsulates all the relevant information that the agent needs to make a decision.</li> <li>Example: Consider a self-driving car navigating through a city. The state could include the car's current position, speed, the positions of other vehicles, traffic lights, and pedestrians. All these factors together define the state of the environment at any moment.</li> </ul>"},{"location":"course_notes/intro-to-rl/#2-action-mathbfa_t","title":"2. Action (\\(\\mathbf{a}_t\\))","text":"<ul> <li>Definition: An action is a decision or move made by the agent that affects the environment. The set of all possible actions that an agent can take is called the action space.</li> <li>Example: In the self-driving car scenario, possible actions might include accelerating, braking, turning left, turning right, or maintaining the current speed. Each action changes the state of the environment, such as the car's position or speed.</li> </ul>"},{"location":"course_notes/intro-to-rl/#3-reward-r_t","title":"3. Reward (\\(r_t\\))","text":"<ul> <li> <p>Definition: A reward is a feedback signal that the agent receives from the environment after taking an action. The reward indicates the immediate benefit or cost of the action taken. The goal of the agent is to maximize the cumulative reward over time. The reward can be provided by an expert or learned from demonstrations. This can be achieved by directly copying observed behavior or inferring rewards from observed behavior (Inverse RL) .</p> </li> <li> <p>Example: For the self-driving car, rewards could be assigned as follows:</p> </li> <li>Positive Reward: Reaching the destination safely (+100), obeying traffic rules (+10).</li> <li> <p>Negative Reward: Colliding with another vehicle (-100), running a red light (-50).</p> </li> <li> <p>Sparse Reward Environments</p> </li> <li> <p>Definition: In sparse reward environments, the agent receives rewards very infrequently. Instead of getting feedback after every action, the agent might only receive a reward after completing a long sequence of actions or achieving a significant milestone. For example, in a game, the agent might only receive a reward upon winning or losing, with no feedback provided during the game.</p> </li> <li> <p>Challenges:</p> <ul> <li>Difficulty in Learning: Sparse rewards make it challenging for the agent to learn which actions lead to positive outcomes. Since the agent receives little to no feedback during most of its interactions, it struggles to associate specific actions with rewards. This can lead to slow or ineffective learning.</li> <li>Exploration: In sparse reward environments, the agent must explore extensively to discover actions that yield rewards. Without frequent feedback, the agent may take a long time to stumble upon the correct sequence of actions, making the learning process inefficient.</li> <li>Credit Assignment Problem: Determining which actions in a sequence contributed to a reward is difficult in sparse reward settings. The agent may not be able to accurately attribute the reward to the correct actions, leading to suboptimal policies.</li> </ul> </li> <li> <p>Example: Consider an RL agent learning to play a complex strategy game where the only reward is given at the end of the game (e.g., +1 for winning and -1 for losing). The agent must explore countless moves and strategies without any intermediate feedback, making it challenging to learn effective strategies. The agent might take a very long time to discover the sequence of actions that leads to a win.</p> </li> </ul>"},{"location":"course_notes/intro-to-rl/#4-policy-pi_theta","title":"4. Policy (\\(\\pi_{\\theta}\\))","text":"<ul> <li>Definition: A policy is a strategy or set of rules that the agent follows to decide which action to take in a given state. It maps states to actions and can be deterministic (always choosing a specific action in a state) or stochastic (choosing actions based on a probability distribution).</li> <li>Example: In the self-driving car example, a policy might dictate that the car should slow down when it detects a pedestrian crossing the street or stop when it encounters a red traffic light. The policy is what the agent learns and optimizes to maximize cumulative rewards.</li> </ul>        Fig1. RL Framework"},{"location":"course_notes/intro-to-rl/#the-anatomy-of-reinforcement-learning","title":"The Anatomy of Reinforcement Learning","text":"<p>An RL agent's training is done through an interactive process between the agent and the environment. This process involves several key steps that allow the agent to learn and improve its decision-making capabilities. </p>"},{"location":"course_notes/intro-to-rl/#1-evaluate-the-action-how-well-was-our-choice-of-x-y-z-execute-gripping","title":"1. Evaluate the Action: \"How well was our choice of (x, y, z); execute gripping!\"","text":"<ul> <li>This step involves assessing the effectiveness of the action taken by the agent. The agent performs an action, such as gripping an object at a specific location (x, y, z), and then evaluates how well this action was executed.</li> </ul>"},{"location":"course_notes/intro-to-rl/#2-fit-a-model","title":"2. Fit a Model","text":"<ul> <li>Fitting a model refers to creating a representation of the environment or the task based on the data collected from interactions. This model helps the agent predict the outcomes of future actions and understand the dynamics of the environment.</li> </ul>"},{"location":"course_notes/intro-to-rl/#3-estimate-the-return-gripping-objects-estimate-the-return","title":"3. Estimate the Return: \"Gripping objects estimate the return\"","text":"<ul> <li>Estimating the return involves calculating the expected cumulative reward from a given state or action. The return is a key concept in RL, as the agent aims to maximize this cumulative reward over time.</li> </ul>"},{"location":"course_notes/intro-to-rl/#4-understand-physical-consequences-or-when-i-grip-some-object-on-a-specific-location-what-happens-physically-ie-how-its-x-y-zs-change","title":"4. Understand Physical Consequences: \"Or ... when I grip some object on a specific location, what happens physically; i.e. how its (x, y, z)'s change?\"","text":"<ul> <li>This step emphasizes the importance of understanding the physical consequences of actions. The agent needs to know how its actions (like gripping an object at a specific location) affect the environment, particularly the object's position (x, y, z).</li> </ul>"},{"location":"course_notes/intro-to-rl/#5-improve-the-policy-improve-the-policy-best-guess","title":"5. Improve the Policy: \"Improve the policy 'best guess'\"","text":"<ul> <li>Improving the policy involves refining the agent's strategy for choosing actions based on the feedback received from the environment. The \"best guess\" refers to the agent's current understanding of the optimal actions, which is continuously updated as the agent learns.</li> </ul>        Fig2. The Anatomy of RL"},{"location":"course_notes/intro-to-rl/#comparing-reinforcement-learning-with-other-learning-methods","title":"Comparing Reinforcement Learning with Other Learning Methods","text":"<p>To better understand the unique characteristics of Reinforcement Learning, it's helpful to compare it with other learning methods. The table below provides a comparison of RL with other approaches such as Supervised Learning (SL), Unsupervised Learning (UL), and Imitation Learning (IL). Let's break down the table and discuss the key differences and similarities.</p>"},{"location":"course_notes/intro-to-rl/#comparison-table","title":"Comparison Table","text":"AI Planning SL UL RL IL Optimization X X X Learns from experience X X X X Generalization X X X X X Delayed Consequences X X X Exploration X"},{"location":"course_notes/intro-to-rl/#key-concepts","title":"Key Concepts","text":"<ol> <li> <p>Optimization</p> <ul> <li>AI Planning: Involves optimizing a sequence of actions to achieve a goal.</li> <li>RL: Focuses on optimizing policies to maximize cumulative rewards.</li> <li>IL: Also involves optimization, often by learning from demonstrations.</li> </ul> </li> <li> <p>Learns from Experience</p> <ul> <li>SL: Learns from labeled data, where each input has a corresponding output.</li> <li>UL: Learns from unlabeled data by identifying patterns.</li> <li>RL: Learns by interacting with the environment and receiving feedback.</li> <li>IL: Learns from demonstrations of good policies.</li> </ul> </li> <li> <p>Generalization</p> <ul> <li>All methods (AI Planning, SL, UL, RL, IL) aim to generalize from training data to new, unseen situations.</li> </ul> </li> <li> <p>Delayed Consequences</p> <ul> <li>AI Planning: Considers the long-term effects of actions.</li> <li>RL: Takes into account the delayed consequences of actions to maximize future rewards.</li> <li>IL: Can also consider delayed consequences if the demonstrations include long-term strategies.</li> </ul> </li> <li> <p>Exploration</p> <ul> <li>RL: Requires exploration of the environment to discover optimal policies.</li> <li>Other methods (SL, UL, IL) typically do not involve exploration in the same way.</li> <li>In particular, IL is limited to the data and experiences provided by the expert model. Since IL relies on demonstrations, it cannot leverage the benefits of exploration to discover better strategies beyond what the expert has demonstrated. In contrast, RL has the capability to explore and learn from trial and error, allowing it to outperform expert demonstrations in some cases by discovering novel, more efficient policies.</li> </ul> </li> </ol>"},{"location":"course_notes/intro-to-rl/#planning-vs-learning","title":"Planning vs Learning","text":"<p>Two fundamental problems in sequential decision making:</p> <ol> <li> <p>Reinforcement learning:</p> <ul> <li>The environment is initially unknown</li> <li>The agent interacts with the environment</li> <li>The agent improves its policy</li> </ul> </li> <li> <p>Planning:</p> <ul> <li>A model of the environment is known</li> <li>The agent performs computations with its model (without any external interaction)</li> <li>The agent improves its policy, a.k.a. deliberation, reasoning, introspection, pondering, thought, search </li> </ul> </li> </ol>"},{"location":"course_notes/intro-to-rl/#introduction-to-markov-decision-processes-mdps","title":"Introduction to Markov Decision Processes (MDPs)","text":"<p>Markov Decision Processes (MDPs) are a fundamental framework used in Reinforcement Learning to model decision-making problems. MDPs provide a mathematical foundation for describing an environment in which an agent interacts, takes actions, and receives rewards. Let's break down the components of an MDP.</p>"},{"location":"course_notes/intro-to-rl/#components-of-an-mdp","title":"Components of an MDP","text":"<p>An MDP is defined by the following components:</p> <ol> <li> <p>Set of States (\\(S\\)): </p> <ul> <li>The set of all possible states that the environment can be in. A state \\(s \\in S\\) represents a specific configuration or situation of the environment at a given time.</li> </ul> </li> <li> <p>Set of Actions (\\(A\\)): </p> <ul> <li>The set of all possible actions that the agent can take. An action \\(a \\in A\\) is a decision made by the agent that affects the environment.</li> </ul> </li> <li> <p>Transition Function (\\(P(s' | s, a)\\)):</p> <ul> <li>The transition function defines the probability of transitioning from state \\(s\\) to state \\(s'\\) when action \\(a\\) is taken. This function captures the dynamics of the environment.</li> <li>Markov Property: The transition function satisfies the Markov property, which states that the future state \\(s'\\) depends only on the current state \\(s\\) and action \\(a\\), and not on the sequence of states and actions that preceded it. This is why the process is called \"Markovian.\"</li> </ul> </li> <li> <p>Reward Function (\\(R(s, a, s')\\)):</p> <ul> <li>The reward function specifies the immediate reward received by the agent after transitioning from state \\(s\\) to state \\(s'\\) by taking action \\(a\\). The reward is a scalar value that indicates the benefit or cost of the action.</li> </ul> </li> <li> <p>Start State (\\(s_0\\)):</p> <ul> <li>The initial state from which the agent starts its interaction with the environment.</li> </ul> </li> <li> <p>Discount Factor (\\(\\gamma\\)):</p> <ul> <li>The discount factor \\(\\gamma\\) (where \\(0 \\leq \\gamma \\leq 1\\)) determines the present value of future rewards. A discount factor close to 1 makes the agent prioritize long-term rewards, while a discount factor close to 0 makes the agent focus on immediate rewards.</li> </ul> </li> <li> <p>Horizon (\\(H\\)):</p> <ul> <li>The horizon \\(H\\) represents the time horizon over which the agent interacts with the environment. It can be finite (fixed number of steps) or infinite (continuous interaction).</li> </ul> </li> </ol>"},{"location":"course_notes/intro-to-rl/#episodes-and-environment-types","title":"Episodes and Environment Types","text":"<ul> <li>An episode is a sequence of interactions that starts from an initial state and ends when a terminal condition is met.  </li> <li>Episodic Environments: The interaction consists of episodes, meaning the agent's experience is divided into separate episodes with a clear start and end (e.g., playing a game with levels or a robot completing a task like picking up an object).  </li> <li>Non-Episodic (Continuous) Environments: There is no clear termination, and the agent continuously interacts with the environment without resetting (e.g., stock market trading, autonomous vehicle navigation).  </li> </ul>"},{"location":"course_notes/intro-to-rl/#difference-between-horizon-and-episode","title":"Difference Between Horizon and Episode","text":"<ul> <li>The episode refers to a full sequence of interactions that has a clear beginning and an end.  </li> <li>The horizon (\\(H\\)) defines the length of time the agent considers while making decisions, which can be within a single episode (in episodic environments) or over an ongoing interaction (in non-episodic environments).  </li> <li>In finite-horizon episodic tasks, the episode length is usually equal to the horizon. However, in infinite-horizon tasks, the agent keeps interacting with the environment indefinitely.</li> </ul>"},{"location":"course_notes/intro-to-rl/#the-goal-of-reinforcement-learning","title":"The Goal of Reinforcement Learning","text":"<p>The goal of RL in the context of an MDP is to find a policy \\(\\pi\\) that maximizes the expected cumulative reward over time. The policy \\(\\pi\\) is a function that maps states to actions, and it can be deterministic or stochastic.</p>"},{"location":"course_notes/intro-to-rl/#objective","title":"Objective:","text":"\\[ \\max_{\\pi} \\mathbb{E}\\left[\\sum_{t=0}^{H} \\gamma^{t} R(S_t, A_t, S_{t+1}) \\mid \\pi\\right] \\] <ul> <li> <p>Expected Value: The use of the expected value \\(\\mathbb{E}\\) is necessary because the transition function \\(P(s' | s, a)\\) is probabilistic. The agent does not know exactly which state \\(s'\\) it will end up in after taking action \\(a\\) in state \\(s\\). Therefore, the agent must consider the expected reward over all possible future states, weighted by their probabilities.</p> </li> <li> <p>Cumulative Reward: The agent aims to maximize the sum of discounted rewards over time. The discount factor \\(\\gamma\\) ensures that the agent balances immediate rewards with future rewards. A higher \\(\\gamma\\) makes the agent more farsighted, while a lower \\(\\gamma\\) makes it more shortsighted.</p> </li> </ul>"},{"location":"course_notes/intro-to-rl/#example-grid-world","title":"Example: Grid World","text":"<p>Consider a simple grid world where the agent must navigate from a start state to a goal state while avoiding obstacles. The states \\(S\\) are the grid cells, the actions \\(A\\) are movements (up, down, left, right), and the reward function \\(R(s, a, s')\\) provides positive rewards for reaching the goal and negative rewards for hitting obstacles. The transition function \\(P(s' | s, a)\\) defines the probability of actually moving to a neighboring cell when an action is taken.</p>       Fig3. Grid World Game"},{"location":"course_notes/intro-to-rl/#an-example-of-gridworld","title":"An Example of Gridworld","text":"<p>In the provided Gridworld example, the agent starts from the yellow square and has to navigate to a goal while avoiding the cliff. The rewards are: - +1 for reaching the close exit - +10 for reaching the distant exit - -10 penalty for stepping into the cliff (red squares)</p>       Fig4. Grid World Example  <p>The agent's choice depends on: - The discount factor (\\(\\gamma\\)), which determines whether it prioritizes short-term or long-term rewards. - The noise level, which introduces randomness into actions.</p> <p>Depending on the values of \\(\\gamma\\) and noise, the agent's behavior varies: 1. \\(\\gamma\\) = 0.1, noise = 0.5:    - The agent prefers the close exit (+1) but takes the risk of stepping into the cliff (-10). 2. \\(\\gamma\\) = 0.99, noise = 0:    - The agent prefers the distant exit (+10) while avoiding the cliff (-10). 3. \\(\\gamma\\) = 0.99, noise = 0.5:    - The agent still prefers the distant exit (+10), but due to noise, it risks the cliff (-10). 4. \\(\\gamma\\) = 0.1, noise = 0:    - The agent chooses the close exit (+1) while avoiding the cliff. </p>"},{"location":"course_notes/intro-to-rl/#stochastic-policy","title":"Stochastic Policy","text":"<p>Another source of randomness in MDPs comes from stochastic policies. Unlike the transition function \\(P(s' | s, a)\\), which describes the environment\u2019s inherent randomness in executing actions, a stochastic policy \\(\\pi(a | s)\\) defines the probability of selecting an action \\(a\\) when in state \\(s\\). This means that even if the environment were fully deterministic, the agent itself may act probabilistically.</p>"},{"location":"course_notes/intro-to-rl/#example-gridworld-with-a-stochastic-policy","title":"Example: Gridworld with a Stochastic Policy","text":"<p>Consider a modified version of the previous Gridworld example. Instead of always choosing the action with the highest expected return, the agent follows a stochastic policy where it selects each possible action with a certain probability:</p> <ul> <li>With 99% probability, the agent follows its optimal policy.</li> <li>With 1% probability, it selects a random action.</li> </ul> <p>Transition Probability \\(P(s'|s, a)\\)  is the probability that taking action \\(a\\) in state \\(s\\) results in transitioning to state \\(s'\\). This is determined by the environment. If the environment is slippery, moving \"right\" from \\((2,2)\\) may lead to \\((2,3)\\) with 80% probability, but also to \\((3,2)\\) with 20%. Stochastic Policy \\(\\pi(a | s)\\) determines the probability that the agent chooses action \\(a\\) when in state \\(s\\). This is determined by the agent's strategy.</p> <ul> <li>If the policy \\(\\pi(a | s)\\) is deterministic, the agent always selects the same action in a given state.</li> <li>If the policy \\(\\pi(a | s)\\) is stochastic, the agent introduces randomness in its decision-making process, which can be beneficial in exploration and avoiding local optima.</li> </ul>"},{"location":"course_notes/intro-to-rl/#graphical-model-of-mdps","title":"Graphical Model of MDPs","text":"<p>MDPs can be represented graphically as a sequence of states (\\(\\mathbf{s}\\)), actions (\\(\\mathbf{a}\\)), and transitions (\\(p\\)):</p> <ul> <li>The agent starts at state \\(\\mathbf{s}_1\\).</li> <li>It selects an action \\(\\mathbf{a}_1\\), which moves it to \\(\\mathbf{s}_2\\) based on the probability \\(p(\\mathbf{s}_2 | \\mathbf{s}_1, \\mathbf{a}_1)\\).</li> <li>The process continues, forming a decision-making chain where each action influences future states and rewards.</li> </ul>       Fig5. Graphical Model of MDPs"},{"location":"course_notes/intro-to-rl/#partially-observable-mdps-pomdps","title":"Partially Observable MDPs (POMDPs)","text":"<p>In real-world scenarios, the agent may not have full visibility of the environment, leading to Partially Observable MDPs (POMDPs). - Hidden states: The true state \\(\\mathbf{s}_t\\) is not fully known to the agent. - Observations (\\(O\\)): Instead of directly knowing \\(\\mathbf{s}_t\\), the agent receives a noisy or incomplete observation \\(\\mathbf{o}_t\\). - Decision-making challenge: The agent must infer the state from past observations and actions.</p>       Fig6. Partially Observable MDPs (POMDPs)"},{"location":"course_notes/intro-to-rl/#policy-as-a-function-of-state-mathbfs_t-or-observation-mathbfo_t","title":"Policy as a Function of State (\\(\\mathbf{s}_t\\)) or Observation (\\(\\mathbf{o}_t\\))","text":"<ul> <li>Fully Observed Policy: When the agent has access to the full state \\(\\mathbf{s}_t\\), the policy is denoted as \\(\\pi_{\\theta}(\\mathbf{a}_t | \\mathbf{s}_t)\\). This means the action \\(\\mathbf{a}_t\\) is chosen based on the current state \\(\\mathbf{s}_t\\).</li> <li> <p>Partially Observed Policy: When the agent only has access to observations \\(\\mathbf{o}_t\\), the policy is denoted as \\(\\pi_{\\theta}(\\mathbf{a}_t | \\mathbf{o}_t)\\). This means the action \\(\\mathbf{a}_t\\) is chosen based on the current observation \\(\\mathbf{o}_t\\).</p> <ul> <li> <p>Observation: At each time step, the agent receives an observation \\(\\mathbf{o}_t\\) that provides partial information about the current state \\(\\mathbf{s}_t\\). For example, \\(\\mathbf{o}_1\\) is the observation corresponding to state \\(\\mathbf{s}_1\\).</p> </li> <li> <p>Policy: The policy \\(\\pi_{\\theta}\\) maps observations to actions. For instance, \\(\\pi_{\\theta}(\\mathbf{a}_1 | \\mathbf{o}_1)\\) determines the action \\(\\mathbf{a}_1\\) based on the observation \\(\\mathbf{o}_1\\).</p> </li> </ul> </li> </ul>       Fig7. POMDP Policy"},{"location":"course_notes/intro-to-rl/#utility-function-in-reinforcement-learning","title":"Utility Function in Reinforcement Learning","text":"<p>In Reinforcement Learning, the utility function plays a central role in evaluating the long-term desirability of states or state-action pairs. The utility function quantifies the expected cumulative reward that an agent can achieve from a given state or state-action pair, following a specific policy. Let's explore the concept of the utility function and its mathematical formulation.</p>"},{"location":"course_notes/intro-to-rl/#definition-of-utility-function","title":"Definition of Utility Function","text":"<p>The utility function measures the expected cumulative reward that an agent can accumulate over time, starting from a particular state or state-action pair, and following a given policy. There are two main types of utility functions:</p> <ol> <li> <p>State Value Function (\\(V^{\\pi}(s)\\)):</p> <ul> <li>The state value function \\(V^{\\pi}(s)\\) represents the expected cumulative reward when starting from state \\(s\\) and following policy \\(\\pi\\) thereafter.</li> <li>Mathematically, it is defined as:     \\(\\(V^{\\pi}(s) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R(S_t, A_t, S_{t+1}) \\mid S_0 = s, \\pi \\right]\\)\\)</li> <li>Here, \\(\\gamma\\) is the discount factor, and \\(R(S_t, A_t, S_{t+1})\\) is the reward received at time \\(t\\).</li> </ul> </li> <li> <p>Action-Value Function (\\(Q^{\\pi}(s, a)\\)):</p> <ul> <li>The action-value function \\(Q^{\\pi}(s, a)\\) represents the expected cumulative reward when starting from state \\(s\\), taking action \\(a\\), and following policy \\(\\pi\\) thereafter.</li> <li>Mathematically, it is defined as:     \\(\\(Q^{\\pi}(s, a) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R(S_t, A_t, S_{t+1}) \\mid S_0 = s, A_0 = a, \\pi \\right]\\)\\)</li> </ul> </li> </ol>"},{"location":"course_notes/intro-to-rl/#why-is-the-utility-function-important","title":"Why is the Utility Function Important?","text":"<p>The utility function is crucial for several reasons:</p> <ul> <li>Policy Evaluation: It allows the agent to evaluate how good a particular policy \\(\\pi\\) is by estimating the expected cumulative reward for each state or state-action pair.</li> <li>Policy Improvement: By comparing the utility of different actions, the agent can improve its policy by choosing actions that lead to higher cumulative rewards.</li> <li>Optimal Policy: The ultimate goal of RL is to find the optimal policy \\(\\pi^*\\) that maximizes the utility function for all states or state-action pairs.</li> </ul>"},{"location":"course_notes/intro-to-rl/#bellman-equation-for-utility-functions","title":"Bellman Equation for Utility Functions","text":"<p>The utility functions satisfy the Bellman equation, which provides a recursive relationship between the value of a state (or state-action pair) and the values of its successor states. The Bellman equation is fundamental for solving MDPs and is used in many RL algorithms.</p> <ol> <li>Bellman Equation for State Value Function:</li> </ol> \\[V^{\\pi}(s) = \\sum_{a} \\pi(a | s) \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma V^{\\pi}(s') \\right]\\] <ul> <li> <p>This equation states that the value of a state \\(s\\) under policy \\(\\pi\\) is the expected immediate reward plus the discounted value of the next state \\(s'\\).</p> </li> <li> <p>Bellman Equation for Action-Value Function:</p> </li> </ul> \\[Q^{\\pi}(s, a) = \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma \\sum_{a'} \\pi(a' | s') Q^{\\pi}(s', a') \\right]\\] <ul> <li>This equation states that the value of taking action \\(a\\) in state \\(s\\) under policy \\(\\pi\\) is the expected immediate reward plus the discounted value of the next state \\(s'\\) and action \\(a'\\).</li> </ul>"},{"location":"course_notes/intro-to-rl/#example-grid-world_1","title":"Example: Grid World","text":"<p>Consider the Grid World example where the agent navigates to a goal while avoiding obstacles. The utility function helps the agent evaluate the long-term desirability of each cell (state) in the grid:</p> <ul> <li>State Value Function (\\(V^{\\pi}(s)\\)): The agent calculates the expected cumulative reward for each cell, considering the rewards for reaching the goal and penalties for hitting obstacles.</li> <li>Action-Value Function (\\(Q^{\\pi}(s, a)\\)): The agent evaluates the expected cumulative reward for each possible action (up, down, left, right) in each cell, helping it decide the best action to take.</li> </ul>       Fig8. An example of estimated $V^{\\pi}(s)$ values in grid world         Fig9. An example of estimated $Q^{\\pi}(s, a)$ values in grid world"},{"location":"course_notes/intro-to-rl/#authors","title":"Author(s)","text":"<ul> <li> <p>Masoud Tahmasbi</p> <p>Teaching Assistant</p> <p>masoudtahmasbifard@gmail.com</p> <p> </p> </li> </ul>"},{"location":"course_notes/inverse/","title":"Inverse RL","text":""},{"location":"course_notes/meta/","title":"Meta-RL","text":""},{"location":"course_notes/model-based/","title":"Week 5: Model-Based Methods","text":"<p>Note</p> <p>This document merges Lectures 9 and 10 from Prof. Mohammad Hossein Rohban's DRL course, Lecture 9: Model-Based RL slides from Prof. Sergey Levine\u2019s CS 294-112 (Deep RL) with a more rigorous, survey-based structure drawing on Moerland et al. (2022). We provide intuitions, mathematical details, and references to relevant works.</p>"},{"location":"course_notes/model-based/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Week 5: Model-Based Methods</li> <li>Table of Contents</li> <li>1. Introduction \\&amp; Scope</li> <li>2. Markov Decision Processes</li> <li>3. Categories of Model-Based RL<ul> <li>3.1 Planning</li> <li>3.2 Model-Free RL</li> <li>3.3 Model-Based RL (Model + Global Policy/Value)</li> <li>3.3.1 Model-Based RL with a Known Model</li> <li>3.3.2 Model-Based RL with a Learned Model</li> <li>3.4 Planning Over a Learned Model Without a Global Policy</li> </ul> </li> <li>4. Basic Schemes<ul> <li>Version 0.5: Single-Shot Model + Planning</li> <li>Core Steps</li> <li>Shortcomings</li> <li>Why Move On?</li> <li>Version 1.0: Iterative Re-Fitting + Planning</li> <li>Core Steps</li> <li>Shortcomings</li> <li>Why Move On?</li> <li>Version 1.5: Model Predictive Control (MPC)</li> <li>Core Steps</li> <li>Shortcomings</li> <li>Why Move On?</li> <li>Version 2.0: Backprop Through the Learned Model</li> <li>Core Steps</li> <li>Shortcomings</li> <li>Why Move On?</li> <li>Reward Overshooting (Overoptimistic Planning)</li> <li>What It Is</li> <li>Why It Happens</li> <li>Consequences</li> <li>Mitigation Strategies</li> <li>Other Challenges and Notes</li> </ul> </li> <li>5. Dynamics Model Learning<ul> <li>5.1 Basic Considerations</li> <li>5.2 Stochasticity</li> <li>5.2.1 Multi-Modal Transitions and the Conditional Mean Problem</li> <li>5.2.2 Descriptive (Distribution) Models</li> <li>5.2.3 Generative Approaches</li> <li>5.2.4 Training Objectives</li> <li>5.2.5 Practical Considerations and Challenges</li> <li>5.2.6 Example: Gaussian Transitions via Maximum Likelihood</li> <li>5.2.7 Concluding Remarks on Stochastic Transitions</li> <li>5.3 Uncertainty</li> <li>Bayesian Neural Networks</li> <li>Ensembles and Bootstrapping</li> <li>5.4 Partial Observability</li> <li>5.5 Non-Stationarity</li> <li>5.6 Multi-Step Prediction</li> <li>5.7 State Abstraction</li> <li>5.7.1 Common Approaches to Representation Learning</li> <li>5.7.2 Planning in Latent Space</li> <li>5.8 Temporal Abstraction</li> <li>5.8.1 Options Framework</li> <li>5.8.2 Goal-Conditioned Policies</li> <li>5.8.3 Subgoal Discovery</li> <li>5.8.4 Benefits of Temporal Abstraction</li> </ul> </li> <li>6. Integration of Planning and Learning<ul> <li>6.1 Which State to Start Planning From?</li> <li>6.2 Planning Budget vs. Real Data Collection</li> <li>6.3 How to Plan? (Planning Algorithms)</li> <li>6.3.1 Monte Carlo Tree Search (MCTS)</li> <li>6.4 Integration in the Learning and Acting Loop</li> <li>6.5 Dyna and Dyna-Style Methods</li> </ul> </li> <li>7. Modern Model-Based RL Algorithms<ul> <li>7.1 World Models (Ha \\&amp; Schmidhuber, 2018)</li> <li>7.2 PETS (Chua et al., 2018)</li> <li>7.3 MBPO (Janner et al., 2019)</li> <li>7.4 Dreamer (Hafner et al., 2020\u20132023)</li> <li>7.5 MuZero (DeepMind, 2020)</li> </ul> </li> <li>8. Key Benefits (and Drawbacks) of MBRL<ul> <li>8.1 Data Efficiency</li> <li>8.2 Exploration</li> <li>8.3 Optimality</li> <li>8.4 Transfer</li> <li>8.5 Safety</li> <li>8.6 Explainability</li> <li>8.7 Disbenefits</li> </ul> </li> <li>9. Conclusion</li> <li>10. References</li> </ul>"},{"location":"course_notes/model-based/#1-introduction-scope","title":"1. Introduction &amp; Scope","text":"<p>Model-Based Reinforcement Learning (MBRL) combines planning (using a model of environment dynamics) and learning (to approximate value functions or policies globally). MBRL benefits from being able to reason about environment dynamics \u201cin imagination,\u201d thus often achieving higher sample efficiency than purely model-free RL. However, ensuring accurate models and mitigating compounding errors pose key challenges.</p> <p>We address:</p> <ol> <li>The MDP framework and definitions.  </li> <li>Model learning: from basic supervised regression to advanced methods handling stochasticity, uncertainty, partial observability, etc.  </li> <li>Integrating planning: how to incorporate planning loops, short vs. long horizons, and real-world data interplay.  </li> <li>Modern MBRL algorithms (World Models, PETS, MBPO, Dreamer, MuZero).  </li> <li>Benefits and drawbacks of MBRL.</li> </ol>"},{"location":"course_notes/model-based/#2-markov-decision-processes","title":"2. Markov Decision Processes","text":"<p>We adopt the standard Markov Decision Process (MDP) formulation [Puterman, 2014]:</p> \\[ \\mathcal{M} = \\bigl(\\mathcal{S}, \\mathcal{A}, P, R, p(s_0), \\gamma\\bigr), \\] <p>where: - \\(\\mathcal{S}\\) is the (possibly high-dimensional) state space. - \\(\\mathcal{A}\\) is the action space (can be discrete or continuous). - \\(P(s_{t+1}\\mid s_t,a_t)\\) is the transition distribution. - \\(R(s_t,a_t,s_{t+1})\\) is the reward function. - \\(p(s_0)\\) is the initial-state distribution. - \\(\\gamma \\in [0,1]\\) is the discount factor.</p> <p>A policy \\(\\pi(a \\mid s)\\) dictates which action to choose at each state. The value function and action-value function are:</p> \\[ V^\\pi(s) \\;=\\; \\mathbb{E}\\Bigl[\\sum_{k=0}^\\infty \\gamma^k r_{t+k}\\;\\big|\\;s_t = s,\\;\\pi\\Bigr], \\] \\[ Q^\\pi(s,a) \\;=\\; \\mathbb{E}\\Bigl[\\sum_{k=0}^\\infty \\gamma^k r_{t+k}\\;\\big|\\;s_t = s,\\,a_t = a,\\;\\pi\\Bigr]. \\] <p>We want to find \\(\\pi^\\star\\) that maximizes expected return. Model-Based RL obtains a model of the environment\u2019s dynamics \\( \\hat{P}, \\hat{R}\\), then uses planning with that model (e.g., rollouts, search) to aid in learning or acting.</p>"},{"location":"course_notes/model-based/#3-categories-of-model-based-rl","title":"3. Categories of Model-Based RL","text":"<p>Following Moerland et al., we distinguish:</p> <ol> <li>Planning (known model, local solutions).  </li> <li>Model-Free RL (no explicit model, but learns a global policy or value).  </li> <li>Model-Based RL (learned or known model and a global policy/value solution).</li> </ol> <p>Model-Based RL itself splits into two key variants:</p> <ul> <li>Model-based RL with a known model: E.g., AlphaZero uses perfect board-game rules.  </li> <li>Model-based RL with a learned model: E.g., Dyna, MBPO, Dreamer, where the agent must learn \\(\\hat{P}(s_{t+1}\\mid s_t,a_t)\\).</li> </ul> <p>In addition, one could do planning over a learned model but never store a global policy or value (just do local search each time)\u2014that\u2019s still \u201cplanning + learning,\u201d but not strictly \u201cmodel-based RL\u201d if no global policy is learned in the end .</p>"},{"location":"course_notes/model-based/#31-planning","title":"3.1 Planning","text":"<p>Planning methods assume access to a perfect model of the environment\u2019s dynamics \\(\\mathcal{P}(s_{t+1} \\mid s_t, a_t)\\) and reward function \\(r(s_t,a_t)\\). In other words, the transition probabilities and/or the state transitions are fully known. Given this perfect model, the agent can perform a search procedure (e.g., lookahead search, tree search) to find the best action from the current state.</p> <ul> <li>Local Search: Typically, planning algorithms only compute a solution locally, from the agent\u2019s current state or a small set of states. They do not necessarily store or learn a global policy (i.e., a mapping from any possible state to an action).</li> <li>Classical Example: In board games (like chess or Go), an algorithm such as minimax with alpha\u2013beta pruning uses the known, perfect rules of the game to explore future states and pick an optimal move from the current position.</li> </ul> <p>Because these approaches do not usually store or learn a parametric global policy or value function, they fall under \u201cPlanning\u201d rather than \u201cModel-Based RL.\u201d</p>"},{"location":"course_notes/model-based/#32-model-free-rl","title":"3.2 Model-Free RL","text":"<p>Model-Free RL methods do not explicitly use or learn the environment\u2019s transition model. Instead, they optimize a policy \\(\\pi_\\theta(a_t \\mid s_t)\\) or a value function \\(V_\\theta(s_t)\\) (or both) solely based on interactions with the environment.</p> <ul> <li>No Transition Model: The policy or value function is learned directly from sampled trajectories \\((s_t, a_t, r_t, s_{t+1}, \\dots)\\). There is no component that learns \\(\\hat{P}(s_{t+1} \\mid s_t,a_t)\\).</li> <li>Global Solutions: Model-free methods generally learn global solutions: policies or value functions valid across all states encountered during training.</li> <li>Examples: Deep Q-Networks (DQN), Policy Gradient methods (REINFORCE, PPO), and actor\u2013critic approaches.</li> </ul> <p>Despite being effective, model-free methods may require large amounts of environment interaction, since they cannot leverage planning over a learned or known model.</p>"},{"location":"course_notes/model-based/#33-model-based-rl-model-global-policyvalue","title":"3.3 Model-Based RL (Model + Global Policy/Value)","text":"<p>In Model-Based RL, the agent has (or learns) a model of the environment and uses it to learn a global policy or value function. The policy or value function can then be used to make decisions for all states, not just the current one. This class of methods can further be subdivided into two main variants:</p>"},{"location":"course_notes/model-based/#331-model-based-rl-with-a-known-model","title":"3.3.1 Model-Based RL with a Known Model","text":"<p>In some tasks, the transition model \\(\\mathcal{P}(s_{t+1} \\mid s_t, a_t)\\) is known in advance (e.g., it is given by the rules of the environment). The agent can then use this perfect model to plan and to learn a global policy or value function. </p> <ul> <li>AlphaZero: A canonical example in board games (chess, Go, shogi). The rules of the game form a perfect simulator. AlphaZero does extensive lookahead (tree search), but it also uses that data to update a global policy and value network. Thus, it integrates planning and policy learning.</li> <li>Advantages: Since the model is perfect, there is no model-learning error. The primary challenge is how to efficiently search with that model and how to integrate the search results into a global solution.</li> </ul>"},{"location":"course_notes/model-based/#332-model-based-rl-with-a-learned-model","title":"3.3.2 Model-Based RL with a Learned Model","text":"<p>In many real-world tasks, the transition model is not known in advance. The agent must learn an approximate model \\(\\hat{P}(s_{t+1} \\mid s_t,a_t)\\) from environment interactions.</p> <ul> <li>Learning the Model: The agent collects transitions \\((s_t, a_t, r_t, s_{t+1})\\) and trains a parametric model to predict the next state(s) and reward given the current state\u2013action pair.</li> <li>Planning with the Learned Model: The agent can then plan ahead (e.g., via simulated rollouts or lookahead) in this learned model. Although approximate, it allows the agent to generate additional training data or refine its strategy without costly real-world interactions.</li> <li>Examples:</li> <li>Dyna (Sutton, 1990): Interleaves real experience with \u201cimagined\u201d experience from the learned model to update the value function or policy.</li> <li>MBPO (Model-Based Policy Optimization): Uses a learned model to generate short rollouts for policy optimization.</li> <li>Dreamer: Trains a world model and then uses latent imagination (rollouts in latent space) to learn a global policy.</li> </ul>"},{"location":"course_notes/model-based/#34-planning-over-a-learned-model-without-a-global-policy","title":"3.4 Planning Over a Learned Model Without a Global Policy","text":"<p>An additional possibility is to only do planning with a learned model\u2014without ever storing or committing to a parametric global policy or value function. In this scenario, the agent:</p> <ul> <li>Learns or refines a model of the environment.</li> <li>Uses local search (e.g., tree search or some other planning method) each time to select actions.</li> <li>Does not maintain a single policy or value function that applies across all states.</li> </ul> <p>Although this constitutes \u201cplanning + learning\u201d (the learning is in the model, and the planning is local search), it does not fully qualify as \u201cModel-Based RL\u201d in the strict sense\u2014because there is no global policy or value function being learned. Instead, the agent repeatedly plans from scratch (or near-scratch) in the learned model.</p>"},{"location":"course_notes/model-based/#4-basic-schemes","title":"4. Basic Schemes","text":"<p>References present high-level approaches, sometimes referred to as:</p> <ul> <li>Version 0.5: Collect random samples once, fit a model, do single-shot planning. Risks severe distribution mismatch.  </li> <li>Version 1.0: Iterative approach (collect data, re-fit model, plan). Improves coverage, but naive open-loop plans can fail.  </li> <li>Version 1.5: Model Predictive Control (MPC): replan at each step or on short horizons \u2013 more robust but computationally heavier.  </li> <li>Version 2.0: Backprop through the learned model directly into the policy \u2013 can be efficient at runtime, but numerically unstable for complex or stochastic tasks.</li> </ul>"},{"location":"course_notes/model-based/#version-05-single-shot-model-planning","title":"Version 0.5: Single-Shot Model + Planning","text":""},{"location":"course_notes/model-based/#core-steps","title":"Core Steps","text":"<ol> <li> <p>One-Time Data Collection </p> <ul> <li>Collect a static dataset of (state, action, next-state, reward) tuples, typically via random or fixed exploration.</li> <li>No further data is gathered afterward.</li> </ul> </li> <li> <p>One-Time Model Learning </p> <ul> <li>Fit a dynamics model \\( p_\\theta(s' \\mid s, a) \\) using the static dataset.</li> <li>The model may be inaccurate in regions not well-represented in the dataset.</li> </ul> </li> <li> <p>One-Time Planning </p> <ul> <li>Use the learned model to plan or optimize a policy (e.g., via trajectory optimization or tree search).</li> <li>The plan is executed in the real environment without re-planning.</li> </ul> </li> </ol>"},{"location":"course_notes/model-based/#shortcomings","title":"Shortcomings","text":"<ul> <li>Distribution Mismatch: The policy can enter states the model has never \u201cseen,\u201d leading to large prediction errors (extrapolation).  </li> <li>Compounding Errors: Small modeling inaccuracies early on can push the system into unmodeled states, magnifying the errors.  </li> <li>No Iterative Refinement: With no new data collection, there\u2019s no way to correct model inaccuracies discovered during execution.</li> </ul>"},{"location":"course_notes/model-based/#why-move-on","title":"Why Move On?","text":"<ul> <li>Severe Mismatch &amp; Error Growth: Because you never adapt to real outcomes beyond the initial dataset, errors can escalate and result in catastrophic failures.  </li> <li>Limited Practicality: Version 0.5 can work in highly controlled or small problems, but most real tasks require iterative data gathering and model improvements.</li> </ul>"},{"location":"course_notes/model-based/#version-10-iterative-re-fitting-planning","title":"Version 1.0: Iterative Re-Fitting + Planning","text":""},{"location":"course_notes/model-based/#core-steps_1","title":"Core Steps","text":"<ol> <li> <p>Iterative Data Collection</p> <ul> <li>Use a current policy (or plan) to interact with the environment.</li> <li>Gather new transitions (state, action, next-state, reward) and add them to the dataset.</li> </ul> </li> <li> <p>Re-Fit the Model</p> <ul> <li>Update the dynamics model \\( p_\\theta(s' \\mid s, a) \\) using the expanded dataset.</li> <li>The model gradually learns the dynamics in regions the policy visits.</li> </ul> </li> <li> <p>Re-Plan or Update the Policy</p> <ul> <li>After each model update, re-run planning or policy optimization to refine the policy.</li> <li>Deploy the updated policy in the real environment, collect more data, and repeat.</li> </ul> </li> </ol>"},{"location":"course_notes/model-based/#shortcomings_1","title":"Shortcomings","text":"<ul> <li>Open-Loop Execution: Even though the model is updated iteratively, each plan can be executed \u201copen loop,\u201d so stochastic events or modest model errors can derail a plan until the next re-planning cycle.  </li> <li>Long-Horizon Vulnerability: If the planning horizon is substantial, inaccuracies can still compound within a single rollout.</li> </ul>"},{"location":"course_notes/model-based/#why-move-on_1","title":"Why Move On?","text":"<ul> <li>Stochastic / Complex Tasks: Open-loop plans are brittle. You need a way to correct for real-time deviations instead of waiting until the next iteration.  </li> <li>Distribution Still Grows: While iterating helps gather more relevant data, you may still face large drifts in states if the environment is noisy or high-dimensional.</li> </ul>"},{"location":"course_notes/model-based/#version-15-model-predictive-control-mpc","title":"Version 1.5: Model Predictive Control (MPC)","text":""},{"location":"course_notes/model-based/#core-steps_2","title":"Core Steps","text":"<ol> <li>Short-Horizon Re-Planning at Each Step</li> <li> <p>At each time \\(t\\):</p> <ol> <li>Observe the real current state \\( s_t \\).</li> <li>Plan a short sequence of actions \\(\\{a_t, \\dots, a_{t+H-1}\\}\\) with the learned model.</li> <li>Execute only the first action \\( a_t \\).</li> <li>Observe the next real state \\( s_{t+1} \\).</li> <li>Re-plan from \\( s_{t+1} \\).</li> </ol> </li> <li> <p>Closed-Loop Control</p> </li> <li> <p>Frequent re-planning reduces the impact of model errors because the system constantly \u201cchecks in\u201d with reality.</p> </li> <li> <p>Iterative Model Updates (Optional)</p> </li> <li>As in Version 1.0, you can continue to collect data and update the model periodically.</li> </ol>"},{"location":"course_notes/model-based/#shortcomings_2","title":"Shortcomings","text":"<ul> <li>High Computational Cost: Planning at every time step can be expensive, especially in high-dimensional or time-critical domains.  </li> <li>Still Requires a Good Model: Significant model inaccuracies can still cause erroneous plans, though re-planning mitigates compounding errors.</li> </ul>"},{"location":"course_notes/model-based/#why-move-on_2","title":"Why Move On?","text":"<ul> <li>Continuous Planning Overhead: MPC may be infeasible when real-time constraints or massive action spaces make on-the-fly planning too slow.  </li> <li>Desire for a Learned Policy: A direct policy could yield near-instant action selection at runtime, motivating Version 2.0.</li> </ul>"},{"location":"course_notes/model-based/#version-20-backprop-through-the-learned-model","title":"Version 2.0: Backprop Through the Learned Model","text":""},{"location":"course_notes/model-based/#core-steps_3","title":"Core Steps","text":"<ol> <li>Differentiable Dynamics Model</li> <li> <p>Train a neural network or another differentiable function \\( p_\\theta(s' \\mid s, a) \\).</p> </li> <li> <p>End-to-End Policy Optimization</p> </li> <li>Unroll the model over multiple timesteps, applying the policy \\(\\pi_\\phi\\) to get actions, and accumulate predicted rewards.</li> <li>Backpropagate through the learned model to update policy parameters \\(\\phi\\).</li> <li> <p>Once trained, the resulting policy can be deployed directly\u2014no online planning is needed.</p> </li> <li> <p>High Efficiency at Deployment</p> </li> <li>Action selection is a simple forward pass of the policy network, suitable for time-critical or large-scale applications.</li> </ol>"},{"location":"course_notes/model-based/#shortcomings_3","title":"Shortcomings","text":"<ul> <li>Numerical Instability: Backpropagating through many timesteps can lead to exploding or vanishing gradients.  </li> <li>Model Exploitation: The policy can \u201cexploit\u201d any inaccuracies in the model, especially over long horizons or in stochastic environments.  </li> <li>Careful Regularization: Shorter horizon unrolls, ensembles, or uncertainty estimation are often used to keep policy learning stable and robust.</li> </ul>"},{"location":"course_notes/model-based/#why-move-on_3","title":"Why Move On?","text":"<p>Well, Version 2.0 is often seen as an \u201cend goal\u201d rather than a stepping stone\u2014because once you have a learned policy that requires no online planning, you enjoy high-speed inference. However: - Complexity and Instability: Real-world tasks may need a mix of methods (e.g., partial MPC, ensembles) to handle uncertainty and prevent exploitation of model errors.</p>"},{"location":"course_notes/model-based/#reward-overshooting-overoptimistic-planning","title":"Reward Overshooting (Overoptimistic Planning)","text":""},{"location":"course_notes/model-based/#what-it-is","title":"What It Is","text":"<ul> <li>Definition: When a planner or policy exploits incorrect or extrapolated high reward predictions from the learned model, leading to unrealistic or unsafe behavior in the real environment.</li> </ul>"},{"location":"course_notes/model-based/#why-it-happens","title":"Why It Happens","text":"<ul> <li>Sparse Coverage: The model has little data in certain regions, so it overestimates rewards there.  </li> <li>Open-Loop Plans: Versions 0.5 and 1.0 may chase these \u201cfantasy\u201d states for an entire rollout before correcting in the next iteration.  </li> <li>Uncertainty Blindness: If the approach doesn\u2019t penalize states with high model uncertainty, the planner may favor them solely because the model \u201cthinks\u201d they are high-reward.</li> </ul>"},{"location":"course_notes/model-based/#consequences","title":"Consequences","text":"<ul> <li>Poor Real-World Transfer: The agent\u2019s performance can appear great under the learned model but fail in actual interaction.  </li> <li>Safety Violations: In real-world robotics or critical applications, overshooting can lead to dangerous actions.</li> </ul>"},{"location":"course_notes/model-based/#mitigation-strategies","title":"Mitigation Strategies","text":"<ul> <li>Frequent Re-Planning (MPC): Correct for erroneous predictions step by step.  </li> <li>Iterative Data Collection: Gradually gather real data in uncertain regions.  </li> <li>Uncertainty-Aware Models: Ensembles or Bayesian approaches that identify high-uncertainty states and penalize them.  </li> <li>Regularization: Shorter horizon rollouts, trust regions, or other constraints limit destructive exploitation of model errors.</li> </ul>"},{"location":"course_notes/model-based/#other-challenges-and-notes","title":"Other Challenges and Notes","text":"<ol> <li> <p>Reward Function Misspecification. If the reward function itself is imperfect or learned, it can exacerbate overshooting or produce unintended behaviors.</p> </li> <li> <p>Stochastic Environments. Open-loop methods (Version 0.5, 1.0) can fail if they don\u2019t adapt in real time. MPC (Version 1.5) or robust policy optimization (Version 2.0) are better at handling randomness.</p> </li> <li> <p>Exploding/Vanishing Gradients. A big challenge for Version 2.0 when unrolling many timesteps through a neural model.</p> </li> <li> <p>Safety Concerns. In physical or high-stakes domains, any form of model inaccuracy can be dangerous. MPC is often the pragmatic choice in safety-critical tasks.</p> </li> <li> <p>Computational Trade-Offs. MPC (Version 1.5) can be expensive online. End-to-end policy learning (Version 2.0) moves the heavy lifting offline, but training is more delicate.</p> </li> </ol>"},{"location":"course_notes/model-based/#5-dynamics-model-learning","title":"5. Dynamics Model Learning","text":"<p>Learning a model \\(\\hat{P}(s_{t+1}\\mid s_t,a_t)\\) + \\(\\hat{R}(s_t,a_t)\\) is often done via supervised learning on transitions \\((s_t,a_t,s_{t+1},r_t)\\). This section surveys advanced considerations from Moerland et al. .</p>"},{"location":"course_notes/model-based/#51-basic-considerations","title":"5.1 Basic Considerations","text":"<ol> <li> <p>Type of Model </p> <ul> <li>Forward (most common): \\((s_t,a_t)\\mapsto s_{t+1}\\).  </li> <li>Backward (reverse model): \\(s_{t+1}\\mapsto (s_t,a_t)\\). Used in prioritized sweeping.  </li> <li>Inverse: \\((s_t, s_{t+1}) \\mapsto a_t\\). Sometimes used in representation learning or feedback control.</li> </ul> </li> <li> <p>Estimation Method </p> <ul> <li>Parametric (e.g., neural networks, linear regressors, GPs).  </li> <li>Non-parametric (e.g., nearest neighbors, kernel methods).  </li> <li>Exact (tabular) vs. approximate (function approximation).</li> </ul> </li> <li> <p>Region of Validity </p> <ul> <li>Global model: Attempt to capture all states. Common in large-scale MBRL.  </li> <li>Local model: Fit only around the current trajectory or region of interest (common in robotics, e.g., local linearization).</li> </ul> </li> </ol> <p> Overview of different types of mappings in model learning. 1) Standard Markovian transition model \\( s_t, a_t \\rightarrow s_{t+1} \\). 2) Partial observability. We model \\( s_0 \\ldots s_t, a_t \\rightarrow s_{t+1} \\), leveraging the state history to make an accurate prediction. 3) Multi-step prediction (Section 4.6), where we model \\( s_t, a_t \\ldots a_{t+n-1} \\rightarrow s_{t+n} \\), to predict the \\( n \\) step effect of a sequence of actions. 4) State abstraction, where we compress the state into a compact representation \\( z_t \\) and model the transition in this latent space. 5) Temporal/action abstraction, better known as hierarchical reinforcement learning, where we learn an abstract action \\( u_t \\) that brings us to \\( s_{t+n} \\). Temporal abstraction directly implies multi-step prediction, as otherwise the abstract action \\( u_t \\) is equal to the low level action \\( a_t \\). All the above ideas (2\u20135) are orthogonal and can be combined.</p>"},{"location":"course_notes/model-based/#52-stochasticity","title":"5.2 Stochasticity","text":"<p>Real MDPs can be stochastic, meaning that the environment transition from \\((s_t, a_t)\\) to \\(s_{t+1}\\) is governed by a distribution:</p> \\[ P\\bigl(s_{t+1} \\mid s_t, a_t \\bigr). \\] <p>Unlike a deterministic setting (where we might write \\(s_{t+1} = f(s_t, a_t)\\)), this transition function yields a probability distribution over all possible next states rather than a single outcome.</p>"},{"location":"course_notes/model-based/#521-multi-modal-transitions-and-the-conditional-mean-problem","title":"5.2.1 Multi-Modal Transitions and the Conditional Mean Problem","text":"<p>When training a purely deterministic network (e.g., a standard neural network with mean-squared error, MSE) to predict \\(s_{t+1}\\) from \\((s_t, a_t)\\), the model typically learns the conditional mean of the next-state distribution. This can be problematic if the true transition distribution is multi-modal, since the mean might not align with any actual or likely realization of the environment.</p> <p></p> <p>Illustration of stochastic transition dynamics. Left: 500 samples from an example transition function \\(P(s_{t+1} \\mid s, a)\\). The vertical dashed line indicates the cross-section distribution on the right. Right: distribution of \\(s_{t+1}\\) for a particular \\((s, a)\\). We observe a multimodal distribution. The conditional mean of this distribution, which would be predicted by MSE training, is shown as a vertical line.</p> <p>Formally, if the true next state is a random variable \\(S_{t+1}\\), then MSE-based regression gives</p> \\[ \\hat{s}_{t+1} \\;=\\; \\mathbb{E}\\bigl[S_{t+1}\\,\\big|\\,(s_t,a_t)\\bigr]. \\] <p>Hence, if \\(S_{t+1}\\) can take on several distinct modes with similar probabilities, a single mean prediction \\(\\hat{s}_{t+1}\\) may not capture the actual modes at all.</p>"},{"location":"course_notes/model-based/#522-descriptive-distribution-models","title":"5.2.2 Descriptive (Distribution) Models","text":"<p>To capture multi-modal dynamics rigorously, one can represent the full distribution \\(P(s_{t+1}\\mid s_t,a_t)\\). Common choices include:</p> <ol> <li> <p>Gaussian Distribution     Assume</p> \\[ s_{t+1} \\;\\sim\\; \\mathcal{N}\\bigl(\\mu_\\theta(s_t,a_t),\\,\\Sigma_\\theta(s_t,a_t)\\bigr), \\] <p>where \\(\\theta\\) denotes model parameters. Typically trained by maximizing log-likelihood of observed transitions.</p> </li> <li> <p>Gaussian Mixture Models (GMM)     Use a sum of \\(K\\) Gaussians:</p> \\[ s_{t+1} \\;\\sim\\; \\sum_{k=1}^{K}\\;\\alpha_k(\\theta; s_t,a_t)\\;\\mathcal{N}\\!\\Bigl(\\mu_k,\\;\\Sigma_k\\Bigr). \\] <p>The mixture weights \\(\\alpha_k\\) sum to 1. This better captures multi-modality than a single Gaussian but can be more complex to train (e.g., via EM).</p> </li> <li> <p>Tabular/Histogram-Based     For lower-dimensional or discrete states:</p> \\[ \\hat{P}\\bigl(s' \\mid s,a\\bigr) \\;=\\; \\frac{n(s,a,s')}{\\sum_{\\tilde{s}}\\,n(s,a,\\tilde{s})}, \\] <p>where \\(n(\\cdot)\\) counts observed transitions. This is often infeasible in large or continuous domains.</p> </li> </ol>"},{"location":"course_notes/model-based/#523-generative-approaches","title":"5.2.3 Generative Approaches","text":"<p>Instead of closed-form probability distributions, one might learn a generative mapping that samples from \\(P(s_{t+1}\\mid s_t,a_t)\\). Examples:</p> <ol> <li> <p>Variational Autoencoders (VAEs)     Introduce a latent variable \\(\\mathbf{z}\\). Then</p> \\[     s_{t+1} \\;=\\; f_\\theta\\!\\bigl(\\mathbf{z},\\,s_t,\\,a_t\\bigr),      \\quad      \\mathbf{z}\\;\\sim\\;\\mathcal{N}(\\mathbf{0},\\mathbf{I}), \\] <p>and fit \\(\\theta\\) via variational inference. Inference-time sampling of \\(\\mathbf{z}\\) yields diverse future states.</p> </li> <li> <p>Normalizing Flows     Transform a simple base distribution (like a Gaussian) through a stack of invertible mappings \\(\\{f_\\theta^{(\\ell)}\\}\\):</p> \\[ \\mathbf{z}\\,\\sim\\,\\mathcal{N}(\\mathbf{0},\\mathbf{I}),  \\quad s_{t+1} \\;=\\; (f_\\theta^{(L)} \\circ \\cdots \\circ f_\\theta^{(1)})(\\mathbf{z}). \\] <p>Optimized via maximum likelihood, enabling expressive densities.</p> </li> <li> <p>Generative Adversarial Networks (GANs)     A discriminator \\(D\\) distinguishes real vs. generated next states, while the generator \\(G\\) attempts to fool \\(D\\). Though flexible, GAN training can be unstable or prone to mode collapse.</p> </li> <li> <p>Autoregressive Models     Factorize high-dimensional \\(s_{t+1}\\) into a chain of conditionals. Useful for image-based transitions but can be computationally heavy.</p> </li> </ol>"},{"location":"course_notes/model-based/#524-training-objectives","title":"5.2.4 Training Objectives","text":"<p>Most distribution models are trained by maximizing likelihood or minimizing negative log-likelihood over a dataset \\(\\{(s_t^{(i)}, a_t^{(i)}, s_{t+1}^{(i)})\\}\\). For example:</p> \\[     \\theta^*      =      \\arg\\max_\\theta      \\sum_{i=1}^N      \\log P\\bigl(s_{t+1}^{(i)} \\mid s_t^{(i)},a_t^{(i)};\\,\\theta \\bigr)     -\\;     \\Omega(\\theta), \\] <p>where \\(\\Omega(\\theta)\\) might be a regularization term. GAN-based models use a min-max objective, while VAE-based methods use an ELBO that includes a reconstruction term and a KL prior penalty on the latent space.</p>"},{"location":"course_notes/model-based/#525-practical-considerations-and-challenges","title":"5.2.5 Practical Considerations and Challenges","text":"<ul> <li> <p>Divergence in Multi-Step Rollouts   Even with a stochastic model, errors can accumulate as predictions are fed back into the model. Mitigations include unrolling during training, multi-step loss functions, or specialized architectures.</p> </li> <li> <p>Mode Collapse / Rare Transitions   Multi-modal distributions can be hard to learn in practice. Models must capture all critical modes, especially for safety or robotics contexts where minority transitions may be crucial.</p> </li> <li> <p>High-Dimensional Observations   Image-based tasks often leverage latent-variable models (e.g., VAE-like) to reduce dimensionality. Encoding \\(\\rightarrow\\) (predict in latent space) \\(\\rightarrow\\) decoding is common.</p> </li> <li> <p>Expressiveness vs. Efficiency   Complex generative models (e.g., large mixtures or flows) capture stochasticity better but are often slower to train and evaluate. Many real-world agents resort to simpler unimodal Gaussians, balancing speed and accuracy.</p> </li> </ul>"},{"location":"course_notes/model-based/#526-example-gaussian-transitions-via-maximum-likelihood","title":"5.2.6 Example: Gaussian Transitions via Maximum Likelihood","text":"<p>A common assumption uses a unimodal Gaussian:</p> \\[ s_{t+1} \\;\\sim\\; \\mathcal{N}\\Bigl(\\mu_\\theta(s_t,a_t),\\;\\Sigma_\\theta(s_t,a_t)\\Bigr). \\] <p>Assume diagonal covariance \\(\\Sigma_\\theta=\\mathrm{diag}(\\sigma_1^2,\\dots,\\sigma_d^2)\\). The log-likelihood for one observed transition is:</p> \\[ \\log p\\bigl(s_{t+1}\\mid s_t,a_t;\\,\\theta\\bigr) \\,=\\, \\sum_{j=1}^d  \\Bigl[ -\\tfrac12\\,\\ln\\bigl(2\\pi\\,\\sigma_j^2(\\cdot)\\bigr) -\\; \\tfrac{\\bigl(s_{t+1}[j]-\\mu_j(\\cdot)\\bigr)^2}{2\\,\\sigma_j^2(\\cdot)} \\Bigr]. \\] <p>Maximizing this finds a Gaussian best-fit to the empirical data. While unimodal, it remains a popular, tractable choice for continuous control.</p>"},{"location":"course_notes/model-based/#527-concluding-remarks-on-stochastic-transitions","title":"5.2.7 Concluding Remarks on Stochastic Transitions","text":"<ul> <li> <p>Why Model Stochasticity?   Real-world dynamics often have multiple plausible next states. Failing to capture these can produce inaccurate planning and suboptimal policies.</p> </li> <li> <p>Descriptive vs. Generative   Some tasks demand full density estimation (e.g., risk-sensitive planning), while others only require sampling plausible transitions (e.g., for Monte Carlo rollouts).</p> </li> <li> <p>Integration with RL   Once a model is learned, the agent can plan by either averaging over transitions or sampling them. Handling many branching futures can be computationally expensive; practical approaches limit the depth or use heuristic expansions.</p> </li> </ul> <p>In sum, real-world MDPs often present multi-modal and stochastic dynamics. A purely deterministic predictor trained via MSE collapses the distribution to a single mean. Instead, we can use distribution (e.g., GMM) or generative (e.g., VAE, flows, GANs) approaches, trained via maximum likelihood, adversarial losses, or variational inference. Proper handling of stochastic transitions is essential for robust planning, policy optimization, and multi-step simulation in model-based RL.</p>"},{"location":"course_notes/model-based/#53-uncertainty","title":"5.3 Uncertainty","text":"<p>A critical challenge in MBRL is model uncertainty\u2014the model is learned from limited data, so predictions can be unreliable in unfamiliar state-action regions. We distinguish:</p> <ul> <li>Aleatoric (intrinsic) uncertainty: inherent stochasticity in transitions.  </li> <li>Epistemic (model) uncertainty: arises from limited training data. This can, in principle, be reduced by gathering more data.</li> </ul> <p>A rigorous approach is to maintain a distribution over possible models, then plan by integrating or sampling from that distribution to avoid catastrophic exploitation of untrusted model regions.</p>"},{"location":"course_notes/model-based/#bayesian-neural-networks","title":"Bayesian Neural Networks","text":"<p>One approach is a Bayesian neural network (BNN):</p> \\[ \\theta \\sim p(\\theta), \\quad s_{t+1} \\sim P_\\theta(\\cdot \\mid s_t, a_t). \\] <p>We keep a posterior \\(p(\\theta\\mid D)\\) over network weights \\(\\theta\\) given dataset \\(D\\). Predictive distribution for the next state is then:</p> \\[ p(s_{t+1}\\mid s_t,a_t, D) = \\int P_\\theta(s_{t+1}\\mid s_t,a_t)\\,p(\\theta\\mid D)\\,d\\theta. \\] <p>In practice, approximations like variational dropout or Laplace approximation are used to sample from \\(p(\\theta)\\).</p>"},{"location":"course_notes/model-based/#ensembles-and-bootstrapping","title":"Ensembles and Bootstrapping","text":"<p>Another popular method is an ensemble of \\(N\\) models \\(\\{\\hat{P}_{\\theta_i}\\}\\). Each model is trained on a bootstrapped subset of the data (or with different initialization seeds). The variance across predictions:</p> \\[ \\mathrm{Var}\\bigl[\\hat{P}_{\\theta_i}(s_{t+1}\\mid s_t,a_t)\\bigr] \\] <p>indicates epistemic uncertainty. In practice:</p> \\[ \\hat{\\mu}_\\mathrm{ensemble}(s_{t+1}) \\approx \\frac{1}{N}\\sum_{i=1}^N \\hat{\\mu}_i(s_{t+1}), \\quad \\hat{\\Sigma}_\\mathrm{ensemble}(s_{t+1}) \\approx \\frac{1}{N}\\sum_{i=1}^N \\bigl(\\hat{\\mu}_i - \\hat{\\mu}_\\mathrm{ensemble}\\bigr)^2. \\] <p>During planning, one may:</p> <ol> <li>Sample one model from the ensemble at each step (like PETS).</li> <li>Average the predictions or treat it as a Gaussian mixture model.</li> </ol> <p>Either way, uncertain regions typically manifest as a large disagreement among ensemble members, warning the planner not to trust that zone.</p> <p></p> <p>Mathematically: if the \u201ctrue\u201d dynamics distribution is \\(P^\\star\\) and each model \\(\\hat{P}_{\\theta_i}\\) is an unbiased estimator, then large variance across \\(\\{\\hat{P}_{\\theta_i}\\}\\) signals a region outside the training distribution. Minimizing that variance can guide exploration or help shape conservative planning.</p>"},{"location":"course_notes/model-based/#54-partial-observability","title":"5.4 Partial Observability","text":"<p>Sometimes the environment is not fully observable. We can\u2019t identify the full state \\(s\\) from a single observation \\(o\\). Solutions:</p> <ul> <li>Windowing: Keep last \\(n\\) observations \\((o_t, o_{t-1}, \\dots)\\).  </li> <li>Belief state: Use a hidden Markov model or Bayesian filter.  </li> <li>Recurrence: Use RNNs/LSTMs that carry hidden state \\(\\mathbf{h}_t\\).  </li> <li>External Memory: Neural Turing Machines, etc., for long-range dependencies.</li> </ul>"},{"location":"course_notes/model-based/#55-non-stationarity","title":"5.5 Non-Stationarity","text":"<p>Non-stationary dynamics means that \\(P\\) or \\(R\\) changes over time. A single learned model can become stale. Approaches include:</p> <ul> <li>Partial models [Doya et al., 2002]: Maintain multiple submodels for different regimes, detect changes in transition error.  </li> <li>High learning-rate or forgetting older data to adapt quickly.</li> </ul>"},{"location":"course_notes/model-based/#56-multi-step-prediction","title":"5.6 Multi-Step Prediction","text":"<p>One-step predictions can accumulate error when rolled out repeatedly. Some solutions:</p> <ul> <li>Train for multi-step: Unroll predictions for \\(k\\) steps and backprop against ground truth \\((s_{t+k})\\).  </li> <li>Dedicated multi-step models: Instead of chaining one-step predictions, learn \\(f^{(k)}(s_t,a_{t:k-1})\\approx s_{t+k}\\).</li> </ul> <p></p>"},{"location":"course_notes/model-based/#57-state-abstraction","title":"5.7 State Abstraction","text":"<p>In many domains, the raw observation space \\(s\\) can be very high-dimensional (e.g., pixel arrays), making direct modeling of \\((s_t, a_t) \\mapsto s_{t+1}\\) intractable. State abstraction (or representation learning) tackles this issue by learning a more compact latent state \\(\\mathbf{z}_t\\), capturing the essential factors of variation. Once in this latent space, the model learns transitions \\(\\mathbf{z}_{t+1} = f_{\\text{trans}}(\\mathbf{z}_t, a_t)\\), and can decode back to the original space if needed:</p> \\[ \\mathbf{z}_t = f_{\\text{enc}}(s_t),  \\quad \\mathbf{z}_{t+1} = f_{\\text{trans}}(\\mathbf{z}_t, a_t),  \\quad s_{t+1} \\approx f_{\\text{dec}}(\\mathbf{z}_{t+1}). \\] <p>This structure reduces modeling complexity and can enable more efficient planning or control in the latent domain.</p>"},{"location":"course_notes/model-based/#571-common-approaches-to-representation-learning","title":"5.7.1 Common Approaches to Representation Learning","text":"<ol> <li> <p>Autoencoders and Variational Autoencoders (VAEs)     An autoencoder aims to learn an encoding \\(\\mathbf{z}=f_{\\text{enc}}(s)\\) that, when passed through a decoder \\(f_{\\text{dec}}\\), reconstructs the original state \\(s\\). In variational autoencoders (VAEs), one imposes a prior distribution \\(p(\\mathbf{z})\\) (often Gaussian) over the latent space, adding a Kullback\u2013Leibler (KL) divergence penalty:</p> \\[ \\max_{\\theta,\\phi}  \\;\\; \\mathbb{E}_{q_\\phi(\\mathbf{z}\\mid s)} \\Bigl[\\log p_\\theta(s\\mid \\mathbf{z})\\Bigr] \\;-\\; D_{\\mathrm{KL}}\\bigl(q_\\phi(\\mathbf{z}\\mid s)\\,\\|\\,p(\\mathbf{z})\\bigr), \\] <p>where \\(q_\\phi(\\mathbf{z}\\mid s)\\) is the encoder distribution and \\(p_\\theta(s\\mid \\mathbf{z})\\) is the decoder. This ensures the learned latent code \\(\\mathbf{z}\\) both reconstructs well and remains \u201corganized\u201d under the prior \\(p(\\mathbf{z})\\). Once learned, a latent dynamics model \\(\\mathbf{z}_{t+1}=f_{\\text{trans}}(\\mathbf{z}_t,a_t)\\) can be fitted by minimizing a prediction loss (e.g., mean-squared error on \\(\\mathbf{z}\\)-space).</p> </li> <li> <p>Object-Based Approaches     For environments that can be factorized into distinct objects (e.g., multiple physical entities), Graph Neural Networks (GNNs) or object-centric models can more naturally capture the underlying structure. Concretely, each latent node \\(\\mathbf{z}_i\\) corresponds to an object\u2019s state (e.g., location, velocity, shape), and edges model interactions among objects. Formally,</p> \\[ \\mathbf{z}_{t+1}^{(i)}  \\;=\\; f_{\\text{trans}}^{(i)} \\Bigl(     \\mathbf{z}_t^{(i)},\\,a_t,\\,     \\{\\mathbf{z}_t^{(j)}\\}_{j \\in \\mathcal{N}(i)} \\Bigr), \\] <p>where \\(\\mathcal{N}(i)\\) denotes neighbors of object \\(i\\). This is particularly effective in physics-based settings [Battaglia et al., 2016], allowing each object\u2019s transition to depend primarily on relevant neighbors (e.g., collisions). Such structured representations often facilitate better generalization to new configurations (e.g., changing the number or arrangement of objects).</p> </li> <li> <p>Contrastive Losses for Semantic/Controllable Features     Sometimes, a purely reconstruction-based loss can over-focus on visually salient but decision-irrelevant details. Contrastive methods use pairs (or triplets) of observations to emphasize meaningful relationships. For instance, if two states \\(s\\) and \\(s'\\) are known to be dynamically close (reachable with few actions), one encourages their embeddings \\(\\mathbf{z}\\) and \\(\\mathbf{z}'\\) to be close under some metric. Formally, a contrastive loss might look like:</p> \\[ \\ell_{\\mathrm{contrast}}(\\mathbf{z}, \\mathbf{z}')  \\;=\\; y\\,\\|\\mathbf{z}-\\mathbf{z}'\\|^2  \\;+\\; (1-y)\\,\\bigl[\\alpha - \\|\\mathbf{z}-\\mathbf{z}'\\|\\bigr]_{+}, \\] <p>where \\(y=1\\) if the states should be similar, and \\(y=0\\) otherwise, and \\(\\alpha\\) is a margin. Examples include time-contrastive approaches [Sermanet et al., 2018] that bring together frames from different camera angles but the same physical scene, or goal-oriented distances [Ghosh et al., 2018]. These tasks guide the learned \\(\\mathbf{z}\\)-space to capture features that matter for control, rather than trivial background details.</p> </li> </ol>"},{"location":"course_notes/model-based/#572-planning-in-latent-space","title":"5.7.2 Planning in Latent Space","text":"<p>Once a latent representation is established, an agent can:</p> <ol> <li> <p>Plan directly in \\(\\mathbf{z}\\)-space    For instance, run a forward-search or gradient-based policy optimization with states \\(\\mathbf{z}\\) and transitions \\(f_{\\text{trans}}(\\mathbf{z}, a)\\). If the decoder \\(f_{\\text{dec}}\\) is not explicitly needed (e.g., if the agent only needs to output actions), planning in the latent domain can reduce computational overhead and reduce the \u201ccurse of dimensionality.\u201d</p> </li> <li> <p>Decode for interpretability or environment feedback    If the environment requires real-world actions or if interpretability is desired, one can decode predicted latent states \\(\\mathbf{z}_{t+1}\\) to \\(\\hat{s}_{t+1}\\). The environment then checks feasibility or yields a reward. This is especially relevant when the environment is external (like a simulator or the real world) expecting inputs in the original space.</p> </li> </ol> <p>A primary challenge is rollout mismatch: if \\(\\mathbf{z}_{t+1}\\) is never trained to match \\(f_{\\text{enc}}(s_{t+1})\\), repeated application of \\(f_{\\text{trans}}\\) might accumulate errors. Solutions include explicit consistency constraints (i.e., \\(\\|\\mathbf{z}_{t+1} - f_{\\text{enc}}(s_{t+1})\\|\\) penalties) or probabilistic latent inference (e.g., Kalman filter variants, sequential VAEs).</p>"},{"location":"course_notes/model-based/#58-temporal-abstraction","title":"5.8 Temporal Abstraction","text":"<p>In Markov Decision Processes (MDPs), each action typically spans one environment step. But many tasks have natural subroutines that can be chunked into higher-level actions. This is the essence of hierarchical reinforcement learning (HRL): define \u201cmacro-actions\u201d that unfold over multiple timesteps, reducing effective planning depth and often improving data efficiency.</p>"},{"location":"course_notes/model-based/#581-options-framework","title":"5.8.1 Options Framework","text":"<p>An Option \\(\\omega\\) is a tuple \\((I_\\omega, \\pi_\\omega, \\beta_\\omega)\\) [Sutton et al., 1999]:</p> <ul> <li>Initiation set \\(I_\\omega \\subseteq \\mathcal{S}\\): states from which \\(\\omega\\) can be invoked.</li> <li>Subpolicy \\(\\pi_\\omega(a \\mid s)\\): governs low-level actions while the option runs.</li> <li>Termination condition \\(\\beta_\\omega(s)\\): probability that \\(\\omega\\) terminates upon reaching state \\(s\\).</li> </ul> <p>When executing option \\(\\omega\\), the agent follows \\(\\pi_\\omega\\) until a stochastic termination event triggers, transitioning back to the high-level policy\u2019s control. The high-level policy thus selects from a set of options, each a multi-step \u201cchunk\u201d of actions. This can drastically reduce the horizon of the planning problem.</p> <p>Mathematically, a semi-MDP formalism captures these temporally extended actions, where option \\(\\omega\\) yields a multi-step transition \\((s, \\omega)\\mapsto s'\\). One can learn option-value functions \\(Q(s,\\omega)\\) or plan with pseudo-rewards inside each option. In practice, good options can significantly accelerate learning and planning compared to primitive actions.</p>"},{"location":"course_notes/model-based/#582-goal-conditioned-policies","title":"5.8.2 Goal-Conditioned Policies","text":"<p>Alternatively, goal-conditioned or universal value functions [Schaul et al., 2015] define a function \\(Q(s, a, g)\\) that specifies the expected return for taking action \\(a\\) in state \\(s\\) while aiming to achieve goal \\(g\\). One can then plan by selecting subgoals:</p> \\[ g_1, g_2, \\dots, g_K, \\] <p>where each \u201cmacro-step\u201d is the agent trying to reach \\(g_i\\) from the current state. Feudal RL frameworks [Dayan and Hinton, 1993] similarly treat higher-level \u201cmanagers\u201d that set subgoals for lower-level \u201cworkers.\u201d A learned, goal-conditioned subpolicy \\(\\pi(a\\mid s,g)\\) can generalize across different goals \\(g\\), unlike the options approach that typically uses separate subpolicies per option.</p>"},{"location":"course_notes/model-based/#583-subgoal-discovery","title":"5.8.3 Subgoal Discovery","text":"<p>A key research question is how to discover effective macro-actions (options) or subgoals. Approaches include:</p> <ol> <li> <p>Graph-based Bottlenecks    Construct or approximate a graph of states/regions. Identify bottlenecks as states that connect densely connected regions. Formally, if a state \\(s\\) lies on many shortest paths between subregions of the state space, it can be a strategic \u201cbridge.\u201d Setting it as a subgoal can simplify global planning [Menache et al., 2002].</p> </li> <li> <p>Empowerment / Coverage    Encourage subpolicies that cover different parts of the state space or yield high controllability. For instance, one might maximize mutual information between subpolicy latent codes and resulting states, ensuring distinct subpolicies lead to distinct outcomes. This fosters diverse skill discovery.</p> </li> <li> <p>End-to-End Learning    Methods like Option-Critic [Bacon et al., 2017] embed option structure into a differentiable policy architecture and optimize for return. The subpolicies and termination functions emerge from gradient-based training, though careful regularization is often needed to avoid degenerate solutions (e.g., a single subpolicy doing everything).</p> </li> </ol>"},{"location":"course_notes/model-based/#584-benefits-of-temporal-abstraction","title":"5.8.4 Benefits of Temporal Abstraction","text":"<ul> <li>Reduced Planning Depth   Since each macro-action can span multiple timesteps, the effective decision horizon shrinks, often simplifying search or dynamic programming.</li> <li>Transfer and Reuse   Once discovered, options/subpolicies can be reused in related tasks. If those subroutines correspond to meaningful skills, the agent may quickly adapt to new goals.</li> <li>Data Efficiency   Higher-level actions can yield more stable and purposeful exploration, collecting relevant experience faster than random primitive actions.</li> </ul>"},{"location":"course_notes/model-based/#6-integration-of-planning-and-learning","title":"6. Integration of Planning and Learning","text":"<p>With a learned model in hand (or a known one), we combine planning and learning to optimize a policy \\(\\pi\\). We address four major questions:</p> <ol> <li>Which state to start planning from? </li> <li>Budget and frequency: how many real steps vs. planning steps?  </li> <li>Planning algorithm: forward search, MCTS, gradient-based, etc.  </li> <li>Integration: how planning outputs feed into policy/value updates and final action selection.</li> </ol> <p></p>"},{"location":"course_notes/model-based/#61-which-state-to-start-planning-from","title":"6.1 Which State to Start Planning From?","text":"<ul> <li>Uniform over all states (like classical dynamic programming).  </li> <li>Visited states only (like Dyna [Sutton, 1990], which samples from replay).  </li> <li>Prioritized (Prioritized Sweeping [Moore &amp; Atkeson, 1993]) if some states need urgent update.  </li> <li>Current state only (common in online MPC or MCTS from the real agent\u2019s state).</li> </ul>"},{"location":"course_notes/model-based/#62-planning-budget-vs-real-data-collection","title":"6.2 Planning Budget vs. Real Data Collection","text":"<p>Two sub-questions:</p> <ol> <li> <p>Frequency: plan after every environment step, or collect entire episodes first?  </p> <ul> <li>Dyna plans after each step (like 100 imaginary updates per real step).  </li> <li>PILCO [Deisenroth &amp; Rasmussen, 2011] fits a GP model after each episode.</li> </ul> </li> <li> <p>Budget: how many model rollouts or expansions per planning cycle?  </p> <ul> <li>Dyna might do 100 short rollouts.  </li> <li>AlphaZero expands a single MCTS iteration by up to 1600 \u00d7 depth calls.</li> </ul> </li> </ol> <p>Some methods adaptively adjust planning vs. real data based on model uncertainty [Kalweit &amp; Boedecker, 2017]. The right ratio can significantly affect performance.</p>"},{"location":"course_notes/model-based/#63-how-to-plan-planning-algorithms","title":"6.3 How to Plan? (Planning Algorithms)","text":"<p>Broadly:</p> <ol> <li> <p>Discrete (non-differentiable) search:</p> <ul> <li>One-step lookahead  </li> <li>Tree search (MCTS, minimax)  </li> <li>Forward vs. backward: e.g., prioritized sweeping uses a reverse model to propagate value changes quickly</li> </ul> </li> <li> <p>Differential (gradient-based) planning:</p> <ul> <li>Requires a differentiable model \\(\\hat{P}\\).  </li> <li>E.g., iterative LQR, or direct backprop through unrolled transitions (Dreamer).  </li> <li>Suited for continuous control with smooth dynamics.</li> </ul> </li> <li> <p>Depth &amp; Breadth choices:</p> <ul> <li>Some do short-horizon expansions (MBPO uses 1\u20135 step imaginary rollouts).  </li> <li>Others do deeper expansions if computing resources allow (AlphaZero MCTS).</li> </ul> </li> <li> <p>Uncertainty handling:</p> <ul> <li>Plan only near states with low model uncertainty or penalize uncertain states.  </li> <li>Ensemble-based expansions [Chua et al., 2018].</li> </ul> </li> </ol> <p>Cross-Entropy Method (CEM) \u2013 Pseudocode</p> <pre><code>    # Suppose we want to find the best action sequence of length H\n    # that maximizes the expected return under our model.\n\n    Initialize distribution params (mean mu, covariance Sigma)\n    for iteration in range(N_iterations):\n        # 1. Sample K sequences from current distribution\n        candidate_sequences = sample_from_gaussian(mu, Sigma, K)\n\n        # 2. Evaluate each sequence's return\n        returns = []\n        for seq in candidate_sequences:\n            returns.append( evaluate_return(seq, model) )\n\n        # 3. Select the top M (elite) sequences\n        elite_indices = top_indices(returns, M)\n        elites = [candidate_sequences[i] for i in elite_indices]\n\n        # 4. Update mu, Sigma to fit the elites\n        mu = mean(elites)\n        Sigma = cov(elites)\n\n    # Final distribution reflects the best action sequence\n    best_action_seq = mu\n    return best_action_seq\n</code></pre>"},{"location":"course_notes/model-based/#631-monte-carlo-tree-search-mcts","title":"6.3.1 Monte Carlo Tree Search (MCTS)","text":"<p>Monte Carlo Tree Search is a powerful method for discrete action planning\u2014famously used in AlphaGo, AlphaZero, MuZero. Key components:</p> <ol> <li> <p>Tree Representation </p> <ul> <li>Each node is a state, edges correspond to actions.  </li> <li>MCTS incrementally expands the search tree from a root (the current state).</li> </ul> </li> <li> <p>Four Steps commonly described as:</p> <ol> <li>Selection: Repeatedly choose child nodes (actions) from the root, typically via Upper Confidence Bound or policy heuristics, until reaching a leaf node.  </li> <li>Expansion: If the leaf is not terminal (or at max depth), add one or more child nodes for possible next actions.  </li> <li>Simulation: From that new node, simulate a rollout (random or policy-driven) until reaching a terminal state or horizon.  </li> <li>Backpropagation: Propagate the return from the simulation up the tree to update value/statistics at each node.</li> </ol> </li> <li> <p>Mathematical Form </p> <ul> <li>Let \\(N(s,a)\\) be the number of visits to child action \\(a\\) from state \\(s\\).  </li> <li>Let \\(\\hat{Q}(s,a)\\) be the estimated action-value from MCTS.  </li> <li> <p>UCB selection uses:</p> \\[ a_\\text{select} = \\arg\\max_{a}\\Bigl[\\hat{Q}(s,a) + c \\sqrt{\\frac{\\ln \\sum_{b} N(s,b)}{N(s,a)}}\\Bigr]. \\] <p>(One can also incorporate a learned prior policy \\(\\pi_\\theta\\) to bias exploration.)</p> </li> </ul> </li> <li> <p>Planning &amp; Policy Extraction </p> <ul> <li>After many simulations from the root, MCTS typically normalizes node visits or Q-values to produce a final policy distribution \\(\\alpha\\).  </li> <li>This policy \\(\\alpha\\) may be used for real action selection, or to train a global policy network (as in AlphaZero).</li> </ul> </li> </ol> <p>MCTS Pseudocode</p> <pre><code>MCTS(root_state, model, N_simulations)\nInitialize the search tree with root_state\n\nfor simulation in 1 to N_simulations do\n    node \u2190 root of the tree\n\n    # Selection\n    while node is fully expanded and node is not terminal do\n        action \u2190 select child of node using UCB\n        node \u2190 child corresponding to action\n\n    # Expansion\n    if node is not terminal then\n        expand node using model (generate all children)\n        node \u2190 select one of the new children\n\n    # Simulation\n    reward \u2190 simulate from node.state using model\n\n    # Backpropagation\n    backpropagate reward up the tree from node\n\n# Final decision\nReturn action from root with highest visit count\n</code></pre> <p></p>"},{"location":"course_notes/model-based/#64-integration-in-the-learning-and-acting-loop","title":"6.4 Integration in the Learning and Acting Loop","text":"<p>Key integration channels:</p> <ol> <li> <p>Planning input from existing policy/value?  </p> <ul> <li>E.g., MCTS uses a prior policy to guide expansions.</li> </ul> </li> <li> <p>Planning output as a training target for the global policy/value?  </p> <ul> <li>E.g., Dyna uses imaginary transitions to update Q-values.  </li> <li>AlphaZero uses MCTS results as a learning target for \\(\\pi\\) and \\(V\\).</li> </ul> </li> <li> <p>Action selection from the planning procedure or from the learned policy?</p> <ul> <li>E.g., MPC picks the best action from a planned sequence.  </li> <li>Or a final learned policy is used if no real-time planning is feasible.</li> </ul> </li> </ol> <p></p> <p>Various combinations exist: some methods rely mostly on the learned policy but refine or correct it with a short replan (MBPO), while others do a full MCTS at every step (MuZero).</p>"},{"location":"course_notes/model-based/#65-dyna-and-dyna-style-methods","title":"6.5 Dyna and Dyna-Style Methods","text":"<p>One of the earliest and most influential frameworks for model-based RL is Dyna [Sutton, 1990]. The key insight is to integrate:</p> <ul> <li>Real experience from the environment (sampled transitions)  </li> <li>Model learning from that real data  </li> <li>Imagined experience from the learned model to augment the policy/value updates.</li> </ul> <p>Dyna Pseudocode</p> <pre><code>Input: \u03b1 (learning rate), \u03b3 (discount factor), \u03b5 (exploration rate), \n       n (number of planning steps), num_episodes\n\nInitialize Q(s, a) arbitrarily for all states s \u2208 S, actions a \u2208 A\nInitialize Model as an empty mapping: Model(s, a) \u2192 (r, s')\n\nfor each episode do\n    Initialize state s \u2190 starting state\n\n    while s is not terminal do\n        \u25b8 Action Selection (\u03b5-greedy)\n        With probability \u03b5: choose random action a\n        Else: choose a \u2190 argmax_a Q(s, a)\n\n        \u25b8 Real Interaction\n        Take action a, observe reward r and next state s'\n\n        \u25b8 Q-Learning Update\n        Q(s, a) \u2190 Q(s, a) + \u03b1 [ r + \u03b3 \u00b7 max_a' Q(s', a') \u2212 Q(s, a) ]\n\n        \u25b8 Model Update\n        Model(s, a) \u2190 (r, s')\n\n        \u25b8 Planning (n simulated updates)\n        for i = 1 to n do\n            Randomly select previously seen (\u015d, \u00e2)\n            (r\u0302, \u015d') \u2190 Model(\u015d, \u00e2)\n\n            Q(\u015d, \u00e2) \u2190 Q(\u015d, \u00e2) + \u03b1 [ r\u0302 + \u03b3 \u00b7 max_a' Q(\u015d', a') \u2212 Q(\u015d, \u00e2) ]\n\n        \u25b8 Move to next real state\n        s \u2190 s'\n\n\n</code></pre> <ol> <li>Real Interaction: We pick action \\(a\\) in state \\(s\\) using \\(\\epsilon\\)-greedy w.r.t. \\(Q\\).  </li> <li>Update Q from real transition \\((s,a,s',r)\\).  </li> <li>Update Model: store or learn to predict \\(\\hat{P}(s'\\mid s,a)\\), \\(\\hat{R}(s,a)\\).  </li> <li>Imagination (N_planning steps): randomly sample a state-action pair from replay or memory, query the model for \\(\\hat{s}', \\hat{r}\\). Update \\(Q\\) with that synthetic transition.</li> </ol> <p></p> <p>Benefits:       - Dyna can drastically reduce real environment interactions by effectively replaying or generating new transitions from the learned model.       - Even short rollouts or repeated \u201cone-step planning\u201d from random visited states helps refine Q-values more quickly.</p> <p>Dyna-Style in modern deep RL:       - Many algorithms (e.g., MBPO) add short-horizon imaginary transitions to an off-policy buffer.       - They differ in details: how many model steps, how they sample states for imagination, how they manage uncertainty, etc.</p>"},{"location":"course_notes/model-based/#7-modern-model-based-rl-algorithms","title":"7. Modern Model-Based RL Algorithms","text":"<p>Modern model-based RL builds on classical ideas (global/local models, MPC, iterative re-fitting) but incorporates powerful neural representations, uncertainty handling, and integrated planning-learning frameworks. Below are five influential algorithms that illustrate the state of the art in contemporary MBRL.</p>"},{"location":"course_notes/model-based/#71-world-models-ha-schmidhuber-2018","title":"7.1 World Models (Ha &amp; Schmidhuber, 2018)","text":"<p>Core Idea Train a latent generative model of the environment (specifically from high-dimensional inputs like images), and then learn or optimize a policy entirely within this learned latent space\u2014the so-called \u201cdream environment.\u201d</p> <p></p> <p>Key Components</p> <ol> <li> <p>Variational Autoencoder (VAE):  </p> <ul> <li>Maps raw observation \\(\\mathbf{o}_t\\) to a compact latent representation \\(\\mathbf{z}_t\\).  </li> <li>\\(\\mathbf{z}_t = E_\\phi(\\mathbf{o}_t)\\) where \\(E_\\phi\\) is the learned encoder.  </li> <li>Reconstruction loss ensures \\(E_\\phi\\) and a corresponding decoder \\(D_\\phi\\) compress and reconstruct images effectively.</li> </ul> </li> <li> <p>Recurrent Dynamics Model (MDN-RNN):  </p> <ul> <li>Predicts the next latent \\(\\mathbf{z}_{t+1}\\) given \\(\\mathbf{z}_t\\) and action \\(a_t\\).  </li> <li> <p>Often parameterized as a Mixture Density Network inside an RNN:  </p> \\[   \\mathbf{z}_{t+1} \\sim p_\\theta(\\mathbf{z}_{t+1} \\mid \\mathbf{z}_t, a_t). \\] </li> <li> <p>This distribution can be modeled by a mixture of Gaussians, providing a probabilistic estimate of the next latent state.</p> </li> </ul> </li> <li> <p>Controller (Policy):  </p> <ul> <li>A small neural network \\(\\pi_\\eta\\) that outputs actions \\(a_t = \\pi_\\eta(\\mathbf{z}_t)\\) in the latent space.  </li> <li>Trained (in the original paper) via an evolutionary strategy (e.g., CMA-ES) entirely in the dream world.  </li> </ul> </li> </ol> <p>Algorithmic Flow</p> <ol> <li>Unsupervised Phase: Run a random or exploratory policy in the real environment, collect observations \\(\\mathbf{o}_1, \\mathbf{o}_2, ...\\).  </li> <li>Train the VAE to learn \\(\\mathbf{z} = E_\\phi(\\mathbf{o})\\).  </li> <li>Train the MDN-RNN on sequences \\((\\mathbf{z}_t, a_t, \\mathbf{z}_{t+1})\\).  </li> <li>\u201cDream\u201d: Roll out the MDN-RNN from random latents and evaluate candidate controllers \\(\\pi_\\eta\\).  </li> <li>Update \\(\\pi_\\eta\\) based on the dream performance (e.g., via evolutionary search).</li> </ol> <p>Significance</p> <ul> <li>Demonstrated that an agent can learn a world model of high-dimensional environments (CarRacing, VizDoom) and train policies in \u201clatent imagination.\u201d  </li> <li>Paved the way for subsequent latent-space MBRL (PlaNet, Dreamer).</li> </ul>"},{"location":"course_notes/model-based/#72-pets-chua-et-al-2018","title":"7.2 PETS (Chua et al., 2018)","text":"<p>Core Idea Probabilistic Ensembles with Trajectory Sampling (PETS) uses ensemble neural network dynamics models to capture epistemic uncertainty, combined with sampling-based planning (like the Cross-Entropy Method, CEM) for continuous control.</p> <p></p> <p>Modeling Uncertainty</p> <ul> <li>Train \\(N\\) distinct neural networks \\(\\{\\hat{P}_{\\theta_i}\\}\\), each predicting \\(\\mathbf{s}_{t+1}\\) given \\(\\mathbf{s}_t, a_t\\).  </li> <li>Each network outputs a mean \\(\\mathbf{\\mu}_i\\) and variance \\(\\mathbf{\\Sigma}_i\\) for \\(\\mathbf{s}_{t+1}\\).  </li> <li>Ensemble Disagreement can signal model uncertainty, guiding more cautious or exploratory planning.</li> </ul> <p>Planning via Trajectory Sampling </p> <ol> <li>At state \\(\\mathbf{s}_0\\), sample multiple candidate action sequences \\(\\{\\mathbf{a}_{0:H}\\}\\).  </li> <li>For each sequence, roll out in all or a subset of the ensemble models:</li> <li> \\[     \\mathbf{s}_{t+1}^{(i)} \\sim \\hat{P}_{\\theta_i}(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_t^{(i)}, a_t). \\] </li> <li> <p>Evaluate cumulative predicted reward \\(\\sum_{t=0}^{H-1} r(\\mathbf{s}_t^{(i)}, a_t)\\).  </p> </li> <li>(Optional) Refine the action distribution using CEM:  <ul> <li>Fit a Gaussian to the top-performing sequences.  </li> <li>Resample from that Gaussian, repeat until convergence.</li> </ul> </li> </ol> <p>Mathematically, the planning objective is:</p> \\[ \\max_{\\{a_0, \\ldots, a_{H-1}\\}} \\;\\; \\mathbb{E}_{\\hat{P}_{\\theta_i}}\\!\\Bigl[\\sum_{t=0}^{H-1} \\gamma^t r(\\mathbf{s}_t, a_t)\\Bigr], \\] <p>where the expectation is approximated by sampling from the ensemble.</p> <p>Significance</p> <ul> <li>Achieved strong sample efficiency on continuous control (HalfCheetah, Ant, etc.), often matching model-free baselines (SAC, PPO) with far fewer environment interactions.  </li> <li>Demonstrated the importance of probabilistic ensembling to avoid catastrophic model exploitation.</li> </ul>"},{"location":"course_notes/model-based/#73-mbpo-janner-et-al-2019","title":"7.3 MBPO (Janner et al., 2019)","text":"<p>Core Idea Model-Based Policy Optimization (MBPO) merges the Dyna-like approach (using a learned model to generate synthetic experience) with a short rollout horizon to control compounding errors. It then trains a model-free RL algorithm (Soft Actor-Critic, SAC) using both real and model-generated data.</p> <p>Algorithmic Steps</p> <ol> <li>Learn an ensemble of dynamics models \\(\\{\\hat{P}_{\\theta_i}\\}\\) from real data.  </li> <li>From each real state \\(\\mathbf{s}\\) in the replay buffer:</li> <li>Sample a short-horizon trajectory (1\u20135 steps) using \\(\\hat{P}_{\\theta_i}\\), with actions from the current policy \\(\\pi_\\phi\\).  </li> <li>Store these \u201cimagined\u201d transitions \\(\\bigl(\\mathbf{s}, a, \\hat{r}, \\mathbf{s}'\\bigr)\\) in the replay buffer.</li> <li>Train SAC on the combined real + model-generated transitions.  </li> <li>Periodically collect more real data with the updated policy, re-fit the model ensemble, and repeat.</li> </ol> <p>Key Equations</p> <ul> <li> <p>The model-based transitions:</p> \\[     \\mathbf{s}_{t+1}^\\text{model} \\sim \\hat{P}_\\theta(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_t, a_t),     \\quad     r_t^\\text{model} \\sim \\hat{R}_\\theta(\\mathbf{s}_t, a_t). \\] </li> <li> <p>The short horizon \\(H_\\text{roll}\\) is chosen to limit error accumulation, e.g. \\(H_\\text{roll} = 1\\) or \\(5\\).</p> </li> </ul> <p>Why Short Rollouts?</p> <ul> <li>Long-horizon imagination can deviate quickly from real states =&gt; inaccurate transitions.  </li> <li>By restricting to a small horizon, MBPO ensures the model is only used in near-realistic states, greatly reducing compounding bias.</li> </ul> <p>Performance</p> <ul> <li>MBPO matches or exceeds the final returns of top model-free algorithms using ~10% of the environment interactions, combining high sample efficiency with strong asymptotic performance.</li> </ul>"},{"location":"course_notes/model-based/#74-dreamer-hafner-et-al-20202023","title":"7.4 Dreamer (Hafner et al., 2020\u20132023)","text":"<p>Core Idea Learn a recurrent latent dynamics model from images, then backprop through multi-step model rollouts to train a policy and value function. Dreamer exemplifies a \u201clearned simulator + actor-critic in latent space.\u201d</p> <p></p> <p>Latent World Model</p> <ol> <li>Encoder \\(e_\\phi(\\mathbf{o}_t)\\) compresses raw observation \\(\\mathbf{o}_t\\) into a latent state \\(\\mathbf{z}_t\\).  </li> <li>Recurrent Transition \\(p_\\theta(\\mathbf{z}_{t+1}\\mid \\mathbf{z}_t, a_t)\\) predicts the next latent, plus a reward model \\(\\hat{r}_\\theta(\\mathbf{z}_t,a_t)\\).  </li> <li>Decoder \\(d_\\phi(\\mathbf{z}_t)\\) (optional) can reconstruct \\(\\mathbf{o}_t\\) for training, but not necessarily used at inference.</li> </ol> <p>Policy Learning in Imagination</p> <ul> <li> <p>An actor \\(\\pi_\\psi(a_t\\mid \\mathbf{z}_t)\\) and critic \\(V_\\psi(\\mathbf{z}_t)\\) are learned by backprop through the latent rollouts:</p> \\[     \\max_{\\psi} \\;\\; \\mathbb{E}_{\\substack{\\mathbf{z}_0 \\sim q(\\mathbf{z}_0|\\mathbf{o}_0) \\\\ a_t \\sim \\pi_\\psi(\\cdot|\\mathbf{z}_t) \\\\ \\mathbf{z}_{t+1} \\sim p_\\theta(\\cdot|\\mathbf{z}_t,a_t)}}\\!\\biggl[\\sum_{t=0}^{H-1} \\gamma^t \\hat{r}_\\theta(\\mathbf{z}_t, a_t)\\biggr]. \\] </li> <li> <p>Dreamer uses advanced techniques (e.g., reparameterization, actor-critic with value expansion, etc.) to stabilize training.</p> </li> </ul> <p>Highlights</p> <ul> <li>DreamerV1: SOTA results on DM Control from image inputs.  </li> <li>DreamerV2: Extended to Atari, surpassing DQN with a single architecture.  </li> <li>DreamerV3: Achieved multi-domain generality (Atari, ProcGen, DM Control, robotics, Minecraft). The first algorithm to solve \u201ccollect a diamond\u201d in Minecraft from scratch without demonstrations.</li> </ul> <p>Significance</p> <ul> <li>Demonstrates that purely learning a latent world model + training by imagination can match or surpass leading model-free methods in terms of both sample efficiency and final returns.</li> </ul>"},{"location":"course_notes/model-based/#75-muzero-deepmind-2020","title":"7.5 MuZero (DeepMind, 2020)","text":"<p>Core Idea Combines Monte Carlo Tree Search (MCTS) with a learned latent state to achieve superhuman performance on Go, Chess, Shogi, and set records on Atari\u2014without knowing the environment\u2019s rules explicitly.</p> <p></p> <p>Network Architecture</p> <ol> <li>Representation Function \\(h\\):  </li> <li>Maps the observation history to a latent state \\(s_0 = h(\\mathbf{o}_{1:t})\\).  </li> <li>Dynamics Function \\(g\\):  </li> <li>Predicts the next latent state \\(s_{k+1}=g(s_k, a_k)\\) and immediate reward \\(r_k\\).  </li> <li>Prediction Function \\(f\\):  </li> <li>From a latent state \\(s_k\\), outputs a policy \\(\\pi_k\\) (action logits) and a value \\(v_k\\).</li> </ol> <p>MCTS in Latent Space</p> <ul> <li>Starting from \\(s_0\\), expand a search tree by simulating actions via \\(g\\).  </li> <li>Each node stores mean value estimates \\(\\hat{V}\\), visit counts, etc.  </li> <li>The final search policy \\(\\alpha\\) is used to update the network (target policy), and the environment reward is used to refine the reward/dynamics parameters.</li> </ul> <p>Key Equations</p> <ul> <li> <p>MuZero is trained to minimize errors in reward, value, and policy predictions:</p> \\[     \\mathcal{L}(\\theta) = \\sum_{t=1}^{T} \\bigl(\\ell_\\mathrm{value}(v_\\theta(s_t), z_t) + \\ell_\\mathrm{policy}(\\pi_\\theta(s_t), \\pi_t) + \\ell_\\mathrm{dyn}\\bigl(g_\\theta(s_t, a_t), s_{t+1}\\bigr)\\bigr), \\] </li> </ul> <p>with \\(\\pi_t\\) and \\(z_t\\) from the improved MCTS-based targets.</p> <p>Achievements</p> <ul> <li>Matches AlphaZero performance on Go, Chess, Shogi, but without an explicit rules model.  </li> <li>Set new records on Atari-57.  </li> <li>Demonstrates that an end-to-end learned model can be as effective for MCTS as a known simulator, provided it is trained to be \u201cvalue-equivalent\u201d (predict future rewards and values accurately).</li> </ul> <p>Impact</p> <ul> <li>Showed that \u201clearning to model the environment\u2019s reward and value structure is enough\u201d\u2014MuZero does not need pixel-perfect next-state reconstructions.  </li> <li>Successfully extended MCTS-based planning to domains with unknown or complex dynamics.</li> </ul>"},{"location":"course_notes/model-based/#8-key-benefits-and-drawbacks-of-mbrl","title":"8. Key Benefits (and Drawbacks) of MBRL","text":""},{"location":"course_notes/model-based/#81-data-efficiency","title":"8.1 Data Efficiency","text":"<p>MBRL can yield higher sample efficiency:</p> <ul> <li>Simulating transitions in the model extracts more learning signal from each real step  </li> <li>E.g., PETS, MBPO, Dreamer require fewer environment interactions than top model-free methods</li> </ul>"},{"location":"course_notes/model-based/#82-exploration","title":"8.2 Exploration","text":"<p>A learned uncertainty-aware model can direct exploration to uncertain states:</p> <ul> <li>Bayesian or ensemble-based MBRL  </li> <li>Potentially more efficient than naive \\(\\epsilon\\)-greedy in high dimensions</li> </ul>"},{"location":"course_notes/model-based/#83-optimality","title":"8.3 Optimality","text":"<p>With a perfect model, MBRL can find better or equal policies vs. model-free. But if the model is imperfect, compounding errors can lead to suboptimal solutions. Research aims to close that gap (MBPO, Dreamer, MuZero).</p>"},{"location":"course_notes/model-based/#84-transfer","title":"8.4 Transfer","text":"<p>A global dynamics model can be re-used across tasks or reward functions:</p> <ul> <li>E.g., a learned robotic physics model can quickly adapt to new goals</li> <li>Saves extensive retraining</li> </ul>"},{"location":"course_notes/model-based/#85-safety","title":"8.5 Safety","text":"<p>In real-world tasks (robotics, healthcare), we can plan or verify constraints inside the model before acting. Uncertainty estimation is crucial.</p>"},{"location":"course_notes/model-based/#86-explainability","title":"8.6 Explainability","text":"<p>A learned model can sometimes be probed or visualized, offering partial interpretability (though deep generative models remain somewhat opaque).</p>"},{"location":"course_notes/model-based/#87-disbenefits","title":"8.7 Disbenefits","text":"<ol> <li>Model bias: Imperfect models =&gt; compounding errors  </li> <li>Computational overhead: Planning can be expensive  </li> <li>Implementation complexity: We must keep models accurate, stable, and do policy updates in tandem</li> </ol>"},{"location":"course_notes/model-based/#9-conclusion","title":"9. Conclusion","text":"<p>Model-Based RL integrates planning and learning in the RL framework, offering strong sample efficiency and structured decision-making. Algorithms like MBPO, Dreamer, and MuZero demonstrate that short rollouts, uncertainty estimates, or latent value-equivalent models can yield high final performance with fewer real samples.</p> <p>Still, challenges remain:</p> <ul> <li>Robustness under partial observability, stochastic transitions, or non-stationary tasks  </li> <li>Balancing planning vs. data collection adaptively  </li> <li>Scaling to high-dimensional, real-world tasks with safety constraints</li> </ul> <p>Future work includes deeper hierarchical methods, advanced uncertainty modeling, bridging theory and practice, and constructing more interpretable or structured models.</p>"},{"location":"course_notes/model-based/#10-references","title":"10. References","text":"<ul> <li> <p>S. Levine (CS 294-112: Deep RL) Model-Based Reinforcement Learning, Lecture 9 slides. Lecture Site, Video Repository Additional resources: Open course materials from UC Berkeley\u2019s Deep RL class </p> </li> <li> <p>T. Moerland et al. (2022) \u201cModel-based Reinforcement Learning: A Survey.\u201d arXiv:2006.16712v4 Additional resources: Official GitHub for references and code snippets mentioned in the paper </p> </li> <li> <p>Sutton, R.S. &amp; Barto, A.G. Reinforcement Learning: An Introduction (2nd edition). MIT Press, 2018. Online Draft Additional resources: Exercise solutions and discussion forum </p> </li> <li> <p>Puterman, M.L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley &amp; Sons, 2014. Publisher Link Additional resources: Various lecture slides summarizing MDP fundamentals </p> </li> <li> <p>Deisenroth, M. &amp; Rasmussen, C.E. (2011) PILCO: A Model-Based and Data-Efficient Approach to Policy Search. ICML. Paper PDF Additional resources: Official code release on GitHub </p> </li> <li> <p>Chua, K. et al. (2018) \u201cDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models (PETS).\u201d NeurIPS. Paper Link Additional resources: Author\u2019s implementation </p> </li> <li> <p>Janner, M. et al. (2019) \u201cWhen to Trust Your Model: Model-Based Policy Optimization.\u201d NeurIPS (MBPO). Paper Link Additional resources: Berkeley AI Research blog post </p> </li> <li> <p>Hafner, D. et al. (2020\u20132023) \u201cDreamer\u201d line of papers (ICML, arXiv). DreamerV2 Code, Dreamer Blog Additional resources: Tutorial videos by Danijar Hafner on latent world models </p> </li> <li> <p>Ha, D. &amp; Schmidhuber, J. (2018) \u201cWorld Models.\u201d NeurIPS. Paper PDF, Project Site Additional resources: Interactive demos and blog articles from David Ha </p> </li> <li> <p>Silver, D. et al. (various) AlphaGo, AlphaZero, MuZero lines of research. DeepMind\u2019s MuZero Blog Additional resources: Further reading on MCTS, AlphaGo, and AlphaZero in \u201cMastering the Game of Go\u201d series </p> </li> </ul>"},{"location":"course_notes/multi-agent/","title":"Multi-Agent RL","text":""},{"location":"course_notes/offline/","title":"Offline RL","text":""},{"location":"course_notes/policy-based/","title":"Week 3: Policy-Based Methods","text":"<p>Reinforcement Learning (RL) focuses on training an agent to interact with an environment by learning a policy \\(\\pi_{\\theta}(a | s)\\) that maximizes the cumulative reward. Policy gradient methods are a class of algorithms that directly optimize the policy by adjusting the parameters \\(\\theta\\) via gradient ascent.</p>"},{"location":"course_notes/policy-based/#why-policy-gradient-methods","title":"Why Policy Gradient Methods?","text":"<p>Unlike value-based methods (e.g., Q-learning), which rely on estimating value functions, policy gradient methods: - Can naturally handle stochastic policies, which are crucial in environments requiring exploration.</p> <ul> <li> <p>Work well in continuous action spaces, where discrete action methods become infeasible.</p> </li> <li> <p>Can directly optimize differentiable policy representations, such as neural networks.</p> </li> <li> <p>Avoid the need for an explicit action-value function approximation, making them more robust in high-dimensional problems.</p> </li> <li> <p>Are capable of optimizing parameterized policies without relying on action selection heuristics.</p> </li> <li> <p>Can incorporate entropy regularization to improve exploration and prevent premature convergence to suboptimal policies.</p> </li> <li> <p>Allow for more stable convergence in some cases compared to value-based methods, which may suffer from instability due to bootstrapping.</p> </li> <li> <p>Can leverage variance reduction techniques (e.g., advantage estimation, baseline subtraction) to improve learning efficiency.</p> </li> </ul>"},{"location":"course_notes/policy-based/#policy-gradient","title":"Policy Gradient","text":"<p>The goal of reinforcement learning is to find an optimal behavior strategy for the agent to obtain optimal rewards. The policy gradient methods target at modeling and optimizing the policy directly. The policy is usually modeled with a parameterized function respect to \\(\\theta\\), \\(\\pi_{\\theta}(a|s)\\). The value of the reward (objective) function depends on this policy and then various algorithms can be applied to optimize \\(\\theta\\) for the best reward.</p> <p>The reward function is defined as:</p> \\[J(\\theta) = \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) V^{\\pi}(s) = \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in  \\mathcal{A}} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a)\\] <p>where \\(d^{\\pi}(s)\\) is the stationary distribution of Markov chain for \\(\\pi_{\\theta}\\) (on-policy state distribution under \\(\\pi\\)). For simplicity, the parameter \\(\\theta\\) would be omitted for the policy \\(\\pi_{\\theta}\\) when the policy is present in the subscript of other functions; for example, \\(d^{\\pi}\\) and \\(Q^{\\pi}\\) should be \\(d^{\\pi_{\\theta}}\\) and \\(Q^{\\pi_{\\theta}}\\) if written in full.</p> <p>Imagine that you can travel along the Markov chain's states forever, and eventually, as the time progresses, the probability of you ending up with one state becomes unchanged --- this is the stationary probability for \\(\\pi_{\\theta}\\). \\(d^{\\pi}(s) = \\lim_{t \\to  \\infty} P(s_t = s | s_0, \\pi_{\\theta})\\) is the probability that \\(s_t = s\\) when starting from \\(s_0\\) and following policy \\(\\pi_{\\theta}\\) for \\(t\\) steps. Actually, the existence of the stationary distribution of Markov chain is one main reason for why PageRank algorithm works.</p> <p>It is natural to expect policy-based methods are more useful in the continuous space. Because there is an infinite number of actions and (or) states to estimate the values for and hence value-based approaches are way too expensive computationally in the continuous space. For example, in generalized policy iteration, the policy improvement step \\(\\arg  \\max_{a \\in  \\mathcal{A}} Q^{\\pi}(s,a)\\) requires a full scan of the action space, suffering from the curse of dimensionality.</p> <p>Using gradient ascent, we can move \\(\\theta\\) toward the direction suggested by the gradient \\(\\nabla_{\\theta} J(\\theta)\\) to find the best \\(\\theta\\) for \\(\\pi_{\\theta}\\) that produces the highest return.</p>"},{"location":"course_notes/policy-based/#policy-gradient-theorem","title":"Policy Gradient Theorem","text":"<p>Computing the gradient \\(\\nabla_{\\theta}J(\\theta)\\) is tricky because it depends on both the action selection (directly determined by \\(\\pi_{\\theta}\\)) and the stationary distribution of states following the target selection behavior (indirectly determined by \\(\\pi_{\\theta}\\)). Given that the environment is generally unknown, it is difficult to estimate the effect on the state distribution by a policy update.</p> <p>Luckily, the policy gradient theorem comes to save the world!  It provides a nice reformation of the derivative of the objective function to not involve the derivative of the state distribution \\(d^{\\pi}(\\cdot)\\) and simplify the gradient computation \\(\\nabla_{\\theta}J(\\theta)\\) a lot.</p> \\[\\nabla_{\\theta}J(\\theta) = \\nabla_{\\theta} \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in  \\mathcal{A}} Q^{\\pi}(s,a) \\pi_{\\theta}(a|s)\\] \\[\\propto  \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in  \\mathcal{A}} Q^{\\pi}(s,a) \\nabla_{\\theta} \\pi_{\\theta}(a|s)\\]"},{"location":"course_notes/policy-based/#proof-of-policy-gradient-theorem","title":"Proof of Policy Gradient Theorem","text":"<p>This session is pretty dense, as it is the time for us to go through the proof and figure out why the policy gradient theorem is correct.</p> Warning <p>This proof may be unnecessary for the first phase of the course. </p> proof <p>We first start with the derivative of the state value function:</p> \\[ \\begin{aligned} \\nabla_{\\theta} V^{\\pi}(s) &amp;= \\nabla_{\\theta} \\left( \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\right) \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\nabla_{\\theta} Q^{\\pi}(s,a) \\right) \\quad \\text{; Derivative product rule.} \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\nabla_{\\theta} \\sum_{s', r} P(s',r|s,a) (r + V^{\\pi}(s')) \\right) \\quad \\text{; Extend } Q^{\\pi} \\text{ with future state value.} \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\sum_{s',r} P(s',r|s,a) \\nabla_{\\theta} V^{\\pi}(s') \\right) \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\sum_{s'} P(s'|s,a) \\nabla_{\\theta} V^{\\pi}(s') \\right) \\quad \\text{; Because } P(s'|s,a) = \\sum_{r} P(s',r|s,a) \\end{aligned} \\] <p>Now we have:</p> \\[ \\begin{aligned} \\nabla_{\\theta} V^{\\pi}(s) &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\sum_{s'} P(s'|s,a) \\nabla_{\\theta} V^{\\pi}(s') \\right) \\end{aligned} \\] <p>This equation has a nice recursive form, and the future state value function \\(V^{\\pi}(s')\\) can be repeatedly unrolled by following the same equation.</p> <p>Let's consider the following visitation sequence and label the probability of transitioning from state \\(s\\) to state \\(x\\) with policy \\(\\pi_{\\theta}\\) after \\(k\\) steps as \\(\\rho^{\\pi}(s \\to x, k)\\).</p> \\[ s \\xrightarrow{a \\sim \\pi_{\\theta}(\\cdot | s)} s' \\xrightarrow{a' \\sim \\pi_{\\theta}(\\cdot | s')} s'' \\xrightarrow{a'' \\sim \\pi_{\\theta}(\\cdot | s'')} \\dots \\] <ul> <li> <p>When \\(k = 0\\): \\(\\rho^{\\pi}(s \\to s, k = 0) = 1\\).</p> </li> <li> <p>When \\(k = 1\\), we scan through all possible actions and sum up the transition probabilities to the target state:</p> </li> </ul> \\[ \\rho^{\\pi}(s \\to s', k = 1) = \\sum_{a} \\pi_{\\theta}(a|s) P(s'|s,a). \\] <ul> <li>Imagine that the goal is to go from state \\(s\\) to \\(x\\) after \\(k+1\\) steps while following policy \\(\\pi_{\\theta}\\). We can first travel from \\(s\\) to a middle point \\(s'\\) (any state can be a middle point, \\(s' \\in S\\)) after \\(k\\) steps and then go to the final state \\(x\\) during the last step. In this way, we are able to update the visitation probability recursively:</li> </ul> \\[ \\rho^{\\pi}(s \\to x, k + 1) = \\sum_{s'} \\rho^{\\pi}(s \\to s', k) \\rho^{\\pi}(s' \\to x, 1). \\] <p>Then we go back to unroll the recursive representation of \\(\\nabla_{\\theta}V^{\\pi}(s)\\). Let</p> \\[ \\phi(s) = \\sum_{a \\in \\mathcal{A}} \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\] <p>to simplify the equations. If we keep on extending \\(\\nabla_{\\theta}V^{\\pi}(\\cdot)\\) infinitely, it is easy to find out that we can transition from the starting state \\(s\\) to any state after any number of steps in this unrolling process and by summing up all the visitation probabilities, we get \\(\\nabla_{\\theta}V^{\\pi}(s)\\)!</p> \\[ \\begin{aligned} \\nabla_{\\theta}V^{\\pi}(s) &amp;= \\phi(s) + \\sum_{a} \\pi_{\\theta}(a|s) \\sum_{s'} P(s'|s,a) \\nabla_{\\theta}V^{\\pi}(s') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\sum_{a} \\pi_{\\theta}(a|s) P(s'|s,a) \\nabla_{\\theta}V^{\\pi}(s') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\nabla_{\\theta}V^{\\pi}(s') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s') Q^{\\pi}(s',a) + \\pi_{\\theta}(a|s') \\sum_{s''} P(s''|s',a) \\nabla_{\\theta}V^{\\pi}(s'') \\right) \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\left[ \\phi(s') + \\sum_{s''} \\rho^{\\pi}(s' \\to s'', 1) \\nabla_{\\theta}V^{\\pi}(s'') \\right] \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\phi(s') + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\sum_{s''} \\rho^{\\pi}(s' \\to s'', 1) \\nabla_{\\theta}V^{\\pi}(s'') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\phi(s') + \\sum_{s''} \\rho^{\\pi}(s \\to s'', 2) \\nabla_{\\theta}V^{\\pi}(s'') \\quad \\text{; Consider } s' \\text{ as the middle point for } s \\to s''. \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\phi(s') + \\sum_{s''} \\rho^{\\pi}(s \\to s'', 2) \\phi(s'') + \\sum_{s'''} \\rho^{\\pi}(s \\to s''', 3) \\nabla_{\\theta}V^{\\pi}(s''') \\\\ &amp;= \\dots \\quad \\text{; Repeatedly unrolling the part of } \\nabla_{\\theta}V^{\\pi}(\\cdot) \\\\ &amp;= \\sum_{x \\in \\mathcal{S}} \\sum_{k=0}^{\\infty} \\rho^{\\pi}(s \\to x, k) \\phi(x) \\end{aligned} \\] <p>The nice rewriting above allows us to exclude the derivative of Q-value function, \\(\\nabla_{\\theta} Q^{\\pi}(s,a)\\). By plugging it into the objective function \\(J(\\theta)\\), we are getting the following:</p> \\[ \\begin{aligned} \\nabla_{\\theta}J(\\theta) &amp;= \\nabla_{\\theta}V^{\\pi}(s_0) \\\\ &amp;= \\sum_{s} \\sum_{k=0}^{\\infty} \\rho^{\\pi}(s_0 \\to s, k) \\phi(s) \\quad \\text{; Starting from a random state } s_0 \\\\ &amp;= \\sum_{s} \\eta(s) \\phi(s) \\quad \\text{; Let } \\eta(s) = \\sum_{k=0}^{\\infty} \\rho^{\\pi}(s_0 \\to s, k) \\\\ &amp;= \\left( \\sum_{s} \\eta(s) \\right) \\sum_{s} \\frac{\\eta(s)}{\\sum_{s} \\eta(s)} \\phi(s) \\quad \\text{; Normalize } \\eta(s), s \\in \\mathcal{S} \\text{ to be a probability distribution.} \\\\ &amp;\\propto \\sum_{s} \\frac{\\eta(s)}{\\sum_{s} \\eta(s)} \\phi(s) \\quad \\text{; } \\sum_{s} \\eta(s) \\text{ is a constant} \\\\ &amp;= \\sum_{s} d^{\\pi}(s) \\sum_{a} \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\quad d^{\\pi}(s) = \\frac{\\eta(s)}{\\sum_{s} \\eta(s)} \\text{ is stationary distribution.} \\end{aligned} \\] <p>In the episodic case, the constant of proportionality (\\(\\sum_{s} \\eta(s)\\)) is the average length of an episode; in the continuing case, it is 1. The gradient can be further written as:</p> \\[ \\begin{aligned} \\nabla_{\\theta}J(\\theta) &amp;\\propto \\sum_{s \\in \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in \\mathcal{A}} Q^{\\pi}(s,a) \\nabla_{\\theta} \\pi_{\\theta}(a|s) \\\\ &amp;= \\sum_{s \\in \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\frac{\\nabla_{\\theta} \\pi_{\\theta}(a|s)}{\\pi_{\\theta}(a|s)} \\quad \\text{; Because } \\ln(x)'=1/x \\\\ &amp;= \\mathbb{E}_{\\pi} [Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a|s)] \\end{aligned} \\] <p>Where \\(\\mathbb{E}_{\\pi}\\) refers to \\(\\mathbb{E}_{s \\sim d^{\\pi}, a \\sim \\pi_{\\theta}}\\) when both state and action distributions follow the policy \\(\\pi_{\\theta}\\) (on policy).</p> <p>The policy gradient theorem lays the theoretical foundation for various policy gradient algorithms. This vanilla policy gradient update has no bias but high variance. Many following algorithms were proposed to reduce the variance while keeping the bias unchanged.</p> \\[ \\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{\\pi} [Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a|s)] \\]"},{"location":"course_notes/policy-based/#policy-gradient-in-continuous-action-space","title":"Policy Gradient in Continuous Action Space","text":"<p>In a continuous action space, the policy gradient theorem is given by:</p> \\[\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{s \\sim d^{\\pi}, a \\sim  \\pi_{\\theta}} \\left[ Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s) \\right]\\] <p>Since the action space is continuous, the summation over actions in the discrete case is replaced by an integral:</p> \\[\\nabla_{\\theta} J(\\theta) = \\int_{\\mathcal{S}} d^{\\pi}(s) \\int_{\\mathcal{A}} Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s) \\pi_{\\theta}(a|s) \\, da \\, ds\\] <p>where:</p> <ul> <li> <p>\\(d^{\\pi}(s)\\) is the stationary state distribution under policy \\(\\pi_{\\theta}\\),</p> </li> <li> <p>\\(\\pi_{\\theta}(a|s)\\) is the probability density function for the continuous action \\(a\\) given state \\(s\\),</p> </li> <li> <p>\\(Q^{\\pi}(s,a)\\) is the state-action value function,</p> </li> <li> <p>\\(\\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s)\\) is the score function (policy gradient term),</p> </li> <li> <p>The integral is taken over all possible states \\(s\\) and actions \\(a\\).</p> </li> </ul> Gaussian Policy Example <p>A common choice for a continuous policy is a Gaussian distribution:</p> \\[a \\sim  \\pi_{\\theta}(a|s) = \\mathcal{N}(\\mu_{\\theta}(s), \\Sigma_{\\theta}(s))\\] <p>where:</p> <ul> <li> <p>\\(\\mu_{\\theta}(s)\\) is the mean of the action distribution, parameterized by \\(\\theta\\),</p> </li> <li> <p>\\(\\Sigma_{\\theta}(s)\\) is the covariance matrix (often assumed diagonal or fixed).</p> </li> </ul> <p>For a Gaussian policy, the logarithm of the probability density is:</p> \\[\\ln  \\pi_{\\theta}(a|s) = -\\frac{1}{2} (a - \\mu_{\\theta}(s))^T \\Sigma_{\\theta}^{-1} (a - \\mu_{\\theta}(s)) - \\frac{1}{2} \\ln |\\Sigma_{\\theta}|\\] <p>Taking the gradient:</p> \\[\\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s) = \\Sigma_{\\theta}^{-1} (a - \\mu_{\\theta}(s)) \\nabla_{\\theta} \\mu_{\\theta}(s)\\] <p>Thus, the policy gradient update becomes:</p> \\[\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s \\sim d^{\\pi}, a \\sim  \\pi_{\\theta}} \\left[ Q^{\\pi}(s,a) \\Sigma_{\\theta}^{-1} (a - \\mu_{\\theta}(s)) \\nabla_{\\theta} \\mu_{\\theta}(s) \\right]\\]"},{"location":"course_notes/policy-based/#reinforce","title":"REINFORCE","text":"<p>REINFORCE (Monte-Carlo policy gradient) relies on an estimated return by Monte-Carlo methods using episode samples to update the policy parameter \\(\\theta\\). REINFORCE works because the expectation of the sample gradient is equal to the actual gradient:</p> \\[ \\begin{aligned} \\nabla_{\\theta}J(\\theta) &amp;= \\mathbb{E}_{\\pi} \\left[ Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a|s) \\right] \\\\ &amp;= \\mathbb{E}_{\\pi} \\left[ G_t \\nabla_{\\theta} \\ln \\pi_{\\theta}(A_t|S_t) \\right] \\quad \\text{; Because } Q^{\\pi}(S_t, A_t) = \\mathbb{E}_{\\pi} \\left[ G_t \\mid S_t, A_t \\right] \\end{aligned} \\] <p>where \\(G_t = \\sum_{k=0}^{\\infty} \\gamma^kR_{t+k+1}\\) is the discounted future reward starting from time setp \\(t\\).</p> <p>Therefore we are able to measure \\(G_t\\) from real sample trajectories and use that to update our policy gradient. It relies on a full trajectory and that's why it is a Monte-Carlo method.</p>"},{"location":"course_notes/policy-based/#algorithm","title":"Algorithm","text":"<p>The process is pretty straightforward:</p> <ol> <li> <p>Initialize the policy parameter \\(\\theta\\) at random.</p> </li> <li> <p>Generate one trajectory on policy \\(\\pi_{\\theta}\\): \\(S_1, A_1, R_2, S_2, A_2, \\dots, S_T\\).</p> </li> <li> <p>For \\(t = 1, 2, \\dots, T\\):</p> <ol> <li> <p>Estimate the return \\(G_t\\);</p> </li> <li> <p>Update policy parameters: \\(\\theta  \\leftarrow  \\theta + \\alpha  \\gamma^t G_t \\nabla_{\\theta} \\ln  \\pi_{\\theta}(A_t|S_t)\\)</p> </li> </ol> </li> </ol> <p>A widely used variation of REINFORCE is to subtract a baseline value from the return \\(G_t\\) to reduce the variance of gradient estimation while keeping the bias unchanged (Remember we always want to do this when possible).</p> <p>For example, a common baseline is to subtract state-value from action-value, and if applied, we would use advantage \\(A(s,a) = Q(s,a) - V(s)\\) in the gradient ascent update. This post nicely explained why a baseline works for reducing the variance, in addition to a set of fundamentals of policy gradient.</p>"},{"location":"course_notes/policy-based/#bias-and-variance","title":"Bias and Variance","text":"<p>As introduced in previous sections, REINFORCE employs Monte Carlo estimation of returns. Recall that Monte Carlo methods estimate expected values by sampling trajectories from the environment. While these estimators are unbiased (they converge to the true expected value given enough samples), they often suffer from high variance, making policy gradient methods challenging to stabilize.</p> <p>In this section, we delve deeper into the bias-variance tradeoff in reinforcement learning, with a focus on policy gradient methods. While these concepts were mentioned earlier, we now analyze them as the central topic:</p> <p>Bias occurs when our gradient estimates systematically deviate from the true expected gradient, leading to suboptimal policy updates.</p> <p>Variance measures how much our gradient estimates fluctuate across different batches of samples, affecting training stability.</p> <p>Policy gradient methods face unique challenges:</p> <ul> <li> <p>High variance from Monte Carlo sampling of full trajectories.</p> </li> <li> <p>Potential bias from function approximation (e.g., neural networks) or improper baselines.</p> </li> </ul> <p>Below, we formalize these concepts and explore techniques to mitigate their effects.</p>"},{"location":"course_notes/policy-based/#monte-carlo-estimators-in-reinforcement-learning","title":"Monte Carlo Estimators in Reinforcement Learning","text":"<p>A Monte Carlo estimator is a method used to approximate the expected value of a function \\(f(X)\\) over a random variable \\(X\\) with a given probability distribution \\(p(X)\\). The true expectation is:</p> \\[E[f(X)] = \\int f(x) p(x) \\, dx\\] <p>However, directly computing this integral may be complex. Instead, we use Monte Carlo estimation by drawing \\(N\\) independent samples \\(X_1, X_2, \\dots, X_N\\) from \\(p(X)\\) and computing:</p> \\[\\hat{\\mu}_{MC} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i)\\] <p>This estimator provides an approximation to the true expectation \\(E[f(X)]\\).</p> <p>By the law of large numbers (LLN), as \\(N \\to  \\infty\\), we have:</p> \\[\\hat{X}_N \\to  \\mathbb{E}[X] \\quad  \\text{(almost surely)}\\] <p>Monte Carlo methods are commonly used in RL for estimating expected rewards, state-value functions, and action-value functions.</p>"},{"location":"course_notes/policy-based/#bias-in-policy-gradient-methods","title":"Bias in Policy Gradient Methods","text":"<p>Bias in reinforcement learning arises when an estimator systematically deviates from the true value. In policy gradient methods, bias is introduced due to function approximation, reward estimation, or gradient computation errors.</p>"},{"location":"course_notes/policy-based/#sources-of-bias","title":"Sources of Bias","text":"<ul> <li> <p>Function Approximation Bias: Policy gradient methods often rely on neural networks or other function approximators for policy representation. Imperfect approximations introduce systematic errors, leading to biased policy updates.</p> </li> <li> <p>Reward Clipping or Discounting: Algorithms using reward clipping or high discount factors (\\(\\gamma\\)) can distort return estimates, causing the learned policy to be biased toward short-term rewards.</p> </li> <li> <p>Baseline Approximation: Variance reduction techniques like baseline subtraction use estimates of expected returns. If the baseline is inaccurately estimated, it introduces bias in the policy gradient computation.</p> </li> </ul> Example of Bias <p>Consider a self-driving car optimizing for fuel efficiency. If the reward function prioritizes immediate fuel consumption over long-term efficiency, the learned policy may favor suboptimal strategies that minimize fuel use in the short term while missing globally optimal driving behaviors.</p>"},{"location":"course_notes/policy-based/#biased-vs-unbiased-estimation","title":"Biased vs. Unbiased Estimation","text":"<p>For example: The biased formula for the sample variance \\(S^2\\) is given by:</p> \\[S^2_{\\text{biased}} = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\overline{X})^2\\] <p>This is an underestimation of the true population variance \\(\\sigma^2\\) because it does not account for the degrees of freedom in estimation.</p> <p>Instead, the unbiased estimator is:</p> \\[S^2_{\\text{unbiased}} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\overline{X})^2.\\] <p>This unbiased estimator correctly accounts for variance in small sample sizes, ensuring \\(\\mathbb{E}[S^2_{\\text{unbiased}}] = \\sigma^2\\).</p>"},{"location":"course_notes/policy-based/#variance-in-policy-gradient-methods","title":"Variance in Policy Gradient Methods","text":"<p>Variance in policy gradient estimates refers to fluctuations in gradient estimates across different training episodes. High variance leads to instability and slow convergence.</p>"},{"location":"course_notes/policy-based/#sources-of-variance","title":"Sources of Variance","text":"<ul> <li> <p>Monte Carlo Estimation: REINFORCE estimates gradients using complete episodes, leading to high variance due to trajectory randomness.</p> </li> <li> <p>Stochastic Policy Outputs: Policies represented as probability distributions (e.g., Gaussian policies) introduce additional randomness in gradient updates.</p> </li> <li> <p>Exploration Strategies: Methods like softmax or epsilon-greedy increase variance by adding stochasticity to action selection.</p> </li> </ul> Example of Variance <p>Consider a robotic arm learning to grasp objects. Due to high variance, in some episodes, it succeeds, while in others, minor variations cause failure. These inconsistencies slow down convergence.</p>"},{"location":"course_notes/policy-based/#techniques-to-reduce-variance-in-policy-gradient-methods","title":"Techniques to Reduce Variance in Policy Gradient Methods","text":"<p>Several strategies help mitigate variance in policy gradient methods while preserving unbiased gradient estimates.</p>"},{"location":"course_notes/policy-based/#baseline-subtraction","title":"Baseline Subtraction","text":"<p>A baseline function \\(b\\) reduces variance without introducing bias:</p> \\[\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_t | s_t) (G_t - b) \\right].\\] <p>A common choice for \\(b\\) is the average return over trajectories:</p> \\[b = \\frac{1}{N} \\sum_{i=1}^{N} G_i.\\] <p>Since \\(b\\) is independent of actions, it does not introduce bias in the gradient estimate while reducing variance. A simple proof for this is illustrated bleow.</p> proof \\[\\begin{aligned} E\\left[\\nabla_\\theta  \\log p_\\theta(\\tau) b\\right] &amp;= \\int p_\\theta(\\tau) \\nabla_\\theta  \\log p_\\theta(\\tau) b \\, d\\tau \\\\ &amp;= \\int  \\nabla_\\theta p_\\theta(\\tau) b \\, d\\tau \\\\ &amp;= b \\nabla_\\theta  \\int p_\\theta(\\tau) \\, d\\tau \\\\ &amp;= b \\nabla_\\theta  1 \\\\ &amp;= 0 \\end{aligned}\\]"},{"location":"course_notes/policy-based/#causality-trick-and-reward-to-go-estimation","title":"Causality Trick and Reward-to-Go Estimation","text":"<p>To ensure that policy updates at time \\(t\\) are only influenced by rewards from that time step onward, we use the causality trick:</p> \\[\\nabla_{\\theta} J(\\theta) \\approx  \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_{i,t} | s_{i,t}) \\left( \\sum_{t'=t}^{T} r(a_{i,t'}, s_{i,t'}) \\right).\\] <p>Instead of summing over all rewards, the reward-to-go estimate restricts the sum to future rewards only:</p> \\[Q(s_t, a_t) = \\sum_{t'=t}^{T} \\mathbb{E}_{\\pi_{\\theta}} [r(s_{t'}, a_{t'}) | s_t, a_t].\\] \\[\\nabla_{\\theta} J(\\theta) \\approx  \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_{i,t} | s_{i,t}) Q(s_{i,t}, a_{i,t}).\\] <p>This prevents rewards from future time steps from affecting past actions, reducing variance. This approach results in much lower variance compared to the traditional Monte Carlo methods.</p> proof \\[ \\begin{aligned} A_{t_0-1} &amp;= s_{t_0-1}, a_{t_0-1}, \\dots, a_0, s_0 \\\\ \\mathbb{E}_{A_{t_0-1}} &amp;\\left[ \\mathbb{E}_{s_{t_0}, a_{t_0} | A_{t_0-1}} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\sum_{t=0}^{t_0 - 1} r(s_t, a_t) \\right] \\right] \\\\ U_{t_0-1} &amp;= \\sum_{t=0}^{t_0 - 1} r(s_t, a_t) \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0}, a_{t_0} | s_{t_0-1}, a_{t_0-1}} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0} | s_{t_0-1}, a_{t_0-1}} \\mathbb{E}_{a_{t_0} | s_{t_0-1}, a_{t_0-1}, s_{t_0}} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0} | s_{t_0-1}, a_{t_0-1}} \\mathbb{E}_{a_{t_0} | s_{t_0}} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0} | s_{t_0-1}, a_{t_0-1}} \\mathbb{E}_{\\pi_{\\theta} (a_{t_0} | s_{t_0})} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ \\mathbb{E}_{\\pi_{\\theta} (a_{t_0} | s_{t_0})} &amp;\\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) = 0 \\\\ \\mathbb{E}_{A_{t_0-1}}&amp; \\left[ \\mathbb{E}_{s_{t_0}, a_{t_0} | A_{t_0-1}} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\sum_{t=0}^{t_0 - 1} r(s_t, a_t) \\right] \\right] = 0 \\end{aligned} \\]"},{"location":"course_notes/policy-based/#discount-factor-adjustment","title":"Discount Factor Adjustment","text":"<p>The discount factor \\(\\gamma\\) helps reduce variance by weighting rewards closer to the present more heavily:</p> \\[G_t = \\sum_{t' = t}^{T} \\gamma^{t'-t} r(s_{t'}, a_{t'}).\\] proof \\[ \\begin{aligned} \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\left( \\sum_{t' = t}^{T} \\gamma^{t' - t} r(s_{i,t'}, a_{i,t'}) \\right) \\\\ \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\right) \\left( \\sum_{t=1}^{T} \\gamma^{t-1} r(s_{i,t}, a_{i,t}) \\right) \\\\ \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\left( \\sum_{t' = t}^{T} \\gamma^{t' - t} r(s_{i,t'}, a_{i,t'}) \\right) \\\\ \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\gamma^{t-1} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\left( \\sum_{t' = t}^{T} \\gamma^{t' - t} r(s_{i,t'}, a_{i,t'}) \\right) \\end{aligned} \\] <p>A lower \\(\\gamma\\) (e.g., 0.9) reduces variance but increases bias, while a higher \\(\\gamma\\) (e.g., 0.99) improves long-term estimation but increases variance. A balance is needed.</p>"},{"location":"course_notes/policy-based/#advantage-estimation-and-actor-critic-methods","title":"Advantage Estimation and Actor-Critic Methods","text":"<p>Actor-critic methods combine policy optimization (actor) with value function estimation (critic). The advantage function is defined as:</p> \\[A^{\\pi}(s_t, a_t) = Q^{\\pi}(s_t, a_t) - V^{\\pi}(s_t),\\] <p>where the action-value function is:</p> \\[Q^{\\pi}(s_t, a_t) = \\sum_{t' = t}^{T} \\mathbb{E}_{\\pi} [r(s_{t'}, a_{t'}) | s_t, a_t],\\] <p>and the state-value function is:</p> \\[V^{\\pi}(s_t) = \\mathbb{E}_{a_t \\sim  \\pi_{\\theta}(a_t | s_t)} [Q^{\\pi}(s_t, a_t)].\\] <p>The policy gradient update using the advantage function becomes:</p> \\[\\nabla_{\\theta} J(\\theta) \\approx  \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_{i,t} | s_{i,t}) A^{\\pi}(s_{i,t}, a_{i,t}).\\] <p>This formulation allows for lower variance in policy updates while leveraging learned state-value estimates. Actor-critic methods are widely used in modern reinforcement learning due to their stability and efficiency.</p>"},{"location":"course_notes/policy-based/#actor-critic","title":"Actor-Critic","text":"<p>Two main components in policy gradient methods are the policy model and the value function. It makes a lot of sense to learn the value function in addition to the policy since knowing the value function can assist the policy update, such as by reducing gradient variance in vanilla policy gradients. That is exactly what the Actor-Critic method does.</p> <p>Actor-Critic methods consist of two models, which may optionally share parameters:</p> <ul> <li> <p>Critic: Updates the value function parameters \\(w\\). Depending on the algorithm, it could be an action-value function \\(Q(s, a)\\) or a state-value function \\(V(s)\\).</p> </li> <li> <p>Actor: Updates the policy parameters \\(\\theta\\) for \\(\\pi_{\\theta}(a | s)\\), in the direction suggested by the critic.</p> </li> </ul> <p>Let's see how it works in a simple action-value Actor-Critic algorithm:</p> <ol> <li> <p>Initialize policy parameters \\(\\theta\\) and value function parameters \\(w\\) at random.</p> </li> <li> <p>Sample initial state \\(s_0\\).</p> </li> <li> <p>For each time step \\(t\\):</p> <ol> <li> <p>Sample reward \\(r_t\\) and next state \\(s_{t+1}\\).</p> </li> <li> <p>Then sample the next action \\(a_{t+1}\\) from policy: \\(\\pi_{\\theta}(s_{t+1})\\)</p> </li> <li> <p>Update the policy parameters: \\(\\theta  \\leftarrow  \\theta + \\alpha  \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_t | s_t) Q(s_t, a_t)\\)</p> </li> </ol> </li> <li> <p>Compute the correction (TD error) for action-value at time \\(t\\): \\(\\delta_t = r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q_w(s_t, a_t)\\)</p> </li> <li> <p>Compute MSE loss : \\(\\mathcal{L}(w) = \\frac{1}{2} \\mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \\sim \\pi_\\theta} \\left[ \\delta_t^2 \\right]\\)</p> </li> <li> <p>Use it to update the parameters of the action-value function : \\(w \\leftarrow w + \\beta  \\delta_t  \\nabla_w Q_w(s_t, a_t)\\)</p> </li> <li> <p>Update \\(\\theta\\) and \\(w\\).</p> </li> </ol> <p>Two learning rates, \\(\\alpha\\) and \\(\\beta\\), are predefined for policy and value function parameter updates, respectively. Also note that the \\(Q(s_{t+1}, a_{t+1})\\) in the TD error uses the freezed values of \\(w\\) for better stablity.</p> Actor-Critic Architecture: Cartpole Example <p>Let's illustrate the Actor-Critic architecture with an example of a classic reinforcement learning problem: the Cartpole environment.</p> <p> </p> <p>In the Cartpole environment, the agent controls a cart that can move horizontally on a track. A pole is attached to the cart, and the agent's task is to balance the pole upright for as long as possible.</p> <ol> <li> <p>Actor (Policy-Based): The actor is responsible for learning the policy, which is the agent's strategy for selecting actions (left or right) based on the observed state (cart position, cart velocity, pole angle, and pole angular velocity).</p> </li> <li> <p>Critic (Value-Based): The critic is responsible for learning the value function, which estimates the expected total reward (return) from each state. The value function helps evaluate how good or bad a specific state is, which guides the actor's updates.</p> </li> <li> <p>Policy Representation: For simplicity, let's use a neural network as the actor. The neural network takes the current state of the cart and pole as input and outputs the probabilities of selecting actions (left or right).</p> </li> <li> <p>Value Function Representation: For the critic, we also use a neural network. The neural network takes the current state as input and outputs an estimate of the expected total reward (value) for that state.</p> </li> <li> <p>Collecting Experiences: The agent interacts with the environment, using the current policy to select actions (left or right). As it moves through the environment, it collects experiences, including states, actions, rewards, and next states.</p> </li> <li> <p>Updating the Critic (Value Function): The critic learns to estimate the value function using the collected experiences. It optimizes its neural network parameters to minimize the difference between the predicted values and the actual rewards experienced by the agent.</p> </li> <li> <p>Calculating the Advantage: The advantage represents how much better or worse an action is compared to the average expected value. It is calculated as the difference between the total return (reward) and the value function estimate for each state-action pair.</p> </li> <li> <p>Updating the Actor (Policy): The actor updates its policy to increase the probabilities of actions with higher advantages and decrease the probabilities of actions with lower advantages. This process helps the actor learn from the critic's feedback and improve its policy to maximize the expected rewards.</p> </li> <li> <p>Iteration and Learning: The learning process is repeated over multiple episodes and iterations. As the agent explores and interacts with the environment, the actor and critic networks gradually improve their performance and converge to better policies and value function estimates.</p> </li> </ol> <p>Through these steps, the Actor-Critic architecture teaches the agent how to balance the pole effectively in the Cartpole environment. The actor learns the best actions to take in different states, while the critic provides feedback on the quality of the actor's decisions. As a result, the agent converges to a more optimal policy, achieving longer balancing times and better performance in the task.</p>"},{"location":"course_notes/policy-based/#summary-of-variance-reduction-methods","title":"Summary of Variance Reduction Methods","text":"<p>To summarize, the key methods for reducing variance in policy gradient methods include:</p> <ul> <li> <p>Baseline Subtraction: Subtracting an average return baseline to reduce variance while keeping gradients unbiased.</p> </li> <li> <p>Causality Trick and Reward-to-Go: Using future rewards from time step \\(t\\) onward to prevent variance from irrelevant past rewards.</p> </li> <li> <p>Discount Factor Adjustment: Adjusting \\(\\gamma\\) to balance variance reduction and long-term reward optimization.</p> </li> <li> <p>Advantage Estimation: Using the advantage function \\(A(s_t, a_t)\\) instead of raw returns to stabilize learning.</p> </li> <li> <p>Actor-Critic Methods: Combining policy gradient updates with value function estimation to create more stable and efficient training.</p> </li> </ul> <p>By employing these techniques, policy gradient methods can achieve more stable and efficient learning with reduced variance.</p>"},{"location":"course_notes/policy-based/#concluding-remarks","title":"Concluding Remarks","text":"<p>Now that we have seen the principles behind a policy-based algorithm, let us see how policy-based algorithms work in practice, and compare advantages and disadvantages of the policy-based approach.</p> <p>Let us start with the advantages. First of all, parameterization is at the core of policy-based methods, making them a good match for deep learning. For value- based methods, deep learning had to be retrofitted, giving rise to complications. Second, policy-based methods can easily find stochastic policies, whereas value- based methods find deterministic policies. Due to their stochastic nature, policy- based methods naturally explore, without the need for methods such as \\(\\epsilon\\)-greedy, or more involved methods that may require tuning to work well. Third, policy-based methods are effective in large or continuous action spaces. Small changes in \\(\\theta\\) lead to small changes in \\(\\pi\\), and to small changes in state distributions (they are smooth). Policy-based algorithms do not suffer (as much) from convergence and stability issues that are seen in \\(\\arg\\max\\)-based algorithms in large or continuous action spaces.</p> <p>On the other hand, there are disadvantages to the episodic Monte Carlo version of the REINFORCE algorithm. Remember that REINFORCE generates a full random episode in each iteration before it assesses the quality. (Value-based methods use a reward to select the next action in each time step of the episode.) Because of this, policy-based methods exhibit low bias since full random trajectories are generated. However, they are also high variance, since the full trajectory is generated randomly, whereas value-based methods use the value for guidance at each selection step.</p> <p>What are the consequences? First, policy evaluation of full trajectories has low sample efficiency and high variance. As a consequence, policy improvement happens infrequently, leading to slow convergence compared to value-based methods. Second, this approach often finds a local optimum, since convergence to the global optimum takes too long.</p> <p>Much research has been performed to address the high variance of the episode- based vanilla policy gradient. The enhancements that have been found have greatly improved performance, so much so that policy-based approaches---such as A3C, PPO, SAC, and DDPG---have become favorite model-free reinforcement learning algorithms for many applications.</p>"},{"location":"course_notes/policy-based/#authors","title":"Author(s)","text":"<ul> <li> <p>Nima Shirzady</p> <p>Teaching Assistant</p> <p>shirzady.1934@gmail.com</p> <p> </p> </li> <li> <p>SeyyedAli MirGhasemi</p> <p>Teaching Assistant</p> <p>sam717269@gmail.com</p> <p> </p> </li> <li> <p>Hesam Hosseini</p> <p>Teaching Assistant</p> <p>hesam138122@gmail.com</p> <p> </p> </li> </ul>"},{"location":"course_notes/policy-based/#references","title":"References","text":"<ol> <li> <p>Reinforcement Learning Explained</p> </li> <li> <p>An Introduction to Deep Reinforcement Learning</p> </li> <li> <p>Deep Reinforcement Learning Processor Design for Mobile Applications</p> </li> <li> <p>REINFORCE \u2014 a policy-gradient based reinforcement Learning algorithm</p> </li> <li> <p>Policy Gradient Algorithms</p> </li> <li> <p>Deep Reinforcement Learning</p> </li> <li> <p>Reinforcement Learning (BartoSutton)</p> </li> </ol>"},{"location":"course_notes/policy-based2/","title":"Week 8: Policy-Based Methods","text":"<p>Reinforcement Learning (RL) focuses on training an agent to interact with an environment by learning a policy \\(\\pi_{\\theta}(a | s)\\) that maximizes the cumulative reward. Policy gradient methods are a class of algorithms that directly optimize the policy by adjusting the parameters \\(\\theta\\) via gradient ascent.</p>"},{"location":"course_notes/policy-based2/#why-policy-gradient-methods","title":"Why Policy Gradient Methods?","text":"<p>Unlike value-based methods (e.g., Q-learning), which rely on estimating value functions, policy gradient methods: - Can naturally handle stochastic policies, which are crucial in environments requiring exploration.</p> <ul> <li> <p>Work well in continuous action spaces, where discrete action methods become infeasible.</p> </li> <li> <p>Can directly optimize differentiable policy representations, such as neural networks.</p> </li> <li> <p>Avoid the need for an explicit action-value function approximation, making them more robust in high-dimensional problems.</p> </li> <li> <p>Are capable of optimizing parameterized policies without relying on action selection heuristics.</p> </li> <li> <p>Can incorporate entropy regularization to improve exploration and prevent premature convergence to suboptimal policies.</p> </li> <li> <p>Allow for more stable convergence in some cases compared to value-based methods, which may suffer from instability due to bootstrapping.</p> </li> <li> <p>Can leverage variance reduction techniques (e.g., advantage estimation, baseline subtraction) to improve learning efficiency.</p> </li> </ul>"},{"location":"course_notes/policy-based2/#policy-gradient","title":"Policy Gradient","text":"<p>The goal of reinforcement learning is to find an optimal behavior strategy for the agent to obtain optimal rewards. The policy gradient methods target at modeling and optimizing the policy directly. The policy is usually modeled with a parameterized function respect to \\(\\theta\\), \\(\\pi_{\\theta}(a|s)\\). The value of the reward (objective) function depends on this policy and then various algorithms can be applied to optimize \\(\\theta\\) for the best reward.</p> <p>The reward function is defined as:</p> \\[J(\\theta) = \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) V^{\\pi}(s) = \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in  \\mathcal{A}} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a)\\] <p>where \\(d^{\\pi}(s)\\) is the stationary distribution of Markov chain for \\(\\pi_{\\theta}\\) (on-policy state distribution under \\(\\pi\\)). For simplicity, the parameter \\(\\theta\\) would be omitted for the policy \\(\\pi_{\\theta}\\) when the policy is present in the subscript of other functions; for example, \\(d^{\\pi}\\) and \\(Q^{\\pi}\\) should be \\(d^{\\pi_{\\theta}}\\) and \\(Q^{\\pi_{\\theta}}\\) if written in full.</p> <p>Imagine that you can travel along the Markov chain's states forever, and eventually, as the time progresses, the probability of you ending up with one state becomes unchanged --- this is the stationary probability for \\(\\pi_{\\theta}\\). \\(d^{\\pi}(s) = \\lim_{t \\to  \\infty} P(s_t = s | s_0, \\pi_{\\theta})\\) is the probability that \\(s_t = s\\) when starting from \\(s_0\\) and following policy \\(\\pi_{\\theta}\\) for \\(t\\) steps. Actually, the existence of the stationary distribution of Markov chain is one main reason for why PageRank algorithm works.</p> <p>It is natural to expect policy-based methods are more useful in the continuous space. Because there is an infinite number of actions and (or) states to estimate the values for and hence value-based approaches are way too expensive computationally in the continuous space. For example, in generalized policy iteration, the policy improvement step \\(\\arg  \\max_{a \\in  \\mathcal{A}} Q^{\\pi}(s,a)\\) requires a full scan of the action space, suffering from the curse of dimensionality.</p> <p>Using gradient ascent, we can move \\(\\theta\\) toward the direction suggested by the gradient \\(\\nabla_{\\theta} J(\\theta)\\) to find the best \\(\\theta\\) for \\(\\pi_{\\theta}\\) that produces the highest return.</p>"},{"location":"course_notes/policy-based2/#policy-gradient-theorem","title":"Policy Gradient Theorem","text":"<p>Computing the gradient \\(\\nabla_{\\theta}J(\\theta)\\) is tricky because it depends on both the action selection (directly determined by \\(\\pi_{\\theta}\\)) and the stationary distribution of states following the target selection behavior (indirectly determined by \\(\\pi_{\\theta}\\)). Given that the environment is generally unknown, it is difficult to estimate the effect on the state distribution by a policy update.</p> <p>Luckily, the policy gradient theorem comes to save the world!  It provides a nice reformation of the derivative of the objective function to not involve the derivative of the state distribution \\(d^{\\pi}(\\cdot)\\) and simplify the gradient computation \\(\\nabla_{\\theta}J(\\theta)\\) a lot.</p> \\[\\nabla_{\\theta}J(\\theta) = \\nabla_{\\theta} \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in  \\mathcal{A}} Q^{\\pi}(s,a) \\pi_{\\theta}(a|s)\\] \\[\\propto  \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in  \\mathcal{A}} Q^{\\pi}(s,a) \\nabla_{\\theta} \\pi_{\\theta}(a|s)\\]"},{"location":"course_notes/policy-based2/#proof-of-policy-gradient-theorem","title":"Proof of Policy Gradient Theorem","text":"<p>This session is pretty dense, as it is the time for us to go through the proof and figure out why the policy gradient theorem is correct.</p> Warning <p>This proof may be unnecessary for the first phase of the course. </p> proof <p>We first start with the derivative of the state value function:</p> \\[ \\begin{aligned} \\nabla_{\\theta} V^{\\pi}(s) &amp;= \\nabla_{\\theta} \\left( \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\right) \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\nabla_{\\theta} Q^{\\pi}(s,a) \\right) \\quad \\text{; Derivative product rule.} \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\nabla_{\\theta} \\sum_{s', r} P(s',r|s,a) (r + V^{\\pi}(s')) \\right) \\quad \\text{; Extend } Q^{\\pi} \\text{ with future state value.} \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\sum_{s',r} P(s',r|s,a) \\nabla_{\\theta} V^{\\pi}(s') \\right) \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\sum_{s'} P(s'|s,a) \\nabla_{\\theta} V^{\\pi}(s') \\right) \\quad \\text{; Because } P(s'|s,a) = \\sum_{r} P(s',r|s,a) \\end{aligned} \\] <p>Now we have:</p> \\[ \\begin{aligned} \\nabla_{\\theta} V^{\\pi}(s) &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\sum_{s'} P(s'|s,a) \\nabla_{\\theta} V^{\\pi}(s') \\right) \\end{aligned} \\] <p>This equation has a nice recursive form, and the future state value function \\(V^{\\pi}(s')\\) can be repeatedly unrolled by following the same equation.</p> <p>Let's consider the following visitation sequence and label the probability of transitioning from state \\(s\\) to state \\(x\\) with policy \\(\\pi_{\\theta}\\) after \\(k\\) steps as \\(\\rho^{\\pi}(s \\to x, k)\\).</p> \\[ s \\xrightarrow{a \\sim \\pi_{\\theta}(\\cdot | s)} s' \\xrightarrow{a' \\sim \\pi_{\\theta}(\\cdot | s')} s'' \\xrightarrow{a'' \\sim \\pi_{\\theta}(\\cdot | s'')} \\dots \\] <ul> <li> <p>When \\(k = 0\\): \\(\\rho^{\\pi}(s \\to s, k = 0) = 1\\).</p> </li> <li> <p>When \\(k = 1\\), we scan through all possible actions and sum up the transition probabilities to the target state:</p> </li> </ul> \\[ \\rho^{\\pi}(s \\to s', k = 1) = \\sum_{a} \\pi_{\\theta}(a|s) P(s'|s,a). \\] <ul> <li>Imagine that the goal is to go from state \\(s\\) to \\(x\\) after \\(k+1\\) steps while following policy \\(\\pi_{\\theta}\\). We can first travel from \\(s\\) to a middle point \\(s'\\) (any state can be a middle point, \\(s' \\in S\\)) after \\(k\\) steps and then go to the final state \\(x\\) during the last step. In this way, we are able to update the visitation probability recursively:</li> </ul> \\[ \\rho^{\\pi}(s \\to x, k + 1) = \\sum_{s'} \\rho^{\\pi}(s \\to s', k) \\rho^{\\pi}(s' \\to x, 1). \\] <p>Then we go back to unroll the recursive representation of \\(\\nabla_{\\theta}V^{\\pi}(s)\\)! Let</p> \\[ \\phi(s) = \\sum_{a \\in \\mathcal{A}} \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\] <p>to simplify the maths. If we keep on extending \\(\\nabla_{\\theta}V^{\\pi}(\\cdot)\\) infinitely, it is easy to find out that we can transition from the starting state \\(s\\) to any state after any number of steps in this unrolling process and by summing up all the visitation probabilities, we get \\(\\nabla_{\\theta}V^{\\pi}(s)\\)!</p> \\[ \\begin{aligned} \\nabla_{\\theta}V^{\\pi}(s) &amp;= \\phi(s) + \\sum_{a} \\pi_{\\theta}(a|s) \\sum_{s'} P(s'|s,a) \\nabla_{\\theta}V^{\\pi}(s') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\sum_{a} \\pi_{\\theta}(a|s) P(s'|s,a) \\nabla_{\\theta}V^{\\pi}(s') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\nabla_{\\theta}V^{\\pi}(s') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s') Q^{\\pi}(s',a) + \\pi_{\\theta}(a|s') \\sum_{s''} P(s''|s',a) \\nabla_{\\theta}V^{\\pi}(s'') \\right) \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\left[ \\phi(s') + \\sum_{s''} \\rho^{\\pi}(s' \\to s'', 1) \\nabla_{\\theta}V^{\\pi}(s'') \\right] \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\phi(s') + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\sum_{s''} \\rho^{\\pi}(s' \\to s'', 1) \\nabla_{\\theta}V^{\\pi}(s'') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\phi(s') + \\sum_{s''} \\rho^{\\pi}(s \\to s'', 2) \\nabla_{\\theta}V^{\\pi}(s'') \\quad \\text{; Consider } s' \\text{ as the middle point for } s \\to s''. \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\phi(s') + \\sum_{s''} \\rho^{\\pi}(s \\to s'', 2) \\phi(s'') + \\sum_{s'''} \\rho^{\\pi}(s \\to s''', 3) \\nabla_{\\theta}V^{\\pi}(s''') \\\\ &amp;= \\dots \\quad \\text{; Repeatedly unrolling the part of } \\nabla_{\\theta}V^{\\pi}(\\cdot) \\\\ &amp;= \\sum_{x \\in \\mathcal{S}} \\sum_{k=0}^{\\infty} \\rho^{\\pi}(s \\to x, k) \\phi(x) \\end{aligned} \\] <p>The nice rewriting above allows us to exclude the derivative of Q-value function, \\(\\nabla_{\\theta} Q^{\\pi}(s,a)\\). By plugging it into the objective function \\(J(\\theta)\\), we are getting the following:</p> \\[ \\begin{aligned} \\nabla_{\\theta}J(\\theta) &amp;= \\nabla_{\\theta}V^{\\pi}(s_0) \\\\ &amp;= \\sum_{s} \\sum_{k=0}^{\\infty} \\rho^{\\pi}(s_0 \\to s, k) \\phi(s) \\quad \\text{; Starting from a random state } s_0 \\\\ &amp;= \\sum_{s} \\eta(s) \\phi(s) \\quad \\text{; Let } \\eta(s) = \\sum_{k=0}^{\\infty} \\rho^{\\pi}(s_0 \\to s, k) \\\\ &amp;= \\left( \\sum_{s} \\eta(s) \\right) \\sum_{s} \\frac{\\eta(s)}{\\sum_{s} \\eta(s)} \\phi(s) \\quad \\text{; Normalize } \\eta(s), s \\in \\mathcal{S} \\text{ to be a probability distribution.} \\\\ &amp;\\propto \\sum_{s} \\frac{\\eta(s)}{\\sum_{s} \\eta(s)} \\phi(s) \\quad \\text{; } \\sum_{s} \\eta(s) \\text{ is a constant} \\\\ &amp;= \\sum_{s} d^{\\pi}(s) \\sum_{a} \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\quad d^{\\pi}(s) = \\frac{\\eta(s)}{\\sum_{s} \\eta(s)} \\text{ is stationary distribution.} \\end{aligned} \\] <p>In the episodic case, the constant of proportionality (\\(\\sum_{s} \\eta(s)\\)) is the average length of an episode; in the continuing case, it is 1. The gradient can be further written as:</p> \\[ \\begin{aligned} \\nabla_{\\theta}J(\\theta) &amp;\\propto \\sum_{s \\in \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in \\mathcal{A}} Q^{\\pi}(s,a) \\nabla_{\\theta} \\pi_{\\theta}(a|s) \\\\ &amp;= \\sum_{s \\in \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\frac{\\nabla_{\\theta} \\pi_{\\theta}(a|s)}{\\pi_{\\theta}(a|s)} \\quad \\text{; Because } \\ln(x)'=1/x \\\\ &amp;= \\mathbb{E}_{\\pi} [Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a|s)] \\end{aligned} \\] <p>Where \\(\\mathbb{E}_{\\pi}\\) refers to \\(\\mathbb{E}_{s \\sim d^{\\pi}, a \\sim \\pi_{\\theta}}\\) when both state and action distributions follow the policy \\(\\pi_{\\theta}\\) (on policy).</p> <p>The policy gradient theorem lays the theoretical foundation for various policy gradient algorithms. This vanilla policy gradient update has no bias but high variance. Many following algorithms were proposed to reduce the variance while keeping the bias unchanged.</p> \\[ \\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{\\pi} [Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a|s)] \\]"},{"location":"course_notes/policy-based2/#policy-gradient-in-continuous-action-space","title":"Policy Gradient in Continuous Action Space","text":"<p>In a continuous action space, the policy gradient theorem is given by:</p> \\[\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{s \\sim d^{\\pi}, a \\sim  \\pi_{\\theta}} \\left[ Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s) \\right]\\] <p>Since the action space is continuous, the summation over actions in the discrete case is replaced by an integral:</p> \\[\\nabla_{\\theta} J(\\theta) = \\int_{\\mathcal{S}} d^{\\pi}(s) \\int_{\\mathcal{A}} Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s) \\pi_{\\theta}(a|s) \\, da \\, ds\\] <p>where:</p> <ul> <li> <p>\\(d^{\\pi}(s)\\) is the stationary state distribution under policy \\(\\pi_{\\theta}\\),</p> </li> <li> <p>\\(\\pi_{\\theta}(a|s)\\) is the probability density function for the continuous action \\(a\\) given state \\(s\\),</p> </li> <li> <p>\\(Q^{\\pi}(s,a)\\) is the state-action value function,</p> </li> <li> <p>\\(\\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s)\\) is the score function (policy gradient term),</p> </li> <li> <p>The integral is taken over all possible states \\(s\\) and actions \\(a\\).</p> </li> </ul> Gaussian Policy Example <p>A common choice for a continuous policy is a Gaussian distribution:</p> \\[a \\sim  \\pi_{\\theta}(a|s) = \\mathcal{N}(\\mu_{\\theta}(s), \\Sigma_{\\theta}(s))\\] <p>where:</p> <ul> <li> <p>\\(\\mu_{\\theta}(s)\\) is the mean of the action distribution, parameterized by \\(\\theta\\),</p> </li> <li> <p>\\(\\Sigma_{\\theta}(s)\\) is the covariance matrix (often assumed diagonal or fixed).</p> </li> </ul> <p>For a Gaussian policy, the logarithm of the probability density is:</p> \\[\\ln  \\pi_{\\theta}(a|s) = -\\frac{1}{2} (a - \\mu_{\\theta}(s))^T \\Sigma_{\\theta}^{-1} (a - \\mu_{\\theta}(s)) - \\frac{1}{2} \\ln |\\Sigma_{\\theta}|\\] <p>Taking the gradient:</p> \\[\\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s) = \\Sigma_{\\theta}^{-1} (a - \\mu_{\\theta}(s)) \\nabla_{\\theta} \\mu_{\\theta}(s)\\] <p>Thus, the policy gradient update becomes:</p> \\[\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s \\sim d^{\\pi}, a \\sim  \\pi_{\\theta}} \\left[ Q^{\\pi}(s,a) \\Sigma_{\\theta}^{-1} (a - \\mu_{\\theta}(s)) \\nabla_{\\theta} \\mu_{\\theta}(s) \\right]\\]"},{"location":"course_notes/policy-based2/#reinforce","title":"REINFORCE","text":"<p>REINFORCE (Monte-Carlo policy gradient) relies on an estimated return by Monte-Carlo methods using episode samples to update the policy parameter \\(\\theta\\). REINFORCE works because the expectation of the sample gradient is equal to the actual gradient:</p> \\[ \\begin{aligned} \\nabla_{\\theta}J(\\theta) &amp;= \\mathbb{E}_{\\pi} \\left[ Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a|s) \\right] \\\\ &amp;= \\mathbb{E}_{\\pi} \\left[ G_t \\nabla_{\\theta} \\ln \\pi_{\\theta}(A_t|S_t) \\right] \\quad \\text{; Because } Q^{\\pi}(S_t, A_t) = \\mathbb{E}_{\\pi} \\left[ G_t \\mid S_t, A_t \\right] \\end{aligned} \\] <p>Therefore we are able to measure \\(G_t\\) from real sample trajectories and use that to update our policy gradient. It relies on a full trajectory and that's why it is a Monte-Carlo method.</p>"},{"location":"course_notes/policy-based2/#algorithm","title":"Algorithm","text":"<p>The process is pretty straightforward:</p> <ol> <li> <p>Initialize the policy parameter \\(\\theta\\) at random.</p> </li> <li> <p>Generate one trajectory on policy \\(\\pi_{\\theta}\\): \\(S_1, A_1, R_2, S_2, A_2, \\dots, S_T\\).</p> </li> <li> <p>For \\(t = 1, 2, \\dots, T\\):</p> <ol> <li> <p>Estimate the return \\(G_t\\);</p> </li> <li> <p>Update policy parameters: \\(\\theta  \\leftarrow  \\theta + \\alpha  \\gamma^t G_t \\nabla_{\\theta} \\ln  \\pi_{\\theta}(A_t|S_t)\\)</p> </li> </ol> </li> </ol> <p>A widely used variation of REINFORCE is to subtract a baseline value from the return \\(G_t\\) to reduce the variance of gradient estimation while keeping the bias unchanged (Remember we always want to do this when possible).</p> <p>For example, a common baseline is to subtract state-value from action-value, and if applied, we would use advantage \\(A(s,a) = Q(s,a) - V(s)\\) in the gradient ascent update. This post nicely explained why a baseline works for reducing the variance, in addition to a set of fundamentals of policy gradient.</p>"},{"location":"course_notes/policy-based2/#gs-in-continuous-action-space","title":"\\(G(s)\\) in Continuous Action Space","text":"<p>In the continuous setting, we define the return \\(G(s)\\) as:</p> \\[G(s) = \\sum_{k=0}^{\\infty} \\gamma^k R(s_k, a_k), \\quad s_0 = s, \\quad a_k \\sim  \\pi_{\\theta}(\\cdot | s_k)\\] <p>where:</p> <ul> <li> <p>\\(R(s_k, a_k)\\) is the reward function for state-action pair \\((s_k, a_k)\\).</p> </li> <li> <p>\\(\\gamma\\) is the discount factor.</p> </li> <li> <p>\\(s_k\\) evolves according to the environment dynamics.</p> </li> <li> <p>\\(a_k \\sim  \\pi_{\\theta}(\\cdot | s_k)\\) means actions are sampled from the policy.</p> </li> </ul>"},{"location":"course_notes/policy-based2/#monte-carlo-approximation-of-qpisa","title":"Monte Carlo Approximation of \\(Q^{\\pi}(s,a)\\)","text":"<p>In expectation, \\(G(s)\\) serves as an unbiased estimator of the state-action value function:</p> \\[Q^{\\pi}(s,a) = \\mathbb{E} \\left[ G(s) \\middle| s_0 = s, a_0 = a \\right]\\] <p>Using this, we rewrite the policy gradient update as:</p> \\[\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s \\sim d^{\\pi}, a \\sim  \\pi_{\\theta}} \\left[ G(s) \\nabla_{\\theta} \\ln  \\pi_{\\theta}(a | s) \\right]\\]"},{"location":"course_notes/policy-based2/#variance-reduction-advantage-function","title":"Variance Reduction: Advantage Function","text":"<p>A baseline is often subtracted to reduce variance while keeping the expectation unchanged:</p> \\[A(s, a) = G(s) - V^{\\pi}(s)\\] \\[\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s \\sim d^{\\pi}, a \\sim  \\pi_{\\theta}} \\left[ A(s, a) \\nabla_{\\theta} \\ln  \\pi_{\\theta}(a | s) \\right]\\] <p>where:</p> <ul> <li>\\(V^{\\pi}(s) = \\mathbb{E}_{a \\sim  \\pi_{\\theta}(\\cdot | s)} [Q^{\\pi}(s,a)]\\)</li> </ul> <p>is the state value function.</p> <ul> <li>\\(A(s,a)\\) measures the advantage of taking action \\(a\\) over the expected policy action.</li> </ul>"},{"location":"course_notes/policy-based2/#bias-and-variance","title":"Bias and Variance","text":"<p>In this section we delve deeper into the bias and variance problem in RL especially in policy gradient </p>"},{"location":"course_notes/policy-based2/#monte-carlo-estimators-in-reinforcement-learning","title":"Monte Carlo Estimators in Reinforcement Learning","text":"<p>A Monte Carlo estimator is a method used to approximate the expected value of a function \\(f(X)\\) over a random variable \\(X\\) with a given probability distribution \\(p(X)\\). The true expectation is:</p> \\[E[f(X)] = \\int f(x) p(x) \\, dx\\] <p>However, directly computing this integral may be complex. Instead, we use Monte Carlo estimation by drawing \\(N\\) independent samples \\(X_1, X_2, \\dots, X_N\\) from \\(p(X)\\) and computing:</p> \\[\\hat{\\mu}_{MC} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i)\\] <p>This estimator provides an approximation to the true expectation \\(E[f(X)]\\).</p> <p>By the law of large numbers (LLN), as \\(N \\to  \\infty\\), we have:</p> \\[\\hat{X}_N \\to  \\mathbb{E}[X] \\quad  \\text{(almost surely)}\\] <p>Monte Carlo methods are commonly used in RL for estimating expected rewards, state-value functions, and action-value functions.</p>"},{"location":"course_notes/policy-based2/#bias-in-policy-gradient-methods","title":"Bias in Policy Gradient Methods","text":"<p>Bias in reinforcement learning arises when an estimator systematically deviates from the true value. In policy gradient methods, bias is introduced due to function approximation, reward estimation, or gradient computation errors.</p>"},{"location":"course_notes/policy-based2/#sources-of-bias","title":"Sources of Bias","text":"<ul> <li> <p>Function Approximation Bias: Policy gradient methods often rely on neural networks or other function approximators for policy representation. Imperfect approximations introduce systematic errors, leading to biased policy updates.</p> </li> <li> <p>Reward Clipping or Discounting: Algorithms using reward clipping or high discount factors (\\(\\gamma\\)) can distort return estimates, causing the learned policy to be biased toward short-term rewards.</p> </li> <li> <p>Baseline Approximation: Variance reduction techniques like baseline subtraction use estimates of expected returns. If the baseline is inaccurately estimated, it introduces bias in the policy gradient computation.</p> </li> </ul> Example of Bias <p>Consider a self-driving car optimizing for fuel efficiency. If the reward function prioritizes immediate fuel consumption over long-term efficiency, the learned policy may favor suboptimal strategies that minimize fuel use in the short term while missing globally optimal driving behaviors.</p>"},{"location":"course_notes/policy-based2/#biased-vs-unbiased-estimation","title":"Biased vs. Unbiased Estimation","text":"<p>For example: The biased formula for the sample variance \\(S^2\\) is given by:</p> \\[S^2_{\\text{biased}} = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\overline{X})^2\\] <p>This is an underestimation of the true population variance \\(\\sigma^2\\) because it does not account for the degrees of freedom in estimation.</p> <p>Instead, the unbiased estimator is:</p> \\[S^2_{\\text{unbiased}} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\overline{X})^2.\\] <p>This unbiased estimator correctly accounts for variance in small sample sizes, ensuring \\(\\mathbb{E}[S^2_{\\text{unbiased}}] = \\sigma^2\\).</p>"},{"location":"course_notes/policy-based2/#variance-in-policy-gradient-methods","title":"Variance in Policy Gradient Methods","text":"<p>Variance in policy gradient estimates refers to fluctuations in gradient estimates across different training episodes. High variance leads to instability and slow convergence.</p>"},{"location":"course_notes/policy-based2/#sources-of-variance","title":"Sources of Variance","text":"<ul> <li> <p>Monte Carlo Estimation: REINFORCE estimates gradients using complete episodes, leading to high variance due to trajectory randomness.</p> </li> <li> <p>Stochastic Policy Outputs: Policies represented as probability distributions (e.g., Gaussian policies) introduce additional randomness in gradient updates.</p> </li> <li> <p>Exploration Strategies: Methods like softmax or epsilon-greedy increase variance by adding stochasticity to action selection.</p> </li> </ul> Example of Variance <p>Consider a robotic arm learning to grasp objects. Due to high variance, in some episodes, it succeeds, while in others, minor variations cause failure. These inconsistencies slow down convergence.</p>"},{"location":"course_notes/policy-based2/#techniques-to-reduce-variance-in-policy-gradient-methods","title":"Techniques to Reduce Variance in Policy Gradient Methods","text":"<p>Several strategies help mitigate variance in policy gradient methods while preserving unbiased gradient estimates.</p>"},{"location":"course_notes/policy-based2/#baseline-subtraction","title":"Baseline Subtraction","text":"<p>A baseline function \\(b\\) reduces variance without introducing bias:</p> \\[\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_t | s_t) (G_t - b) \\right].\\] <p>A common choice for \\(b\\) is the average return over trajectories:</p> \\[b = \\frac{1}{N} \\sum_{i=1}^{N} G_i.\\] <p>Since \\(b\\) is independent of actions, it does not introduce bias in the gradient estimate while reducing variance.</p> proof \\[\\begin{aligned} E\\left[\\nabla_\\theta  \\log p_\\theta(\\tau) b\\right] &amp;= \\int p_\\theta(\\tau) \\nabla_\\theta  \\log p_\\theta(\\tau) b \\, d\\tau \\\\ &amp;= \\int  \\nabla_\\theta p_\\theta(\\tau) b \\, d\\tau \\\\ &amp;= b \\nabla_\\theta  \\int p_\\theta(\\tau) \\, d\\tau \\\\ &amp;= b \\nabla_\\theta  1 \\\\ &amp;= 0 \\end{aligned}\\]"},{"location":"course_notes/policy-based2/#causality-trick-and-reward-to-go-estimation","title":"Causality Trick and Reward-to-Go Estimation","text":"<p>To ensure that policy updates at time \\(t\\) are only influenced by rewards from that time step onward, we use the causality trick:</p> \\[\\nabla_{\\theta} J(\\theta) \\approx  \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_{i,t} | s_{i,t}) \\left( \\sum_{t'=t}^{T} r(a_{i,t'}, s_{i,t'}) \\right).\\] <p>Instead of summing over all rewards, the reward-to-go estimate restricts the sum to future rewards only:</p> \\[Q(s_t, a_t) = \\sum_{t'=t}^{T} \\mathbb{E}_{\\pi_{\\theta}} [r(s_{t'}, a_{t'}) | s_t, a_t].\\] \\[\\nabla_{\\theta} J(\\theta) \\approx  \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_{i,t} | s_{i,t}) Q(s_{i,t}, a_{i,t}).\\] <p>This prevents rewards from future time steps from affecting past actions, reducing variance. This approach results in much lower variance compared to the traditional Monte Carlo methods.</p> proof \\[ \\begin{aligned} A_{t_0-1} &amp;= s_{t_0-1}, a_{t_0-1}, \\dots, a_0, s_0 \\\\ \\mathbb{E}_{A_{t_0-1}} &amp;\\left[ \\mathbb{E}_{s_{t_0}, a_{t_0} | A_{t_0-1}} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\sum_{t=0}^{t_0 - 1} r(s_t, a_t) \\right] \\right] \\\\ U_{t_0-1} &amp;= \\sum_{t=0}^{t_0 - 1} r(s_t, a_t) \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0}, a_{t_0} | s_{t_0-1}, a_{t_0-1}} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0} | s_{t_0-1}, a_{t_0-1}} \\mathbb{E}_{a_{t_0} | s_{t_0-1}, a_{t_0-1}, s_{t_0}} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0} | s_{t_0-1}, a_{t_0-1}} \\mathbb{E}_{a_{t_0} | s_{t_0}} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0} | s_{t_0-1}, a_{t_0-1}} \\mathbb{E}_{\\pi_{\\theta} (a_{t_0} | s_{t_0})} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ \\mathbb{E}_{\\pi_{\\theta} (a_{t_0} | s_{t_0})} &amp;\\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) = 0 \\\\ \\mathbb{E}_{A_{t_0-1}}&amp; \\left[ \\mathbb{E}_{s_{t_0}, a_{t_0} | A_{t_0-1}} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\sum_{t=0}^{t_0 - 1} r(s_t, a_t) \\right] \\right] = 0 \\end{aligned} \\]"},{"location":"course_notes/policy-based2/#discount-factor-adjustment","title":"Discount Factor Adjustment","text":"<p>The discount factor \\(\\gamma\\) helps reduce variance by weighting rewards closer to the present more heavily:</p> \\[G_t = \\sum_{t' = t}^{T} \\gamma^{t'-t} r(s_{t'}, a_{t'}).\\] proof \\[ \\begin{aligned} \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\left( \\sum_{t' = t}^{T} \\gamma^{t' - t} r(s_{i,t'}, a_{i,t'}) \\right) \\\\ \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\right) \\left( \\sum_{t=1}^{T} \\gamma^{t-1} r(s_{i,t}, a_{i,t}) \\right) \\\\ \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\left( \\sum_{t' = t}^{T} \\gamma^{t' - t} r(s_{i,t'}, a_{i,t'}) \\right) \\\\ \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\gamma^{t-1} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\left( \\sum_{t' = t}^{T} \\gamma^{t' - t} r(s_{i,t'}, a_{i,t'}) \\right) \\end{aligned} \\] <p>A lower \\(\\gamma\\) (e.g., 0.9) reduces variance but increases bias, while a higher \\(\\gamma\\) (e.g., 0.99) improves long-term estimation but increases variance. A balance is needed.</p>"},{"location":"course_notes/policy-based2/#advantage-estimation-and-actor-critic-methods","title":"Advantage Estimation and Actor-Critic Methods","text":"<p>Actor-critic methods combine policy optimization (actor) with value function estimation (critic). The advantage function is defined as:</p> \\[A^{\\pi}(s_t, a_t) = Q^{\\pi}(s_t, a_t) - V^{\\pi}(s_t),\\] <p>where the action-value function is:</p> \\[Q^{\\pi}(s_t, a_t) = \\sum_{t' = t}^{T} \\mathbb{E}_{\\pi} [r(s_{t'}, a_{t'}) | s_t, a_t],\\] <p>and the state-value function is:</p> \\[V^{\\pi}(s_t) = \\mathbb{E}_{a_t \\sim  \\pi_{\\theta}(a_t | s_t)} [Q^{\\pi}(s_t, a_t)].\\] <p>The policy gradient update using the advantage function becomes:</p> \\[\\nabla_{\\theta} J(\\theta) \\approx  \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_{i,t} | s_{i,t}) A^{\\pi}(s_{i,t}, a_{i,t}).\\] <p>This formulation allows for lower variance in policy updates while leveraging learned state-value estimates. Actor-critic methods are widely used in modern reinforcement learning due to their stability and efficiency.</p>"},{"location":"course_notes/policy-based2/#actor-critic","title":"Actor-Critic","text":"<p>Two main components in policy gradient methods are the policy model and the value function. It makes a lot of sense to learn the value function in addition to the policy since knowing the value function can assist the policy update, such as by reducing gradient variance in vanilla policy gradients. That is exactly what the Actor-Critic method does.</p> <p>Actor-Critic methods consist of two models, which may optionally share parameters:</p> <ul> <li> <p>Critic: Updates the value function parameters \\(w\\). Depending on the algorithm, it could be an action-value function \\(Q(s, a)\\) or a state-value function \\(V(s)\\).</p> </li> <li> <p>Actor: Updates the policy parameters \\(\\theta\\) for \\(\\pi_{\\theta}(a | s)\\), in the direction suggested by the critic.</p> </li> </ul> <p>Let's see how it works in a simple action-value Actor-Critic algorithm:</p> <ol> <li> <p>Initialize policy parameters \\(\\theta\\) and value function parameters \\(w\\) at random.</p> </li> <li> <p>Sample initial state \\(s_0\\).</p> </li> <li> <p>For each time step \\(t\\):</p> <ol> <li> <p>Sample reward \\(r_t\\) and next state \\(s_{t+1}\\).</p> </li> <li> <p>Then sample the next action \\(a_{t+1}\\) from policy: \\(\\pi_{\\theta}(s_{t+1})\\)</p> </li> <li> <p>Update the policy parameters: \\(\\theta  \\leftarrow  \\theta + \\alpha  \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_t | s_t) Q(s_t, a_t)\\)</p> </li> </ol> </li> <li> <p>Compute the correction (TD error) for action-value at time \\(t\\):</p> </li> </ol> \\[\\delta_t = r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)\\] <ol> <li>Use it to update the parameters of the action-value function:</li> </ol> \\[w \\leftarrow w + \\beta  \\delta_t  \\nabla_w Q(s_t, a_t)\\] <ol> <li>Update \\(\\theta\\) and \\(w\\).</li> </ol> <p>Two learning rates, \\(\\alpha\\) and \\(\\beta\\), are predefined for policy and value function parameter updates, respectively.</p> Actor-Critic Architecture: Cartpole Example <p>Let's illustrate the Actor-Critic architecture with an example of a classic reinforcement learning problem: the Cartpole environment.</p> <p> </p> <p>In the Cartpole environment, the agent controls a cart that can move horizontally on a track. A pole is attached to the cart, and the agent's task is to balance the pole upright for as long as possible.</p> <ol> <li> <p>Actor (Policy-Based): The actor is responsible for learning the policy, which is the agent's strategy for selecting actions (left or right) based on the observed state (cart position, cart velocity, pole angle, and pole angular velocity).</p> </li> <li> <p>Critic (Value-Based): The critic is responsible for learning the value function, which estimates the expected total reward (return) from each state. The value function helps evaluate how good or bad a specific state is, which guides the actor's updates.</p> </li> <li> <p>Policy Representation: For simplicity, let's use a neural network as the actor. The neural network takes the current state of the cart and pole as input and outputs the probabilities of selecting actions (left or right).</p> </li> <li> <p>Value Function Representation: For the critic, we also use a neural network. The neural network takes the current state as input and outputs an estimate of the expected total reward (value) for that state.</p> </li> <li> <p>Collecting Experiences: The agent interacts with the environment, using the current policy to select actions (left or right). As it moves through the environment, it collects experiences, including states, actions, rewards, and next states.</p> </li> <li> <p>Updating the Critic (Value Function): The critic learns to estimate the value function using the collected experiences. It optimizes its neural network parameters to minimize the difference between the predicted values and the actual rewards experienced by the agent.</p> </li> <li> <p>Calculating the Advantage: The advantage represents how much better or worse an action is compared to the average expected value. It is calculated as the difference between the total return (reward) and the value function estimate for each state-action pair.</p> </li> <li> <p>Updating the Actor (Policy): The actor updates its policy to increase the probabilities of actions with higher advantages and decrease the probabilities of actions with lower advantages. This process helps the actor learn from the critic's feedback and improve its policy to maximize the expected rewards.</p> </li> <li> <p>Iteration and Learning: The learning process is repeated over multiple episodes and iterations. As the agent explores and interacts with the environment, the actor and critic networks gradually improve their performance and converge to better policies and value function estimates.</p> </li> </ol> <p>Through these steps, the Actor-Critic architecture teaches the agent how to balance the pole effectively in the Cartpole environment. The actor learns the best actions to take in different states, while the critic provides feedback on the quality of the actor's decisions. As a result, the agent converges to a more optimal policy, achieving longer balancing times and better performance in the task.</p>"},{"location":"course_notes/policy-based2/#summary-of-variance-reduction-methods","title":"Summary of Variance Reduction Methods","text":"<p>To summarize, the key methods for reducing variance in policy gradient</p> <p>methods include:</p> <ul> <li> <p>Baseline Subtraction: Subtracting an average return baseline to reduce variance while keeping gradients unbiased.</p> </li> <li> <p>Causality Trick and Reward-to-Go: Using future rewards from time step \\(t\\) onward to prevent variance from irrelevant past rewards.</p> </li> <li> <p>Discount Factor Adjustment: Adjusting \\(\\gamma\\) to balance variance reduction and long-term reward optimization.</p> </li> <li> <p>Advantage Estimation: Using the advantage function \\(A(s_t, a_t)\\) instead of raw returns to stabilize learning.</p> </li> <li> <p>Actor-Critic Methods: Combining policy gradient updates with value function estimation to create more stable and efficient training.</p> </li> </ul> <p>By employing these techniques, policy gradient methods can achieve more stable and efficient learning with reduced variance.</p>"},{"location":"course_notes/policy-based2/#concluding-remarks","title":"Concluding Remarks","text":"<p>Now that we have seen the principles behind a policy-based algorithm, let us see how policy-based algorithms work in practice, and compare advantages and disadvantages of the policy-based approach.</p> <p>Let us start with the advantages. First of all, parameterization is at the core of policy-based methods, making them a good match for deep learning. For value- based methods, deep learning had to be retrofitted, giving rise to complications. Second, policy-based methods can easily find stochastic policies, whereas value- based methods find deterministic policies. Due to their stochastic nature, policy- based methods naturally explore, without the need for methods such as \\(\\epsilon\\)-greedy, or more involved methods that may require tuning to work well. Third, policy-based methods are effective in large or continuous action spaces. Small changes in \\(\\theta\\) lead to small changes in \\(\\pi\\), and to small changes in state distributions (they are smooth). Policy-based algorithms do not suffer (as much) from convergence and stability issues that are seen in \\(\\arg\\max\\)-based algorithms in large or continuous action spaces.</p> <p>On the other hand, there are disadvantages to the episodic Monte Carlo version of the REINFORCE algorithm. Remember that REINFORCE generates a full random episode in each iteration before it assesses the quality. (Value-based methods use a reward to select the next action in each time step of the episode.) Because of this, policy-based methods exhibit low bias since full random trajectories are generated. However, they are also high variance, since the full trajectory is generated randomly, whereas value-based methods use the value for guidance at each selection step.</p> <p>What are the consequences? First, policy evaluation of full trajectories has low sample efficiency and high variance. As a consequence, policy improvement happens infrequently, leading to slow convergence compared to value-based methods. Second, this approach often finds a local optimum, since convergence to the global optimum takes too long.</p> <p>Much research has been performed to address the high variance of the episode- based vanilla policy gradient. The enhancements that have been found have greatly improved performance, so much so that policy-based approaches---such as A3C, PPO, SAC, and DDPG---have become favorite model-free reinforcement learning algorithms for many applications.</p>"},{"location":"course_notes/policy-based2/#authors","title":"Author(s)","text":"<ul> <li> <p>Nima Shirzady</p> <p>Teaching Assistant</p> <p>shirzady.1934@gmail.com</p> <p> </p> </li> <li> <p>Hamidreza Ebrahimpour</p> <p>Teaching Assistant</p> <p>ebrahimpour.7879@gmail.com</p> <p> </p> </li> <li> <p>Hesam Hosseini</p> <p>Teaching Assistant</p> <p>hesam138122@gmail.com</p> <p> </p> </li> </ul>"},{"location":"course_notes/policy-based2/#references","title":"References","text":"<ol> <li> <p>Reinforcement Learning Explained</p> </li> <li> <p>An Introduction to Deep Reinforcement Learning</p> </li> <li> <p>Deep Reinforcement Learning Processor Design for Mobile Applications</p> </li> <li> <p>REINFORCE \u2014 a policy-gradient based reinforcement Learning algorithm</p> </li> <li> <p>Policy Gradient Algorithms</p> </li> <li> <p>Deep Reinforcement Learning</p> </li> <li> <p>Reinforcement Learning (BartoSutton)</p> </li> </ol>"},{"location":"course_notes/value-based/","title":"Week 2: Value-based Methods","text":""},{"location":"course_notes/value-based/#1-bellman-equations-and-value-functions","title":"1. Bellman Equations and Value Functions","text":""},{"location":"course_notes/value-based/#11-state-value-function-vs","title":"1.1. State Value Function \\(V(s)\\)","text":""},{"location":"course_notes/value-based/#definition","title":"Definition:","text":"<p>The state value function \\(V^\\pi(s)\\) measures the expected return when an agent starts in state \\(s\\) and follows a policy \\(\\pi\\). It provides a scalar value for each state that reflects the desirability of that state under the given policy. Formally, it is defined as:</p> \\[ V^\\pi(s) = \\mathbb{E} \\left[ G_t \\mid s_t = s \\right] \\] <p>Where \\(G_t\\) represents the return (total reward) from time step \\(t\\) onwards:</p> \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\] <ul> <li>\\(R_t\\) is the reward received at time step \\(t\\).</li> <li>\\(\\gamma\\) is the discount factor (\\(0 \\leq \\gamma \\leq 1\\)), controlling how much future rewards are valued compared to immediate rewards.</li> </ul>"},{"location":"course_notes/value-based/#bellman-expectation-equation-for-vpis","title":"Bellman Expectation Equation for \\(V^\\pi(s)\\):","text":"<p>The Bellman Expectation Equation for the state value function expresses the value of a state \\(s\\) in terms of the expected immediate reward and the discounted value of the next state. It is written as:</p> \\[ V^\\pi(s) = \\mathbb{E} \\left[ R_{t+1} + \\gamma V^\\pi(s_{t+1}) \\mid s_t = s \\right] \\] <p>Using the transition probabilities of the environment, this can be expanded as:</p> \\[ V^\\pi(s) = \\sum_{s'} P(s'|s, \\pi(s)) \\left[ R(s, \\pi(s), s') + \\gamma V^\\pi(s') \\right] \\] <p>Where: - \\(P(s'|s, \\pi(s))\\) is the probability of transitioning from state \\(s\\) to state \\(s'\\) when following action \\(\\pi(s)\\). - \\(R(s, \\pi(s), s')\\) is the reward for transitioning from state \\(s\\) to \\(s'\\) under action \\(\\pi(s)\\).</p> <p>This equation allows for the iterative computation of state values in a model-based setting.</p>"},{"location":"course_notes/value-based/#12-action-value-function-qs-a","title":"1.2. Action Value Function \\(Q(s, a)\\)","text":""},{"location":"course_notes/value-based/#definition_1","title":"Definition:","text":"<p>The action value function \\(Q^\\pi(s, a)\\) represents the expected return when an agent starts in state \\(s\\), takes action \\(a\\), and then follows policy \\(\\pi\\):</p> \\[ Q^\\pi(s, a) = \\mathbb{E} \\left[ G_t \\mid s_t = s, a_t = a \\right] \\] <p>Where \\(G_t\\) is the return starting at time \\(t\\).</p>"},{"location":"course_notes/value-based/#bellman-expectation-equation-for-qpis-a","title":"Bellman Expectation Equation for \\(Q^\\pi(s, a)\\):","text":"<p>The Bellman Expectation Equation for the action value function is similar to the one for the state value function but includes both the action and the subsequent states and actions. It is given by:</p> \\[ Q^\\pi(s, a) = \\mathbb{E} \\left[ R_{t+1} + \\gamma Q^\\pi(s_{t+1}, a_{t+1}) \\mid s_t = s, a_t = a \\right] \\] <p>Expanding this into a sum over possible next states, we get:</p> \\[ Q^\\pi(s, a) = \\sum_{s'} P(s'|s, a) \\left[ R(s, a, s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s', a') \\right] \\] <p>Where: - \\(P(s'|s, a)\\) is the transition probability from state \\(s\\) to state \\(s'\\) under action \\(a\\). - \\(\\pi(a'|s')\\) is the probability of taking action \\(a'\\) in state \\(s'\\) under policy \\(\\pi\\).</p>"},{"location":"course_notes/value-based/#bellman-optimality-equation-for-qs-a","title":"Bellman Optimality Equation for \\(Q^*(s, a)\\):","text":"<p>The Bellman Optimality Equation for \\(Q^*(s, a)\\) expresses the optimal action value function. It is given by:</p> \\[ Q^*(s, a) = \\mathbb{E} \\left[ R_{t+1} + \\gamma \\max_{a'} Q^*(s_{t+1}, a') \\mid s_t = s, a_t = a \\right] \\] <p>This shows that the optimal action value at each state-action pair is the immediate reward plus the discounted maximum expected value from the next state, where the next action is chosen optimally.</p>"},{"location":"course_notes/value-based/#2-dynamic-programming","title":"2. Dynamic Programming","text":"<p>Dynamic Programming (DP) is a powerful technique used to solve reinforcement learning problems where the environment is fully known (i.e., the model is available). DP algorithms compute the optimal policy and value functions by iteratively updating estimates based on a model of the environment. </p>"},{"location":"course_notes/value-based/#21-value-iteration","title":"2.1. Value Iteration","text":""},{"location":"course_notes/value-based/#bellman-optimality-equation","title":"Bellman Optimality Equation:","text":"<p>The Bellman Optimality Equation for the value function is:</p> \\[ V_{k+1}(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V_k(s') \\right] \\] <p>Where: - \\(\\max_a\\) selects the action \\(a\\) that maximizes the expected return from state \\(s\\).</p>"},{"location":"course_notes/value-based/#value-iteration-algorithm","title":"Value Iteration Algorithm:","text":"<ol> <li>Initialize the value function \\(V_0(s)\\) arbitrarily.</li> <li> <p>Repeat until convergence:</p> <ul> <li>For each state \\(s\\), update the value function:</li> </ul> \\[ V_{k+1}(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V_k(s') \\right] \\] </li> <li> <p>Once the value function converges, the optimal policy \\(\\pi^*(s)\\) can be derived by selecting the action that maximizes the expected return:</p> </li> </ol> \\[ \\pi^*(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^*(s') \\right] \\]"},{"location":"course_notes/value-based/#convergence","title":"Convergence:","text":"<p>Value Iteration is guaranteed to converge to the optimal value function and policy. The number of iterations required depends on the problem's dynamics, but it typically converges faster than Policy Iteration in terms of the number of iterations, though it may require more computation per iteration.</p>"},{"location":"course_notes/value-based/#22-policy-evaluation","title":"2.2. Policy Evaluation","text":"<p>Policy Evaluation calculates the state value function \\(V^\\pi(s)\\) for a given policy \\(\\pi\\) by iteratively updating the value function using the Bellman Expectation Equation:</p> \\[ V^\\pi(s) = R(s, \\pi(s)) + \\gamma \\sum_{s'} P(s'|s, \\pi(s)) V^\\pi(s') \\] <p>This process is repeated until \\(V^\\pi(s)\\) converges to a fixed point for all s.</p>"},{"location":"course_notes/value-based/#23-policy-improvement","title":"2.3. Policy Improvement","text":"<p>Policy Improvement refines a policy \\(\\pi\\) by making it greedy with respect to the current value function:</p> \\[ \\pi'(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^\\pi(s') \\right] \\] <p>It is proven that the new policy\u2019s value function is at least as good as the previous one:</p> \\[ V^{\\pi'}(s) \\geq V^\\pi(s), \\quad \\forall s. \\] <p>By repeating policy evaluation and improvement, the policy converges to the optimal one.</p> <ol> <li> <p>Single-Step Improvement: </p> <ul> <li>Modify the policy at only \\(t = 0\\), keeping the rest unchanged.  </li> <li>This new policy achieves a higher or equal value:  </li> </ul> \\[ V^{\\pi_{(k+1)}^{(1)}}(s) \\geq V^{\\pi_k}(s), \\quad \\forall s. \\] </li> <li> <p>Extending to Multiple Steps: </p> <ul> <li>Modify the policy at \\(t = 0\\) and \\(t = 1\\), keeping the rest unchanged.  </li> <li>Again, the value function improves: </li> </ul> \\[ V^{\\pi_{(k+1)}^{(2)}}(s) \\geq V^{\\pi_{(k+1)}^{(1)}}(s) \\geq V^{\\pi_k}(s). \\] </li> <li> <p>Repeating for All Steps: </p> <ul> <li>After applying this to all time steps, the final policy matches the fully improved one:  </li> </ul> \\[ \\pi_{(k+1)}^{(\\infty)}(s) = \\pi_{k+1}(s). \\] <ul> <li>This ensures:  </li> </ul> \\[ V^{\\pi_{k+1}}(s) \\geq V^{\\pi_k}(s), \\quad \\forall s. \\] </li> </ol> <p>The value function never decreases with each update.  </p>"},{"location":"course_notes/value-based/#24-policy-iteration","title":"2.4. Policy Iteration","text":"<p>Policy Iteration alternates between policy evaluation and policy improvement to compute the optimal policy.</p> <ol> <li>Initialize policy \\(\\pi_0\\) randomly.</li> <li>Policy Evaluation: Compute the value function \\(V^{\\pi_k}(s)\\) for the current policy \\(\\pi_k\\) using the Bellman Expectation Equation.</li> <li>Policy Improvement: Update the policy \\(\\pi_{k+1}(s)\\) by making it greedy with respect to the current value function:</li> </ol> \\[  \\pi_{k+1}(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^{\\pi_k}(s') \\right]  \\] <ol> <li>Repeat the above steps until the policy converges (i.e., \\(\\pi_k = \\pi_{k+1}\\)).</li> </ol>"},{"location":"course_notes/value-based/#convergence_1","title":"Convergence:","text":"<p>Each policy update ensures that the value function does not decrease.</p> <p>Since there are only a finite number of deterministic policies in a finite Markov Decision Process (MDP), the sequence of improving policies must eventually reach an policy \\(\\pi^*\\), where further improvement do not change it.</p> <p>The value function of the fixed point \\(\\pi^*\\) satisfy the Bellman Optimality Equation:</p> \\[ V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^*(s') \\right] \\] <p>This confirms that the final policy \\(\\pi^*\\) is optimal.</p>"},{"location":"course_notes/value-based/#25-comparison-of-policy-iteration-and-value-iteration","title":"2.5. Comparison of Policy Iteration and Value Iteration","text":"<p>Policy Iteration and Value Iteration are two dynamic programming methods for finding the optimal policy in an MDP. Both rely on iterative updates but differ in efficiency and computation.</p> Feature Policy Iteration Value Iteration Update Method Alternates between policy evaluation and improvement Updates value function directly Computational Cost Per Iteration \\(O(\\|S\\|^3)\\) (solving linear equations) \\(O(\\|S\\| \\cdot \\|A\\|)\\) (maximization over actions) Number of Iterations Fewer iterations, but each is expensive More iterations, but each is cheaper Best for Small state spaces, deterministic transitions Large state spaces, stochastic transitions <p>Policy Iteration explicitly computes the value function for a given policy, requiring solving a system of equations. Each iteration is computationally expensive but results in a significant improvement, leading to faster convergence in terms of iterations.</p> <p>Value Iteration avoids solving a system of equations by updating the value function incrementally. Each iteration is computationally cheaper, but because the value function is updated gradually, more iterations are needed for convergence.</p> <p>Thus, Policy Iteration takes fewer iterations but is computationally heavy per step, while Value Iteration takes more iterations but is computationally lighter per step.</p> <p>Watch on YouTube</p>"},{"location":"course_notes/value-based/#3-monte-carlo-methods","title":"3. Monte Carlo Methods","text":""},{"location":"course_notes/value-based/#31-planning-vs-learning-in-rl","title":"3.1. Planning vs. Learning in RL","text":"<p>Reinforcement learning can be approached in two ways: planning and learning. The main difference is that planning relies on a model of the environment, while learning uses real-world interactions to improve decision-making.</p>"},{"location":"course_notes/value-based/#planning-model-based-rl","title":"Planning (Model-Based RL)","text":"<ul> <li>Uses a model to predict state transitions and rewards.</li> <li>The agent can simulate future actions without interacting with the environment.</li> <li>Examples: Dynamic Programming (DP), Monte Carlo Tree Search (MCTS).</li> </ul>"},{"location":"course_notes/value-based/#learning-model-free-rl","title":"Learning (Model-Free RL)","text":"<ul> <li>No access to a model; the agent learns by interacting with the environment.</li> <li>The agent updates value estimates based on observed rewards.</li> <li>Examples: Monte Carlo, Temporal Difference (TD), Q-Learning.</li> </ul> <p>Planning is efficient when a reliable model is available, but learning is necessary when the model is unknown or too complex to compute.</p> <p>Monte Carlo methods fall under Model-Free RL, where the agent improves through experience. The following sections introduce how Monte Carlo Sampling is used to estimate value functions without needing a model.</p>"},{"location":"course_notes/value-based/#32-introduction-to-monte-carlo","title":"3.2. Introduction to Monte Carlo","text":"<p>Monte Carlo methods use random sampling to estimate numerical results, especially when direct computation is infeasible or the underlying distribution is unknown. These methods are widely applied in physics, finance, optimization, and reinforcement learning.</p> <p>Monte Carlo estimates an expectation:</p> \\[ I = \\mathbb{E}[f(X)] = \\int f(x) p(x) dx \\] <p>using sample averaging:</p> \\[ \\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^{N} f(x_i), \\] <p>where $ x_i $ are independent samples drawn from $ p(x) $. The Law of Large Numbers (LLN) ensures that as \\(N \\to \\infty\\):</p> \\[ \\hat{I}_N \\to I. \\] <p>This guarantees convergence, but the speed of convergence depends on the variance of the samples.</p> <p>Monte Carlo estimates become more accurate as \\(N\\) increases, but independent samples are crucial for unbiased estimation.</p> <p>By the Central Limit Theorem (CLT), for large \\(N\\), the Monte Carlo estimate follows a normal distribution:</p> \\[ \\hat{I}_N \\approx \\mathcal{N} \\left(I, \\frac{\\sigma^2}{N} \\right). \\] <p>This shows that the variance decreases at a rate of \\(O(1/N)\\), meaning that as the number of independent samples increases, the estimate becomes more stable. However, this reduction is slow, requiring a large number of samples to achieve high precision.</p>"},{"location":"course_notes/value-based/#example-estimating-pi","title":"Example: Estimating \\(\\pi\\)","text":"<p>Monte Carlo methods can estimate \\(\\pi\\) by randomly sampling points and analyzing their distribution relative to a known geometric shape.</p>"},{"location":"course_notes/value-based/#steps","title":"Steps:","text":"<ol> <li>Generate \\(N\\) random points \\((x, y)\\) where \\(x, y \\sim U(-1,1)\\), meaning they are uniformly sampled in the square \\([-1,1] \\times [-1,1]\\).</li> <li>Define an indicator function \\(I(x, y)\\) that takes the value:</li> </ol> \\[ I(x, y) = \\begin{cases} 1, &amp; \\text{if } x^2 + y^2 \\leq 1 \\quad \\text{(inside the circle)} \\\\ 0, &amp; \\text{otherwise}. \\end{cases} \\] <p>Since each point is either inside or outside the circle, the variable \\(I(x, y)\\) follows a Bernoulli distribution with probability \\(p = \\frac{\\pi}{4}\\).</p> <ol> <li>Compute the proportion of points inside the circle. The expectation of \\(I(x, y)\\) gives:</li> </ol> \\[ \\mathbb{E}[I] = P(I = 1) = \\frac{\\pi}{4}. \\] <p>By the Law of Large Numbers (LLN), the sample mean of \\(I(x, y)\\) over \\(N\\) points converges to this expected value:</p> \\[ \\frac{\\text{Points inside the circle}}{\\text{Total points}} \\approx \\frac{\\pi}{4}. \\]"},{"location":"course_notes/value-based/#example-integration","title":"Example: Integration","text":"<p>Monte Carlo methods can also estimate definite integrals using random sampling. Given an integral:</p> \\[ I = \\int_a^b f(x) dx, \\] <p>we approximate it using Monte Carlo sampling:</p> \\[ \\hat{I}_N = \\frac{b-a}{N} \\sum_{i=1}^{N} f(x_i), \\] <p>where \\(x_i\\) are sampled uniformly from \\([a, b]\\). By the LLN, as \\(N \\to \\infty\\), the estimate \\(\\hat{I}_N\\) converges to the true integral.</p> <p>Source</p> <p>Watch on YouTube</p>"},{"location":"course_notes/value-based/#33-monte-carlo-prediction","title":"3.3. Monte Carlo Prediction","text":"<p>In reinforcement learning, an episode is a sequence of states, actions, and rewards that starts from an initial state and ends in a terminal state. Each episode represents a complete trajectory of the agent\u2019s interaction with the environment.</p> <p>The return for a time step \\(t\\) in an episode is the cumulative discounted reward:</p> \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\]"},{"location":"course_notes/value-based/#estimating-vpis","title":"Estimating \\(V^\\pi(s)\\)","text":"<p>Monte Carlo methods estimate the state value function \\(V^\\pi(s)\\) by averaging the returns observed after visiting state \\(s\\) in multiple episodes.</p> <p>The estimate of \\(V^\\pi(s)\\) is:</p> \\[ V^\\pi(s) = \\frac{1}{N(s)} \\sum_{i=1}^{N(s)} G_i \\] <p>where: - \\(N(s)\\) is the number of times state \\(s\\) has been visited. - \\(G_i\\) is the return observed from the \\(i\\)-th visit to state \\(s\\).</p> <p>Since Monte Carlo methods rely entirely on sampled episodes, they do not require knowledge of transition probabilities or rewards so they learn directly from experience.</p>"},{"location":"course_notes/value-based/#34-monte-carlo-control","title":"3.4. Monte Carlo Control","text":"<p>In Monte Carlo Control, the goal is to improve the policy \\(\\pi\\) by optimizing it based on the action-value function \\(Q^\\pi(s, a)\\). </p>"},{"location":"course_notes/value-based/#algorithm","title":"Algorithm:","text":"<ol> <li>Generate Episodes: Generate episodes by interacting with the environment under the current policy \\(\\pi\\).</li> <li>Compute Returns: For each state-action pair \\((s, a)\\) in the episode, compute the return \\(G_t\\) from that time step onward.</li> <li>Update Action-Value Function: For each state-action pair, update the action-value function as:</li> </ol> \\[  Q^\\pi(s, a) = \\frac{1}{N(s, a)} \\sum_{i=1}^{N(s, a)} G_i \\] <p>Where \\(N(s, a)\\) is the number of times the state-action pair \\((s, a)\\) has been visited. 4. Policy Improvement: After updating the action-value function, improve the policy by selecting the action that maximizes \\(Q^\\pi(s, a)\\) for each state \\(s\\):</p> \\[  \\pi'(s) = \\arg\\max_a Q^\\pi(s, a)  \\] <p>This method is used to optimize the policy iteratively, improving it by making the policy greedy with respect to the current action-value function.</p>"},{"location":"course_notes/value-based/#35-first-visit-vs-every-visit-monte-carlo","title":"3.5. First-Visit vs. Every-Visit Monte Carlo","text":"<p>There are two main variations of Monte Carlo methods for estimating value functions: First-Visit Monte Carlo and Every-Visit Monte Carlo.</p>"},{"location":"course_notes/value-based/#first-visit-monte-carlo","title":"First-Visit Monte Carlo:","text":"<p>In First-Visit Monte Carlo, the return for a state is only updated the first time it is visited in an episode. This approach helps avoid over-counting and ensures that the value estimate for each state is updated only once per episode.</p>"},{"location":"course_notes/value-based/#algorithm_1","title":"Algorithm:","text":"<ol> <li>Initialize \\(N(s) = 0\\) and \\(G(s) = 0\\) for all states.</li> <li>For each episode, visit each state \\(s\\) for the first time, and when it is first visited, add the return \\(G_t\\) to \\(G(s)\\) and increment \\(N(s)\\).</li> <li>After the episode, update the value estimate for each state as:  </li> </ol> \\[ V^\\pi(s) = \\frac{G(s)}{N(s)} \\]"},{"location":"course_notes/value-based/#every-visit-monte-carlo","title":"Every-Visit Monte Carlo:","text":"<p>In Every-Visit Monte Carlo, the return for each state is updated every time it is visited in an episode. This approach uses all occurrences of a state to update its value function, which can sometimes lead to more stable estimates.</p>"},{"location":"course_notes/value-based/#algorithm_2","title":"Algorithm:","text":"<ol> <li>Initialize \\(N(s) = 0\\) and \\(G(s) = 0\\) for all states.</li> <li>For each episode, visit each state \\(s\\), and every time it is visited, add the return \\(G_t\\) to \\(G(s)\\) and increment \\(N(s)\\).</li> <li>After the episode, update the value estimate for each state as:</li> </ol> \\[ V^\\pi(s) = \\frac{G(s)}{N(s)} \\]"},{"location":"course_notes/value-based/#comparison","title":"Comparison:","text":"<p>First-Visit Monte Carlo updates the value function only the first time a state is encountered in an episode, ensuring an unbiased estimate but using fewer samples, which can result in higher variance and slower learning. In contrast, Every-Visit Monte Carlo updates the value function on all occurrences of a state within an episode, reducing variance and improving sample efficiency by utilizing more data. Although it may introduce bias, it often converges faster, making it more practical in many applications.</p>"},{"location":"course_notes/value-based/#36-incremental-monte-carlo-policy","title":"3.6. Incremental Monte Carlo Policy","text":"<p>In addition to First-Visit and Every-Visit Monte Carlo, an alternative approach is the Incremental Monte Carlo Policy, which updates the value function incrementally after each visit instead of computing an average over all episodes. This method is more memory-efficient and allows real-time updates without storing past returns.</p> <p>Given the return \\(G_{i,t}\\) observed for state \\(s\\) at time \\(t\\) in episode \\(i\\), we update the value function as:</p> \\[ V^\\pi(s) = V^\\pi(s) \\frac{N(s) - 1}{N(s)} + \\frac{G_{i,t}}{N(s)} \\] <p>which can be rewritten as:</p> \\[ V^\\pi(s) = V^\\pi(s) + \\frac{1}{N(s)} (G_{i,t} - V^\\pi(s)) \\] <ul> <li>The update formula behaves like a running average, gradually incorporating new information.</li> </ul> <p>This approach ensures smooth updates, avoids storing all past returns, and is more computationally efficient, especially in long episodes or large state spaces.</p>"},{"location":"course_notes/value-based/#4-temporal-difference-td-learning","title":"4. Temporal Difference (TD) Learning","text":"<p>Temporal Difference (TD) Learning is a method for estimating value functions in reinforcement learning. </p>"},{"location":"course_notes/value-based/#41-td-prediction","title":"4.1. TD Prediction","text":"<p>TD Learning updates the value function using the Bellman equation. It differs from Monte Carlo methods in that it updates after each step rather than waiting for the entire episode to finish. The general TD update rule for state-value function \\(V^\\pi(s_t)\\) is:</p> \\[ V^\\pi(s_t) = V^\\pi(s_t) + \\alpha \\left[ R_{t+1} + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t) \\right] \\] <p>This approach is called bootstrapping since it estimates future rewards based on the current value function rather than waiting for the full return.</p> <p>Just like for state-value functions, we can extend TD Learning to action-value functions (Q-values). The TD update rule for \\(Q(s_t, a_t)\\) is:</p> \\[ Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right] \\] <p>This allows TD learning to be applied to control tasks, where the agent needs to improve its policy while learning. </p>"},{"location":"course_notes/value-based/#43-on-policy-vs-off-policy-td-learning","title":"4.3. On-Policy vs. Off-Policy TD Learning","text":"<p>TD methods can be used for both on-policy and off-policy learning:</p> <ul> <li>SARSA (On-Policy TD Control): Updates the action-value function based on the agent\u2019s actual policy.</li> <li>Q-Learning (Off-Policy TD Control): Updates based on the optimal action, regardless of the agent\u2019s current policy.</li> </ul>"},{"location":"course_notes/value-based/#sarsa-algorithm-on-policy","title":"SARSA Algorithm (On-Policy)","text":"<p>In SARSA, the agent chooses the next action \\(a_{t+1}\\) according to its current policy and updates the Q-value based on the immediate reward and the Q-value for the next state-action pair.</p> <pre><code>Initialize Q(s, a) arbitrarily, for all s \u2208 S, a \u2208 A(s), and Q(terminal-state, \u00b7) = 0\nRepeat (for each episode):\n    Initialize S\n    Choose A from S using policy derived from Q (e.g., \u025b-greedy)\n    Repeat (for each step of episode):\n         Take action A, observe R and next state S'\n         Choose A' from S' using policy derived from Q (e.g., \u025b-greedy)\n\n         Update Q-value:\n         Q(S, A) \u2190 Q(S, A) + \u03b1[R + \u03b3 * Q(S', A') - Q(S, A)]\n\n         Set S \u2190 S', A \u2190 A'\n\n    until S is terminal\n</code></pre>"},{"location":"course_notes/value-based/#q-learning-algorithm-off-policy","title":"Q-Learning Algorithm (Off-Policy)","text":"<p>Q-learning is an off-policy algorithm that learns the best action-value function, no matter what behavior policy the agent used to gather the data.</p> <pre><code>Initialize Q(s, a) arbitrarily, for all s \u2208 S, a \u2208 A(s), and Q(terminal-state, \u00b7) = 0\nRepeat (for each episode):\n    Initialize S\n\n    Repeat (for each step of episode):\n         Choose A from S using policy derived from Q (e.g., \u025b-greedy)\n\n         Take action A, observe R and next state S'\n\n         Update Q-value:\n         Q(S, A) \u2190 Q(S, A) + \u03b1[R + \u03b3 * max_a Q(S', A') - Q(S, A)]\n\n         Set S \u2190 S'\n\n    until S is terminal\n</code></pre>"},{"location":"course_notes/value-based/#44-exploitation-vs-exploration","title":"4.4. Exploitation vs Exploration","text":""},{"location":"course_notes/value-based/#balancing-exploration-and-exploitation","title":"Balancing Exploration and Exploitation","text":"<p>In reinforcement learning, an agent needs to balance exploration (trying new actions) and exploitation (using known actions that give good rewards). To do this, we use an \\(\\epsilon\\)-greedy policy:</p> \\[ \\pi(a_t | s_t) =  \\begin{cases}  \\arg\\max_a Q(s_t, a) &amp; \\text{with probability } 1 - \\epsilon \\\\ \\text{random action} &amp; \\text{with probability } \\epsilon \\end{cases} \\] <p>At the start of learning, \\(\\epsilon\\) is high to encourage exploration. As the agent learns more about the environment, \\(\\epsilon\\) decreases, allowing the agent to focus more on exploiting the best actions it has learned. This process is called epsilon decay.</p> <p>Common ways to decay \\(\\epsilon\\) include:</p> <ul> <li>Linear Decay: </li> </ul> \\[ \\epsilon_t = \\frac{1}{t} \\] <p>where \\(t\\) is the time step.</p> <ul> <li>Exponential Decay:</li> </ul> \\[ \\epsilon_t = \\epsilon_0 \\cdot \\text{decay_rate}^t \\] <p>Watch on YouTube</p>"},{"location":"course_notes/value-based/#5-summary-of-key-concepts-and-methods","title":"5. Summary of Key Concepts and Methods","text":"<p>In reinforcement learning, various methods are used to estimate value functions and find optimal policies. These methods can be broadly categorized into Model-Based and Model-Free learning, as well as On-Policy and Off-Policy learning. Below is a concise summary of these key concepts and a comparison of different approaches.</p>"},{"location":"course_notes/value-based/#51-model-based-vs-model-free-learning","title":"5.1. Model-Based vs. Model-Free Learning","text":""},{"location":"course_notes/value-based/#model-based-learning","title":"Model-Based Learning:","text":"<ul> <li>Definition: The agent uses a model of the environment to predict future states and rewards. The model allows the agent to simulate actions and outcomes.</li> <li>Example: Dynamic Programming (DP) relies on a complete model of the environment.</li> <li>Advantages: Efficient when the model is available and provides exact solutions when the environment is known.</li> </ul>"},{"location":"course_notes/value-based/#model-free-learning","title":"Model-Free Learning:","text":"<ul> <li>Definition: The agent learns directly from interactions with the environment by estimating value functions based on observed rewards, without needing a model.</li> <li>Examples: Monte Carlo (MC), Temporal Difference (TD).</li> <li>Advantages: More flexible, applicable when the model is unknown or too complex to compute.</li> </ul>"},{"location":"course_notes/value-based/#52-on-policy-vs-off-policy-learning","title":"5.2. On-Policy vs. Off-Policy Learning","text":""},{"location":"course_notes/value-based/#on-policy-learning","title":"On-Policy Learning:","text":"<ul> <li>Definition: The agent learns about and improves the policy it is currently following. The policy that generates the data is the same as the one being evaluated and improved.</li> <li>Example: SARSA updates based on actions taken under the current policy.</li> <li>Advantages: Simpler and guarantees that the agent learns from its own actions.</li> </ul>"},{"location":"course_notes/value-based/#off-policy-learning","title":"Off-Policy Learning:","text":"<ul> <li>Definition: The agent learns about an optimal policy while following a different behavior policy. The target policy is updated while the agent explores using a behavior policy.</li> <li>Example: Q-Learning updates based on the optimal action, independent of the behavior policy.</li> <li>Advantages: More flexible, allows for learning from past experiences and using different exploration strategies.</li> </ul>"},{"location":"course_notes/value-based/#53-comparison","title":"5.3. Comparison","text":"Feature TD (Temporal Difference) Monte Carlo (MC) Dynamic Programming (DP) Model Requirement No model required (model-free) No model required (model-free) Requires a full model of the environment Learning Method Updates based on current estimates (bootstrapping) Updates after complete episode (no bootstrapping) Updates based on exact model (transition probabilities) Update Frequency After each step After each episode After each step or full sweep over states Efficiency More sample efficient (incremental learning) Less efficient (requires full episodes) Very efficient, but needs a model Convergence Converges with sufficient exploration Converges with sufficient exploration Converges to optimal policy with a known model Suitability Works well in ongoing tasks Works well for episodic tasks Works well in fully known environments <p>The choice of method depends on the environment, the availability of a model, and the trade-off between exploration and exploitation.</p>"},{"location":"course_notes/value-based/#references","title":"References","text":"<ul> <li>Sutton, R.S., &amp; Barto, A.G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press.</li> <li>monte-carlo for integration</li> </ul>"},{"location":"course_notes/value-based2/","title":"Week 7: Value-based Methods","text":""},{"location":"course_notes/value-based2/#1-bellman-equations-and-value-functions","title":"1. Bellman Equations and Value Functions","text":""},{"location":"course_notes/value-based2/#11-state-value-function-vs","title":"1.1. State Value Function \\(V(s)\\)","text":""},{"location":"course_notes/value-based2/#definition","title":"Definition:","text":"<p>The state value function \\(V^\\pi(s)\\) measures the expected return when an agent starts in state \\(s\\) and follows a policy \\(\\pi\\). It provides a scalar value for each state that reflects the desirability of that state under the given policy. Formally, it is defined as:</p> \\[ V^\\pi(s) = \\mathbb{E} \\left[ G_t \\mid s_t = s \\right] \\] <p>Where \\(G_t\\) represents the return (total reward) from time step \\(t\\) onwards:</p> \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\] <ul> <li>\\(R_t\\) is the reward received at time step \\(t\\).</li> <li>\\(\\gamma\\) is the discount factor (\\(0 \\leq \\gamma \\leq 1\\)), controlling how much future rewards are valued compared to immediate rewards.</li> </ul>"},{"location":"course_notes/value-based2/#bellman-expectation-equation-for-vpis","title":"Bellman Expectation Equation for \\(V^\\pi(s)\\):","text":"<p>The Bellman Expectation Equation for the state value function expresses the value of a state \\(s\\) in terms of the expected immediate reward and the discounted value of the next state. It is written as:</p> \\[ V^\\pi(s) = \\mathbb{E} \\left[ R_{t+1} + \\gamma V^\\pi(s_{t+1}) \\mid s_t = s \\right] \\] <p>Using the transition probabilities of the environment, this can be expanded as:</p> \\[ V^\\pi(s) = \\sum_{s'} P(s'|s, \\pi(s)) \\left[ R(s, \\pi(s), s') + \\gamma V^\\pi(s') \\right] \\] <p>Where: - \\(P(s'|s, \\pi(s))\\) is the probability of transitioning from state \\(s\\) to state \\(s'\\) when following action \\(\\pi(s)\\). - \\(R(s, \\pi(s), s')\\) is the reward for transitioning from state \\(s\\) to \\(s'\\) under action \\(\\pi(s)\\).</p> <p>This equation allows for the iterative computation of state values in a model-based setting.</p>"},{"location":"course_notes/value-based2/#12-action-value-function-qs-a","title":"1.2. Action Value Function \\(Q(s, a)\\)","text":""},{"location":"course_notes/value-based2/#definition_1","title":"Definition:","text":"<p>The action value function \\(Q^\\pi(s, a)\\) represents the expected return when an agent starts in state \\(s\\), takes action \\(a\\), and then follows policy \\(\\pi\\):</p> \\[ Q^\\pi(s, a) = \\mathbb{E} \\left[ G_t \\mid s_t = s, a_t = a \\right] \\] <p>Where \\(G_t\\) is the return starting at time \\(t\\).</p>"},{"location":"course_notes/value-based2/#bellman-expectation-equation-for-qpis-a","title":"Bellman Expectation Equation for \\(Q^\\pi(s, a)\\):","text":"<p>The Bellman Expectation Equation for the action value function is similar to the one for the state value function but includes both the action and the subsequent states and actions. It is given by:</p> \\[ Q^\\pi(s, a) = \\mathbb{E} \\left[ R_{t+1} + \\gamma Q^\\pi(s_{t+1}, a_{t+1}) \\mid s_t = s, a_t = a \\right] \\] <p>Expanding this into a sum over possible next states, we get:</p> \\[ Q^\\pi(s, a) = \\sum_{s'} P(s'|s, a) \\left[ R(s, a, s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s', a') \\right] \\] <p>Where: - \\(P(s'|s, a)\\) is the transition probability from state \\(s\\) to state \\(s'\\) under action \\(a\\). - \\(\\pi(a'|s')\\) is the probability of taking action \\(a'\\) in state \\(s'\\) under policy \\(\\pi\\).</p>"},{"location":"course_notes/value-based2/#bellman-optimality-equation-for-qs-a","title":"Bellman Optimality Equation for \\(Q^*(s, a)\\):","text":"<p>The Bellman Optimality Equation for \\(Q^*(s, a)\\) expresses the optimal action value function. It is given by:</p> \\[ Q^*(s, a) = \\mathbb{E} \\left[ R_{t+1} + \\gamma \\max_{a'} Q^*(s_{t+1}, a') \\mid s_t = s, a_t = a \\right] \\] <p>This shows that the optimal action value at each state-action pair is the immediate reward plus the discounted maximum expected value from the next state, where the next action is chosen optimally.</p>"},{"location":"course_notes/value-based2/#2-dynamic-programming","title":"2. Dynamic Programming","text":"<p>Dynamic Programming (DP) is a powerful technique used to solve reinforcement learning problems where the environment is fully known (i.e., the model is available). DP algorithms compute the optimal policy and value functions by iteratively updating estimates based on a model of the environment. </p>"},{"location":"course_notes/value-based2/#21-value-iteration","title":"2.1. Value Iteration","text":""},{"location":"course_notes/value-based2/#bellman-optimality-equation","title":"Bellman Optimality Equation:","text":"<p>The Bellman Optimality Equation for the value function is:</p> \\[ V_{k+1}(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V_k(s') \\right] \\] <p>Where: - \\(\\max_a\\) selects the action \\(a\\) that maximizes the expected return from state \\(s\\).</p>"},{"location":"course_notes/value-based2/#value-iteration-algorithm","title":"Value Iteration Algorithm:","text":"<ol> <li>Initialize the value function \\(V_0(s)\\) arbitrarily.</li> <li> <p>Repeat until convergence:</p> <ul> <li>For each state \\(s\\), update the value function:</li> </ul> \\[ V_{k+1}(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V_k(s') \\right] \\] </li> <li> <p>Once the value function converges, the optimal policy \\(\\pi^*(s)\\) can be derived by selecting the action that maximizes the expected return:</p> </li> </ol> \\[ \\pi^*(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^*(s') \\right] \\]"},{"location":"course_notes/value-based2/#convergence","title":"Convergence:","text":"<p>Value Iteration is guaranteed to converge to the optimal value function and policy. The number of iterations required depends on the problem's dynamics, but it typically converges faster than Policy Iteration in terms of the number of iterations, though it may require more computation per iteration.</p>"},{"location":"course_notes/value-based2/#22-policy-evaluation","title":"2.2. Policy Evaluation","text":"<p>Policy Evaluation calculates the state value function \\(V^\\pi(s)\\) for a given policy \\(\\pi\\) by iteratively updating the value function using the Bellman Expectation Equation:</p> \\[ V^\\pi(s) = R(s, \\pi(s)) + \\gamma \\sum_{s'} P(s'|s, \\pi(s)) V^\\pi(s') \\] <p>This process is repeated until \\(V^\\pi(s)\\) converges to a fixed point for all s.</p>"},{"location":"course_notes/value-based2/#23-policy-improvement","title":"2.3. Policy Improvement","text":"<p>Policy Improvement refines a policy \\(\\pi\\) by making it greedy with respect to the current value function:</p> \\[ \\pi'(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^\\pi(s') \\right] \\] <p>It is proven that the new policy\u2019s value function is at least as good as the previous one:</p> \\[ V^{\\pi'}(s) \\geq V^\\pi(s), \\quad \\forall s. \\] <p>By repeating policy evaluation and improvement, the policy converges to the optimal one.</p> <ol> <li> <p>Single-Step Improvement: </p> <ul> <li>Modify the policy at only \\(t = 0\\), keeping the rest unchanged.  </li> <li>This new policy achieves a higher or equal value:  </li> </ul> \\[ V^{\\pi_{(k+1)}^{(1)}}(s) \\geq V^{\\pi_k}(s), \\quad \\forall s. \\] </li> <li> <p>Extending to Multiple Steps: </p> <ul> <li>Modify the policy at \\(t = 0\\) and \\(t = 1\\), keeping the rest unchanged.  </li> <li>Again, the value function improves: </li> </ul> \\[ V^{\\pi_{(k+1)}^{(2)}}(s) \\geq V^{\\pi_{(k+1)}^{(1)}}(s) \\geq V^{\\pi_k}(s). \\] </li> <li> <p>Repeating for All Steps: </p> <ul> <li>After applying this to all time steps, the final policy matches the fully improved one:  </li> </ul> \\[ \\pi_{(k+1)}^{(\\infty)}(s) = \\pi_{k+1}(s). \\] <ul> <li>This ensures:  </li> </ul> \\[ V^{\\pi_{k+1}}(s) \\geq V^{\\pi_k}(s), \\quad \\forall s. \\] </li> </ol> <p>The value function never decreases with each update.  </p>"},{"location":"course_notes/value-based2/#24-policy-iteration","title":"2.4. Policy Iteration","text":"<p>Policy Iteration alternates between policy evaluation and policy improvement to compute the optimal policy.</p> <ol> <li>Initialize policy \\(\\pi_0\\) randomly.</li> <li>Policy Evaluation: Compute the value function \\(V^{\\pi_k}(s)\\) for the current policy \\(\\pi_k\\) using the Bellman Expectation Equation.</li> <li>Policy Improvement: Update the policy \\(\\pi_{k+1}(s)\\) by making it greedy with respect to the current value function:</li> </ol> \\[  \\pi_{k+1}(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^{\\pi_k}(s') \\right]  \\] <ol> <li>Repeat the above steps until the policy converges (i.e., \\(\\pi_k = \\pi_{k+1}\\)).</li> </ol>"},{"location":"course_notes/value-based2/#convergence_1","title":"Convergence:","text":"<p>Each policy update ensures that the value function does not decrease.</p> <p>Since there are only a finite number of deterministic policies in a finite Markov Decision Process (MDP), the sequence of improving policies must eventually reach an policy \\(\\pi^*\\), where further improvement do not change it.</p> <p>The value function of the fixed point \\(\\pi^*\\) satisfy the Bellman Optimality Equation:</p> \\[ V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^*(s') \\right] \\] <p>This confirms that the final policy \\(\\pi^*\\) is optimal.</p>"},{"location":"course_notes/value-based2/#25-comparison-of-policy-iteration-and-value-iteration","title":"2.5. Comparison of Policy Iteration and Value Iteration","text":"<p>Policy Iteration and Value Iteration are two dynamic programming methods for finding the optimal policy in an MDP. Both rely on iterative updates but differ in efficiency and computation.</p> Feature Policy Iteration Value Iteration Update Method Alternates between policy evaluation and improvement Updates value function directly Computational Cost Per Iteration \\(O(\\|S\\|^3)\\) (solving linear equations) \\(O(\\|S\\| \\cdot \\|A\\|)\\) (maximization over actions) Number of Iterations Fewer iterations, but each is expensive More iterations, but each is cheaper Best for Small state spaces, deterministic transitions Large state spaces, stochastic transitions <p>Policy Iteration explicitly computes the value function for a given policy, requiring solving a system of equations. Each iteration is computationally expensive but results in a significant improvement, leading to faster convergence in terms of iterations.</p> <p>Value Iteration avoids solving a system of equations by updating the value function incrementally. Each iteration is computationally cheaper, but because the value function is updated gradually, more iterations are needed for convergence.</p> <p>Thus, Policy Iteration takes fewer iterations but is computationally heavy per step, while Value Iteration takes more iterations but is computationally lighter per step.</p> <p>Watch on YouTube</p>"},{"location":"course_notes/value-based2/#3-monte-carlo-methods","title":"3. Monte Carlo Methods","text":""},{"location":"course_notes/value-based2/#31-planning-vs-learning-in-rl","title":"3.1. Planning vs. Learning in RL","text":"<p>Reinforcement learning can be approached in two ways: planning and learning. The main difference is that planning relies on a model of the environment, while learning uses real-world interactions to improve decision-making.</p>"},{"location":"course_notes/value-based2/#planning-model-based-rl","title":"Planning (Model-Based RL)","text":"<ul> <li>Uses a model to predict state transitions and rewards.</li> <li>The agent can simulate future actions without interacting with the environment.</li> <li>Examples: Dynamic Programming (DP), Monte Carlo Tree Search (MCTS).</li> </ul>"},{"location":"course_notes/value-based2/#learning-model-free-rl","title":"Learning (Model-Free RL)","text":"<ul> <li>No access to a model; the agent learns by interacting with the environment.</li> <li>The agent updates value estimates based on observed rewards.</li> <li>Examples: Monte Carlo, Temporal Difference (TD), Q-Learning.</li> </ul> <p>Planning is efficient when a reliable model is available, but learning is necessary when the model is unknown or too complex to compute.</p> <p>Monte Carlo methods fall under Model-Free RL, where the agent improves through experience. The following sections introduce how Monte Carlo Sampling is used to estimate value functions without needing a model.</p>"},{"location":"course_notes/value-based2/#32-introduction-to-monte-carlo","title":"3.2. Introduction to Monte Carlo","text":"<p>Monte Carlo methods use random sampling to estimate numerical results, especially when direct computation is infeasible or the underlying distribution is unknown. These methods are widely applied in physics, finance, optimization, and reinforcement learning.</p> <p>Monte Carlo estimates an expectation:</p> \\[ I = \\mathbb{E}[f(X)] = \\int f(x) p(x) dx \\] <p>using sample averaging:</p> \\[ \\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^{N} f(x_i), \\] <p>where $ x_i $ are independent samples drawn from $ p(x) $. The Law of Large Numbers (LLN) ensures that as \\(N \\to \\infty\\):</p> \\[ \\hat{I}_N \\to I. \\] <p>This guarantees convergence, but the speed of convergence depends on the variance of the samples.</p> <p>Monte Carlo estimates become more accurate as \\(N\\) increases, but independent samples are crucial for unbiased estimation.</p> <p>By the Central Limit Theorem (CLT), for large \\(N\\), the Monte Carlo estimate follows a normal distribution:</p> \\[ \\hat{I}_N \\approx \\mathcal{N} \\left(I, \\frac{\\sigma^2}{N} \\right). \\] <p>This shows that the variance decreases at a rate of \\(O(1/N)\\), meaning that as the number of independent samples increases, the estimate becomes more stable. However, this reduction is slow, requiring a large number of samples to achieve high precision.</p>"},{"location":"course_notes/value-based2/#example-estimating-pi","title":"Example: Estimating \\(\\pi\\)","text":"<p>Monte Carlo methods can estimate \\(\\pi\\) by randomly sampling points and analyzing their distribution relative to a known geometric shape.</p>"},{"location":"course_notes/value-based2/#steps","title":"Steps:","text":"<ol> <li>Generate \\(N\\) random points \\((x, y)\\) where \\(x, y \\sim U(-1,1)\\), meaning they are uniformly sampled in the square \\([-1,1] \\times [-1,1]\\).</li> <li>Define an indicator function \\(I(x, y)\\) that takes the value:</li> </ol> \\[ I(x, y) = \\begin{cases} 1, &amp; \\text{if } x^2 + y^2 \\leq 1 \\quad \\text{(inside the circle)} \\\\ 0, &amp; \\text{otherwise}. \\end{cases} \\] <p>Since each point is either inside or outside the circle, the variable \\(I(x, y)\\) follows a Bernoulli distribution with probability \\(p = \\frac{\\pi}{4}\\).</p> <ol> <li>Compute the proportion of points inside the circle. The expectation of \\(I(x, y)\\) gives:</li> </ol> \\[ \\mathbb{E}[I] = P(I = 1) = \\frac{\\pi}{4}. \\] <p>By the Law of Large Numbers (LLN), the sample mean of \\(I(x, y)\\) over \\(N\\) points converges to this expected value:</p> \\[ \\frac{\\text{Points inside the circle}}{\\text{Total points}} \\approx \\frac{\\pi}{4}. \\]"},{"location":"course_notes/value-based2/#example-integration","title":"Example: Integration","text":"<p>Monte Carlo methods can also estimate definite integrals using random sampling. Given an integral:</p> \\[ I = \\int_a^b f(x) dx, \\] <p>we approximate it using Monte Carlo sampling:</p> \\[ \\hat{I}_N = \\frac{b-a}{N} \\sum_{i=1}^{N} f(x_i), \\] <p>where \\(x_i\\) are sampled uniformly from \\([a, b]\\). By the LLN, as \\(N \\to \\infty\\), the estimate \\(\\hat{I}_N\\) converges to the true integral.</p> <p>Source</p> <p>Watch on YouTube</p>"},{"location":"course_notes/value-based2/#33-monte-carlo-prediction","title":"3.3. Monte Carlo Prediction","text":"<p>In reinforcement learning, an episode is a sequence of states, actions, and rewards that starts from an initial state and ends in a terminal state. Each episode represents a complete trajectory of the agent\u2019s interaction with the environment.</p> <p>The return for a time step \\(t\\) in an episode is the cumulative discounted reward:</p> \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\]"},{"location":"course_notes/value-based2/#estimating-vpis","title":"Estimating \\(V^\\pi(s)\\)","text":"<p>Monte Carlo methods estimate the state value function \\(V^\\pi(s)\\) by averaging the returns observed after visiting state \\(s\\) in multiple episodes.</p> <p>The estimate of \\(V^\\pi(s)\\) is:</p> \\[ V^\\pi(s) = \\frac{1}{N(s)} \\sum_{i=1}^{N(s)} G_i \\] <p>where: - \\(N(s)\\) is the number of times state \\(s\\) has been visited. - \\(G_i\\) is the return observed from the \\(i\\)-th visit to state \\(s\\).</p> <p>Since Monte Carlo methods rely entirely on sampled episodes, they do not require knowledge of transition probabilities or rewards so they learn directly from experience.</p>"},{"location":"course_notes/value-based2/#34-monte-carlo-control","title":"3.4. Monte Carlo Control","text":"<p>In Monte Carlo Control, the goal is to improve the policy \\(\\pi\\) by optimizing it based on the action-value function \\(Q^\\pi(s, a)\\). </p>"},{"location":"course_notes/value-based2/#algorithm","title":"Algorithm:","text":"<ol> <li>Generate Episodes: Generate episodes by interacting with the environment under the current policy \\(\\pi\\).</li> <li>Compute Returns: For each state-action pair \\((s, a)\\) in the episode, compute the return \\(G_t\\) from that time step onward.</li> <li>Update Action-Value Function: For each state-action pair, update the action-value function as:</li> </ol> \\[  Q^\\pi(s, a) = \\frac{1}{N(s, a)} \\sum_{i=1}^{N(s, a)} G_i \\] <p>Where \\(N(s, a)\\) is the number of times the state-action pair \\((s, a)\\) has been visited. 4. Policy Improvement: After updating the action-value function, improve the policy by selecting the action that maximizes \\(Q^\\pi(s, a)\\) for each state \\(s\\):</p> \\[  \\pi'(s) = \\arg\\max_a Q^\\pi(s, a)  \\] <p>This method is used to optimize the policy iteratively, improving it by making the policy greedy with respect to the current action-value function.</p>"},{"location":"course_notes/value-based2/#35-first-visit-vs-every-visit-monte-carlo","title":"3.5. First-Visit vs. Every-Visit Monte Carlo","text":"<p>There are two main variations of Monte Carlo methods for estimating value functions: First-Visit Monte Carlo and Every-Visit Monte Carlo.</p>"},{"location":"course_notes/value-based2/#first-visit-monte-carlo","title":"First-Visit Monte Carlo:","text":"<p>In First-Visit Monte Carlo, the return for a state is only updated the first time it is visited in an episode. This approach helps avoid over-counting and ensures that the value estimate for each state is updated only once per episode.</p>"},{"location":"course_notes/value-based2/#algorithm_1","title":"Algorithm:","text":"<ol> <li>Initialize \\(N(s) = 0\\) and \\(G(s) = 0\\) for all states.</li> <li>For each episode, visit each state \\(s\\) for the first time, and when it is first visited, add the return \\(G_t\\) to \\(G(s)\\) and increment \\(N(s)\\).</li> <li>After the episode, update the value estimate for each state as:  </li> </ol> \\[ V^\\pi(s) = \\frac{G(s)}{N(s)} \\]"},{"location":"course_notes/value-based2/#every-visit-monte-carlo","title":"Every-Visit Monte Carlo:","text":"<p>In Every-Visit Monte Carlo, the return for each state is updated every time it is visited in an episode. This approach uses all occurrences of a state to update its value function, which can sometimes lead to more stable estimates.</p>"},{"location":"course_notes/value-based2/#algorithm_2","title":"Algorithm:","text":"<ol> <li>Initialize \\(N(s) = 0\\) and \\(G(s) = 0\\) for all states.</li> <li>For each episode, visit each state \\(s\\), and every time it is visited, add the return \\(G_t\\) to \\(G(s)\\) and increment \\(N(s)\\).</li> <li>After the episode, update the value estimate for each state as:</li> </ol> \\[ V^\\pi(s) = \\frac{G(s)}{N(s)} \\]"},{"location":"course_notes/value-based2/#comparison","title":"Comparison:","text":"<p>First-Visit Monte Carlo updates the value function only the first time a state is encountered in an episode, ensuring an unbiased estimate but using fewer samples, which can result in higher variance and slower learning. In contrast, Every-Visit Monte Carlo updates the value function on all occurrences of a state within an episode, reducing variance and improving sample efficiency by utilizing more data. Although it may introduce bias, it often converges faster, making it more practical in many applications.</p>"},{"location":"course_notes/value-based2/#36-incremental-monte-carlo-policy","title":"3.6. Incremental Monte Carlo Policy","text":"<p>In addition to First-Visit and Every-Visit Monte Carlo, an alternative approach is the Incremental Monte Carlo Policy, which updates the value function incrementally after each visit instead of computing an average over all episodes. This method is more memory-efficient and allows real-time updates without storing past returns.</p> <p>Given the return \\(G_{i,t}\\) observed for state \\(s\\) at time \\(t\\) in episode \\(i\\), we update the value function as:</p> \\[ V^\\pi(s) = V^\\pi(s) \\frac{N(s) - 1}{N(s)} + \\frac{G_{i,t}}{N(s)} \\] <p>which can be rewritten as:</p> \\[ V^\\pi(s) = V^\\pi(s) + \\frac{1}{N(s)} (G_{i,t} - V^\\pi(s)) \\] <ul> <li>The update formula behaves like a running average, gradually incorporating new information.</li> </ul> <p>This approach ensures smooth updates, avoids storing all past returns, and is more computationally efficient, especially in long episodes or large state spaces.</p>"},{"location":"course_notes/value-based2/#4-temporal-difference-td-learning","title":"4. Temporal Difference (TD) Learning","text":"<p>Temporal Difference (TD) Learning is a method for estimating value functions in reinforcement learning. </p>"},{"location":"course_notes/value-based2/#41-td-prediction","title":"4.1. TD Prediction","text":"<p>TD Learning updates the value function using the Bellman equation. It differs from Monte Carlo methods in that it updates after each step rather than waiting for the entire episode to finish. The general TD update rule for state-value function \\(V^\\pi(s_t)\\) is:</p> \\[ V^\\pi(s_t) = V^\\pi(s_t) + \\alpha \\left[ R_{t+1} + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t) \\right] \\] <p>This approach is called bootstrapping since it estimates future rewards based on the current value function rather than waiting for the full return.</p> <p>Just like for state-value functions, we can extend TD Learning to action-value functions (Q-values). The TD update rule for \\(Q(s_t, a_t)\\) is:</p> \\[ Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right] \\] <p>This allows TD learning to be applied to control tasks, where the agent needs to improve its policy while learning. </p>"},{"location":"course_notes/value-based2/#43-on-policy-vs-off-policy-td-learning","title":"4.3. On-Policy vs. Off-Policy TD Learning","text":"<p>TD methods can be used for both on-policy and off-policy learning:</p> <ul> <li>SARSA (On-Policy TD Control): Updates the action-value function based on the agent\u2019s actual policy.</li> <li>Q-Learning (Off-Policy TD Control): Updates based on the optimal action, regardless of the agent\u2019s current policy.</li> </ul>"},{"location":"course_notes/value-based2/#sarsa-algorithm-on-policy","title":"SARSA Algorithm (On-Policy)","text":"<p>In SARSA, the agent chooses the next action \\(a_{t+1}\\) according to its current policy and updates the Q-value based on the immediate reward and the Q-value for the next state-action pair.</p> <pre><code>Initialize Q(s, a) arbitrarily, for all s \u2208 S, a \u2208 A(s), and Q(terminal-state, \u00b7) = 0\nRepeat (for each episode):\n    Initialize S\n    Choose A from S using policy derived from Q (e.g., \u025b-greedy)\n    Repeat (for each step of episode):\n         Take action A, observe R and next state S'\n         Choose A' from S' using policy derived from Q (e.g., \u025b-greedy)\n\n         Update Q-value:\n         Q(S, A) \u2190 Q(S, A) + \u03b1[R + \u03b3 * Q(S', A') - Q(S, A)]\n\n         Set S \u2190 S', A \u2190 A'\n\n    until S is terminal\n</code></pre>"},{"location":"course_notes/value-based2/#q-learning-algorithm-off-policy","title":"Q-Learning Algorithm (Off-Policy)","text":"<p>Q-learning is an off-policy algorithm that learns the best action-value function, no matter what behavior policy the agent used to gather the data.</p> <pre><code>Initialize Q(s, a) arbitrarily, for all s \u2208 S, a \u2208 A(s), and Q(terminal-state, \u00b7) = 0\nRepeat (for each episode):\n    Initialize S\n\n    Repeat (for each step of episode):\n         Choose A from S using policy derived from Q (e.g., \u025b-greedy)\n\n         Take action A, observe R and next state S'\n\n         Update Q-value:\n         Q(S, A) \u2190 Q(S, A) + \u03b1[R + \u03b3 * max_a Q(S', A') - Q(S, A)]\n\n         Set S \u2190 S'\n\n    until S is terminal\n</code></pre>"},{"location":"course_notes/value-based2/#44-exploitation-vs-exploration","title":"4.4. Exploitation vs Exploration","text":""},{"location":"course_notes/value-based2/#balancing-exploration-and-exploitation","title":"Balancing Exploration and Exploitation","text":"<p>In reinforcement learning, an agent needs to balance exploration (trying new actions) and exploitation (using known actions that give good rewards). To do this, we use an \\(\\epsilon\\)-greedy policy:</p> \\[ \\pi(a_t | s_t) =  \\begin{cases}  \\arg\\max_a Q(s_t, a) &amp; \\text{with probability } 1 - \\epsilon \\\\ \\text{random action} &amp; \\text{with probability } \\epsilon \\end{cases} \\] <p>At the start of learning, \\(\\epsilon\\) is high to encourage exploration. As the agent learns more about the environment, \\(\\epsilon\\) decreases, allowing the agent to focus more on exploiting the best actions it has learned. This process is called epsilon decay.</p> <p>Common ways to decay \\(\\epsilon\\) include:</p> <ul> <li>Linear Decay: </li> </ul> \\[ \\epsilon_t = \\frac{1}{t} \\] <p>where \\(t\\) is the time step.</p> <ul> <li>Exponential Decay:</li> </ul> \\[ \\epsilon_t = \\epsilon_0 \\cdot \\text{decay_rate}^t \\] <p>Watch on YouTube</p>"},{"location":"course_notes/value-based2/#5-summary-of-key-concepts-and-methods","title":"5. Summary of Key Concepts and Methods","text":"<p>In reinforcement learning, various methods are used to estimate value functions and find optimal policies. These methods can be broadly categorized into Model-Based and Model-Free learning, as well as On-Policy and Off-Policy learning. Below is a concise summary of these key concepts and a comparison of different approaches.</p>"},{"location":"course_notes/value-based2/#51-model-based-vs-model-free-learning","title":"5.1. Model-Based vs. Model-Free Learning","text":""},{"location":"course_notes/value-based2/#model-based-learning","title":"Model-Based Learning:","text":"<ul> <li>Definition: The agent uses a model of the environment to predict future states and rewards. The model allows the agent to simulate actions and outcomes.</li> <li>Example: Dynamic Programming (DP) relies on a complete model of the environment.</li> <li>Advantages: Efficient when the model is available and provides exact solutions when the environment is known.</li> </ul>"},{"location":"course_notes/value-based2/#model-free-learning","title":"Model-Free Learning:","text":"<ul> <li>Definition: The agent learns directly from interactions with the environment by estimating value functions based on observed rewards, without needing a model.</li> <li>Examples: Monte Carlo (MC), Temporal Difference (TD).</li> <li>Advantages: More flexible, applicable when the model is unknown or too complex to compute.</li> </ul>"},{"location":"course_notes/value-based2/#52-on-policy-vs-off-policy-learning","title":"5.2. On-Policy vs. Off-Policy Learning","text":""},{"location":"course_notes/value-based2/#on-policy-learning","title":"On-Policy Learning:","text":"<ul> <li>Definition: The agent learns about and improves the policy it is currently following. The policy that generates the data is the same as the one being evaluated and improved.</li> <li>Example: SARSA updates based on actions taken under the current policy.</li> <li>Advantages: Simpler and guarantees that the agent learns from its own actions.</li> </ul>"},{"location":"course_notes/value-based2/#off-policy-learning","title":"Off-Policy Learning:","text":"<ul> <li>Definition: The agent learns about an optimal policy while following a different behavior policy. The target policy is updated while the agent explores using a behavior policy.</li> <li>Example: Q-Learning updates based on the optimal action, independent of the behavior policy.</li> <li>Advantages: More flexible, allows for learning from past experiences and using different exploration strategies.</li> </ul>"},{"location":"course_notes/value-based2/#53-comparison","title":"5.3. Comparison","text":"Feature TD (Temporal Difference) Monte Carlo (MC) Dynamic Programming (DP) Model Requirement No model required (model-free) No model required (model-free) Requires a full model of the environment Learning Method Updates based on current estimates (bootstrapping) Updates after complete episode (no bootstrapping) Updates based on exact model (transition probabilities) Update Frequency After each step After each episode After each step or full sweep over states Efficiency More sample efficient (incremental learning) Less efficient (requires full episodes) Very efficient, but needs a model Convergence Converges with sufficient exploration Converges with sufficient exploration Converges to optimal policy with a known model Suitability Works well in ongoing tasks Works well for episodic tasks Works well in fully known environments <p>The choice of method depends on the environment, the availability of a model, and the trade-off between exploration and exploitation.</p>"},{"location":"course_notes/value-based2/#references","title":"References","text":"<ul> <li>Sutton, R.S., &amp; Barto, A.G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press.</li> <li>monte-carlo for integration</li> </ul>"},{"location":"exams/","title":"Previous Semesters","text":""},{"location":"exams/#spring-2024-exams","title":"Spring 2024 Exams","text":"Exam Download Quizzes Quizzes Midterm Midterm Final Final"},{"location":"exams/#spring-2023-exams","title":"Spring 2023 Exams","text":"Exam Download Quizzes Quizzes Midterm Midterm Final Final"},{"location":"exams/final/","title":"Spring 2025 Final","text":""},{"location":"exams/midterm/","title":"Spring 2025 Midterm","text":""},{"location":"exams/midterm/#midterm-qa","title":"Midterm QA","text":""},{"location":"exams/midterm/#screen-record","title":"Screen Record","text":""},{"location":"guests/abhishek_gupta/","title":"Abhishek Gupta","text":""},{"location":"guests/abhishek_gupta/#about","title":"About","text":"<p>Abhishek Gupta is an Assistant Professor in the Paul G. Allen School of Computer Science and Engineering at the University of Washington, where he leads the Washington Embodied Intelligence and Robotics Development (WEIRD) Lab. His research focuses on enabling robotic systems to learn complex tasks in unstructured, human-centric environments such as homes and offices. Gupta works on real-world reinforcement learning with themes like deployment-time adaptation, human-in-the-loop learning, and leveraging off-domain data from videos, simulations, and generative models. He earned his Ph.D. in machine learning and robotics at UC Berkeley, advised by Sergey Levine and Pieter Abbeel, and later held a postdoctoral position at MIT, collaborating with Russ Tedrake and Pulkit Agrawal. His broader research interests include offline RL, dexterous manipulation, meta-learning, safe and scalable adaptation, and the development of foundation models for embodied AI. Read more</p>"},{"location":"guests/abhishek_gupta/#lecture","title":"Lecture","text":""},{"location":"guests/abhishek_gupta/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/adam_white/","title":"Adam White","text":""},{"location":"guests/adam_white/#about","title":"About","text":"<p>Adam White is an Assistant Professor at the University of Alberta in the Department of Computing Science and a Canada CIFAR AI Chair as well as Principal Investigator of the Reinforcement Learning &amp; Artificial Intelligence Lab (RLAI), and also serves as a Senior Research Scientist at DeepMind. His research focuses on the theoretical and empirical foundations of reinforcement learning\u2014particularly continual learning, knowledge representation, intrinsic motivation, and deployment in real-world control systems\u2014blending work in simulated environments, robotics, and industrial applications. He co-created influential tools and architectures like RL-Glue and Horde for scalable, real-time RL experimentation, co-developed the acclaimed Reinforcement Learning Specialization on Coursera used by tens of thousands, and has received accolades including \u201cPaper of Distinction\u201d and best paper awards in robotics conferences. Read more</p>"},{"location":"guests/adam_white/#lecture","title":"Lecture","text":""},{"location":"guests/adam_white/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/amy_zhang/","title":"Amy Zhang","text":""},{"location":"guests/amy_zhang/#about","title":"About","text":"<p>Amy Zhang is an Assistant Professor of Electrical &amp; Computer Engineering at UT\u202fAustin and a Texas Instruments/Kilby Fellow, studying reinforcement learning with an emphasis on sample efficiency, generalization, and state abstraction in both simulation and real-world robotics. Her work bridges theory and practice to develop robust RL algorithms for sequential decision-making; she also leads UT\u2019s MIDI lab and recently secured an Army Research Office award for integrating combinatorial generalization with RL methods. Read more</p>"},{"location":"guests/amy_zhang/#lecture","title":"Lecture","text":""},{"location":"guests/amy_zhang/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/anne_collins/","title":"Anne Collins","text":""},{"location":"guests/anne_collins/#about","title":"About","text":"<p>Anne\u202fG.\u202fE.\u202fCollins is an Associate Professor in the Department of Psychology at UC\u202fBerkeley and a faculty member in the Helen Wills Neuroscience Institute, where she leads the Computational Cognitive Neuroscience (CCN) Lab. Her research develops and tests computational models of human learning, decision-making, and executive functions\u2014especially how reinforcement learning, working memory, and neural signals interact\u2014to understand flexible, adaptive cognition. Using behavioral experiments, EEG, imaging, and sophisticated modeling (e.g., Bayesian inference, neural networks), her work has revealed how fast working memory systems can interfere with slower but more durable reinforcement learning, with implications for AI and psychiatry.</p>"},{"location":"guests/anne_collins/#lecture","title":"Lecture","text":""},{"location":"guests/anne_collins/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/benjamin_eysenbach/","title":"Benjamin Eysenbach","text":""},{"location":"guests/benjamin_eysenbach/#about","title":"About","text":"<p>Benjamin Eysenbach is an Assistant Professor of Computer Science at Princeton University, where he leads the Princeton Reinforcement Learning Lab. His research focuses on designing reinforcement learning (RL) algorithms that learn intelligent behaviors through self-supervised methods, eliminating the need for explicit rewards or human supervision. Eysenbach's work emphasizes simplicity, scalability, and robustness, contributing to advancements in goal-conditioned RL and contrastive learning techniques. He earned his Ph.D. in Machine Learning from Carnegie Mellon University, advised by Ruslan Salakhutdinov and Sergey Levine, and held research positions at Google Brain. His academic journey began with undergraduate studies in mathematics at MIT. At Princeton, Eysenbach teaches courses on reinforcement learning and probabilistic inference, and his lab's research has been featured in leading conferences such as ICLR and NeurIPS. Read more</p>"},{"location":"guests/benjamin_eysenbach/#lecture","title":"Lecture","text":""},{"location":"guests/benjamin_eysenbach/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/benjamin_van_roy/","title":"Benjamin Van Roy","text":""},{"location":"guests/benjamin_van_roy/#about","title":"About","text":"<p>Benjamin Van Roy is a Professor at Stanford University, where he has been on the faculty since 1998, and a leading expert in reinforcement learning, with research focused on the design and analysis of intelligent agents. He also founded and leads the Efficient Agent Team at Google DeepMind and has directed research initiatives at Morgan Stanley, Unica (acquired by IBM), and Enuvis (acquired by SiRF), which he co-founded. Van Roy holds SB, SM, and PhD degrees from MIT in Electrical Engineering and Computer Science, with his doctoral work advised by John N. Tsitsiklis. A Fellow of both INFORMS and IEEE, he has served on the editorial boards of several top journals in machine learning and operations research. His contributions have been recognized with numerous awards, including the NSF CAREER Award, the INFORMS Frederick W. Lanchester Prize, and multiple teaching honors from Stanford. He has advised dozens of PhD students who have gone on to influential roles in academia, industry, and finance. Read more</p>"},{"location":"guests/benjamin_van_roy/#lecture","title":"Lecture","text":""},{"location":"guests/benjamin_van_roy/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/chris_watkins/","title":"Chris Watkins","text":""},{"location":"guests/chris_watkins/#about","title":"About","text":"<p>Professor Christopher Watkins is a world-class authority on reinforcement learning and evolutionary theory. He is a Professor of Machine Learning at the Department of Computer Science, Royal Holloway, University of London. Renowned for his foundational contributions to artificial intelligence, Professor Watkins introduced the Q-learning algorithm, a pivotal breakthrough that laid the groundwork for modern reinforcement learning. Read more</p>"},{"location":"guests/chris_watkins/#lecture","title":"Lecture","text":""},{"location":"guests/chris_watkins/#slides","title":"Slides","text":"<p>Download Slides</p> <p>Download Slides</p>"},{"location":"guests/christopher_amato/","title":"Christopher Amato","text":""},{"location":"guests/christopher_amato/#about","title":"About","text":"<p>Christopher Amato is an Associate Professor at Northeastern University where he leads the Lab for Learning and Planning in Robotics. He has published many papers in leading artificial intelligence, machine learning and robotics conferences (including winning a best paper prize at AAMAS-14 and being nominated for the best paper at RSS-15, AAAI-19, AAMAS-21 and MRS-21). He has also successfully co-organized several tutorials on multi-agent coordination and has co-authored a book on the subject. He has also won several awards such as Amazon Research Awards and an NSF CAREER Award. His research focuses on reinforcement learning in partially observable and multi-agent/multi-robot systems. Read more</p>"},{"location":"guests/christopher_amato/#lecture","title":"Lecture","text":""},{"location":"guests/christopher_amato/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/ian_osband/","title":"Ian Osband","text":""},{"location":"guests/ian_osband/#about","title":"About","text":"<p>Ian Osband is a researcher in artificial intelligence, focusing on decision-making under uncertainty, particularly in reinforcement learning (RL). He is known for his work on efficient exploration strategies, notably through the development of randomized value functions, as detailed in his Ph.D. thesis, \"Deep Exploration via Randomized Value Functions,\" which earned second place in the national Dantzig dissertation award . Osband completed his Ph.D. at Stanford University under the supervision of Benjamin Van Roy, following undergraduate studies in mathematics at Oxford University. He has held research positions at DeepMind and OpenAI, where he contributed to advancements in RL algorithms and their applications. His work has significantly influenced both theoretical and practical aspects of reinforcement learning, particularly in the areas of exploration and uncertainty estimation. Read more</p>"},{"location":"guests/ian_osband/#lecture","title":"Lecture","text":""},{"location":"guests/ida_momennejad/","title":"Ida Momennejad","text":""},{"location":"guests/ida_momennejad/#about","title":"About","text":"<p>Ida Momennejad is a Principal Researcher at Microsoft Research NYC, where she develops and evaluates generative AI systems inspired by her work in cognitive neuroscience, reinforcement learning, and NeuroAI, with a focus on how humans and machines build internal world models for memory, exploration, and planning. She designs brain- and behavior-inspired algorithms\u2014combining reinforcement learning, neural networks, large language models, machine learning, behavioral experiments, fMRI, and electrophysiology\u2014with applications ranging from AI for Xbox gaming to foundational advances in AI reasoning. Beyond her research, she co-hosts The Learning Salon every Friday at 4 PM ET, mentors creative scientists at the New Museum\u2019s New Inc incubator, and shares her insights on the Microsoft Research AI Frontiers podcast and other outlets. Academically, she earned a BSc in software engineering in Tehran, an MSc in Philosophy of Science in Utrecht, and a PhD in psychology from the Bernstein Center for Computational Neuroscience in Berlin, followed by postdoctoral research at Princeton University and earlier work at Columbia University\u2019s Electrophysiology, Memory, and Navigation Lab. Read more</p>"},{"location":"guests/ida_momennejad/#lecture","title":"Lecture","text":""},{"location":"guests/jakob_foerster/","title":"Jakob Foerster","text":""},{"location":"guests/jakob_foerster/#about","title":"About","text":"<p>Jakob Foerster is an Associate Professor in the Department of Engineering Science at the University of Oxford and a Research Scientist at FAIR, Meta AI. He is also a Supernumerary Fellow at St Anne's College, Oxford. Foerster leads the FLAIR (Foundations of Learning, AI, and Robotics) research group, focusing on multi-agent reinforcement learning, human-AI coordination, and the broader impact of AI on society and science. His work has influenced both academia and industry, with applications spanning finance, bioengineering, and large-scale AI systems. Read more</p>"},{"location":"guests/jakob_foerster/#lecture","title":"Lecture","text":""},{"location":"guests/jakob_foerster/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/jeff_clune/","title":"Jeff Clune","text":""},{"location":"guests/jeff_clune/#about","title":"About","text":"<p>Jeff Clune is a Professor of Computer Science at the University of British Columbia, a Canada CIFAR AI Chair at the Vector Institute, and a Senior Research Advisor at DeepMind. His research centers on deep learning and deep reinforcement learning, with a particular focus on open-ended learning and the evolution of intelligence. Previously, he held research leadership roles at OpenAI and Uber AI Labs\u2014where he was a founding member\u2014and was a faculty member at the University of Wyoming and a research scientist at Cornell. Clune earned his Ph.D. and master\u2019s degrees from Michigan State University and his bachelor\u2019s from the University of Michigan. He has received numerous prestigious honors, including the Presidential Early Career Award for Scientists and Engineers, an NSF CAREER award, and multiple best paper and Test of Time awards at top conferences like NeurIPS, CVPR, ICLR, and ICML. His work has been featured in premier journals such as Nature, Science, and PNAS, and widely covered by major media outlets including The New York Times, BBC, and National Geographic. Read more</p>"},{"location":"guests/jeff_clune/#lecture","title":"Lecture","text":""},{"location":"guests/jeff_clune/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/karl_friston/","title":"Karl Friston","text":""},{"location":"guests/karl_friston/#about","title":"About","text":"<p>Karl Friston is a theoretical neuroscientist and authority on brain imaging. He invented statistical parametric mapping (SPM), voxel-based morphometry (VBM) and dynamic causal modelling (DCM). These contributions were motivated by schizophrenia research and theoretical studies of value-learning, formulated as the dysconnection hypothesis of schizophrenia. Mathematical contributions include variational Laplacian procedures and generalized filtering for hierarchical Bayesian model inversion. Friston currently works on models of functional integration in the human brain and the principles that underlie neuronal interactions. His main contribution to theoretical neurobiology is a free-energy principle for action and perception (active inference). Friston received the first Young Investigators Award in Human Brain Mapping (1996) and was elected a Fellow of the Academy of Medical Sciences (1999). In 2000 he was President of the international Organization of Human Brain Mapping. In 2003 he was awarded the Minerva Golden Brain Award and was elected a Fellow of the Royal Society in 2006. In 2008 he received a Medal, College de France and an Honorary Doctorate from the University of York in 2011. He became of Fellow of the Royal Society of Biology in 2012, received the Weldon Memorial prize and Medal in 2013 for contributions to mathematical biology and was elected as a member of EMBO (excellence in the life sciences) in 2014 and the Academia Europaea in (2015). He was the 2016 recipient of the Charles Branch Award for unparalleled breakthroughs in Brain Research and the Glass Brain Award, a lifetime achievement award in the field of human brain mapping. He holds Honorary Doctorates from the Universities of York, Zurich and Radboud University. Read more</p>"},{"location":"guests/karl_friston/#lecture","title":"Lecture","text":""},{"location":"guests/karl_friston/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/luis_serrano/","title":"Luis Serrano","text":""},{"location":"guests/luis_serrano/#about","title":"About","text":"<p>Luis Serrano is the founder of Serrano Academy\u2014a popular educational YouTube channel with over 150K subscribers\u2014where he makes complex math and machine learning intuitively accessible. He authored the highly praised Grokking Machine Learning (Manning, 2021), designed to teach core ML concepts using only high-school math and intuitive examples. Previously, he led AI education efforts as a machine learning engineer at Google (working on YouTube recommendations), Head of AI content at Udacity, and Lead AI Educator at Apple. Serrano holds a PhD in mathematics from the University of Michigan, an MSc and BSc from the University of Waterloo, and completed a postdoc in quantum machine learning at Universit\u00e9 du Qu\u00e9bec \u00e0 Montr\u00e9al. Read more</p>"},{"location":"guests/luis_serrano/#lecture","title":"Lecture","text":""},{"location":"guests/mark_ho/","title":"Mark Ho","text":""},{"location":"guests/mark_ho/#about","title":"About","text":"<p>Mark Ho is an Assistant Professor of Psychology at New York University and an affiliated faculty member at NYU\u2019s Center for Data Science. His research integrates cognitive science, social psychology, neuroscience, and computer science to develop computational theories of how people\u2019s goals, values, and motivations shape their thoughts, decisions, and social interactions. Ho's work focuses on understanding the principles underlying human problem-solving and social cognition, aiming to inform the design of intelligent systems that interact effectively with humans. He earned his Ph.D. in Cognitive Science and M.S. in Computer Science from Brown University, followed by postdoctoral research at Princeton University. His research has been published in leading journals, including Nature, where he explored how people construct simplified mental representations to plan. Read more</p>"},{"location":"guests/mark_ho/#lecture","title":"Lecture","text":""},{"location":"guests/mark_ho/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/marlos_machado/","title":"Marlos C. Machado","text":""},{"location":"guests/marlos_machado/#about","title":"About","text":"<p>Marlos\u202fC. Machado is an Assistant Professor in the Department of Computing Science at the University of Alberta, a Canada CIFAR AI Chair and Amii Fellow, and a Principal Investigator in the Reinforcement Learning &amp; Artificial Intelligence (RLAI) group. His research focuses on deep reinforcement learning, representation learning, continual learning, and real-world applications\u2014emphasizing algorithms that autonomously discover temporal abstractions (\u201ceigenoptions\u201d) via the successor representation. During his Ph.D., he introduced stochasticity and game modes in the Arcade Learning Environment and popularized eigenoptions; post-Ph.D., he spent four years at DeepMind and Google Brain, contributing to real-world systems like stratospheric balloon control. His work, featured in top venues such as Nature, JMLR, NeurIPS, ICML, and ICLR, as well as media outlets like BBC, Bloomberg, The Verge, and Wired, continues to influence foundational advances in exploration and abstraction in RL. Read more</p>"},{"location":"guests/marlos_machado/#lecture","title":"Lecture","text":""},{"location":"guests/marlos_machado/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/martha_white/","title":"Martha White","text":""},{"location":"guests/martha_white/#about","title":"About","text":"<p>Martha White is an Associate Professor of Computing Science at the University of Alberta, a Canada CIFAR AI Chair, and a Fellow at Amii, where she leads research on adaptive reinforcement learning agents capable of continual learning in real-world environments. Her work focuses on representation learning\u2014employing sparse and recurrent neural architectures\u2014and off-policy methods that allow agents to learn from streams of data, supporting applications such as control of industrial processes like water treatment systems. She also plays a key role in co-founding RL CORE with Adam White, has delivered invited talks at top venues like ICLR, ICML, and NeurIPS, and has earned notable distinctions including IEEE\u2019s \u201cAI\u2019s 10 to Watch\u201d award. Read more</p>"},{"location":"guests/martha_white/#lecture","title":"Lecture","text":""},{"location":"guests/martha_white/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/michael_littman/","title":"Michael Littman","text":""},{"location":"guests/michael_littman/#about","title":"About","text":"<p>Michael L. Littman is a University Professor of Computer Science at Brown University, studying machine learning and decision making under uncertainty. He has earned multiple university-level awards for teaching and his research on reinforcement learning, probabilistic planning, and automated crossword-puzzle solving has been recognized with three best-paper awards and three influential paper awards. Littman is co-director of Brown's Humanity Centered Robotics Initiative and a Fellow of the Association for the Advancement of Artificial Intelligence and the Association for Computing Machinery. He is also a Fellow of the American Association for the Advancement of Science Leshner Leadership Institute for Public Engagement with Science, focusing on Artificial Intelligence. Read more</p>"},{"location":"guests/michael_littman/#lecture","title":"Lecture","text":""},{"location":"guests/michael_littman/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/nan_jiang/","title":"Nan Jiang","text":""},{"location":"guests/nan_jiang/#about","title":"About","text":"<p>Nan Jiang (\u59dc\u6960) is an Associate Professor in the Department of Computer Science at the University of Illinois at Urbana-Champaign (UIUC). His research focuses on building the theoretical foundations of reinforcement learning (RL), particularly in the function-approximation setting, with an emphasis on developing sample-efficient algorithms by drawing from statistical learning theory. He earned his Ph.D. in Computer Science and Engineering from the University of Michigan and was a postdoctoral researcher at Microsoft Research New York City before joining UIUC. Jiang's contributions have been recognized with several honors, including the NSF CAREER Award, a Sloan Research Fellowship, and a Google Research Scholar Award. He also serves as an action editor for the Journal of Machine Learning Research and an editor for Foundations and Trends in Machine Learning. Beyond research, he is deeply committed to education and mentorship, having received multiple teaching awards at UIUC, and his work continues to shape both theoretical and applied aspects of reinforcement learning. Read more</p>"},{"location":"guests/nan_jiang/#lecture","title":"Lecture","text":""},{"location":"guests/nan_jiang/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/pascal_poupart/","title":"Pascal Poupart","text":""},{"location":"guests/pascal_poupart/#about","title":"About","text":"<p>Pascal Poupart is a Professor in the David R. Cheriton School of Computer Science at the University of Waterloo (Canada). He is also a Canada CIFAR AI Chair at the Vector Institute and a member of the Waterloo AI Institute. He serves on the advisory board of the NSF AI Institute for Advances in Optimization (2022-present) at Georgia Tech, UC Berkeley and University of Southern California. He served as Research Director and Principal Research Scientist at the Waterloo Borealis AI Research Lab at the Royal Bank of Canada (2018-2020). He also served as scientific advisor for ProNavigator (2017-2019), ElementAI (2017-2018) and DialPad (2017-2018). His research focuses on the development of algorithms for Machine Learning with application to Natural Language Processing and Material Discovery. He is most well-known for his contributions to the development of Reinforcement Learning algorithms. Notable projects that his research team is currently working on include democratizing large language models, inverse constraint learning, mean field RL, RL foundation models, Bayesian federated learning, uncertainty quantification, probabilistic deep learning, conversational agents, transcription error correction, sport analytics, and material discovery for CO2 recycling. Read more</p>"},{"location":"guests/pascal_poupart/#lecture","title":"Lecture","text":""},{"location":"guests/pascal_poupart/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/peter_dayan/","title":"Peter Dayan","text":""},{"location":"guests/peter_dayan/#about","title":"About","text":"<p>Peter Dayan is a Director at the Max Planck Institute for Biological Cybernetics in T\u00fcbingen and a leading figure in theoretical neuroscience and artificial intelligence. He studied mathematics at Cambridge University and earned his Ph.D. from the University of Edinburgh, followed by postdoctoral work at the Salk Institute and the University of Toronto. After serving as an assistant professor at MIT, he co-founded the renowned Gatsby Computational Neuroscience Unit in London and led it as Director from 2002 to 2017. His research explores decision-making in the brain, the roles of neuromodulators, and computational models of psychiatric disorders, placing him at the intersection of neuroscience and AI. Dayan has received numerous accolades, including the Rumelhart Prize, the Brain Prize, and Germany\u2019s prestigious Alexander von Humboldt Professorship. He is a Fellow of both the Royal Society and the AAAS, and continues to shape our understanding of how brains and machines learn and make decisions. Read more</p>"},{"location":"guests/peter_dayan/#lecture","title":"Lecture","text":""},{"location":"guests/peter_dayan/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/peter_norvig/","title":"Peter Norvig","text":""},{"location":"guests/peter_norvig/#about","title":"About","text":"<p>Peter Norvig (born December\u202f14,\u202f1956) is an American computer scientist, Distinguished Education Fellow at Stanford\u2019s Human\u2011Centered AI Institute, and Director of Research at Google, previously overseeing Google\u2019s core search algorithms and broader research initiatives. He co-authored Artificial Intelligence: A Modern Approach\u2014the leading AI textbook used at over 1,500 universities worldwide\u2014and Paradigms of AI Programming, and authored influential essays such as \u201cThe Unreasonable Effectiveness of Data\u201d. Before Google, he led the Computational Sciences Division at NASA Ames, earning the NASA Exceptional Achievement Award in 2001, and held roles at Junglee, Sun Microsystems Labs, USC, and UC\u202fBerkeley. A Fellow of AAAI, ACM, the California Academy of Sciences, and the American Academy of Arts &amp; Sciences, Norvig also pioneered AI education by teaching a 160,000\u2011student online AI class, helping launch MOOCs via Udacity and edX. Read more</p>"},{"location":"guests/peter_norvig/#lecture","title":"Lecture","text":""},{"location":"guests/peter_stone/","title":"Peter Stone","text":""},{"location":"guests/peter_stone/#about","title":"About","text":"<p>Peter Stone is an American computer scientist who holds the Truchard Foundation Chair of Computer Science at The University of Texas at Austin. He is also Chief Scientist of Sony AI, an Alfred P. Sloan Research Fellow, Guggenheim Fellow, AAAI Fellow, IEEE Fellow, AAAS Fellow, ACM Fellow, and Fulbright Scholar. Read more</p>"},{"location":"guests/peter_stone/#lecture","title":"Lecture","text":""},{"location":"guests/richard_sutton/","title":"Richard Sutton","text":""},{"location":"guests/richard_sutton/#about","title":"About","text":"<p>Richard S. Sutton is a Canadian computer scientist. He is a professor of computing science at the University of Alberta and a research scientist at Keen Technologies. Sutton is considered one of the founders of modern computational reinforcement learning, having several significant contributions to the field, including temporal difference learning and policy gradient methods. Read more on Wikipedia</p>"},{"location":"guests/richard_sutton/#lecture","title":"Lecture","text":""},{"location":"guests/richard_sutton/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/wolfram_schultz/","title":"Wolfram Schultz","text":""},{"location":"guests/wolfram_schultz/#about","title":"About","text":"<p>Wolfram Schultz FRS is a Professor of Neuroscience at the University of Cambridge and a Wellcome Principal Research Fellow, renowned for his groundbreaking discovery that dopamine neurons encode reward prediction errors\u2014a foundational insight linking neuroscience with reinforcement learning and economic decision theory. After earning his medical degree from the University of Heidelberg and a PhD in physiology from the University of Fribourg, he conducted postdoctoral research in Germany, the USA, and Sweden. Schultz's research focuses on how the brain processes reward, utility, risk, and decision-making, with particular attention to dopamine neurons, the striatum, orbitofrontal cortex, and amygdala. His work has significantly advanced the fields of neuroeconomics and computational neuroscience, earning him prestigious honors such as the Brain Prize, the Gruber Prize in Neuroscience, and election as a Fellow of the Royal Society. Read more</p>"},{"location":"guests/wolfram_schultz/#lecture","title":"Lecture","text":""},{"location":"guests/wolfram_schultz/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"home/calender/","title":"Calender","text":"<p>This Google Calendar outlines the schedule for the Deep Reinforcement Learning course, including weekly lecture times, assignment deadlines, and other important events.</p>"},{"location":"homeworks/","title":"Previous Semesters","text":""},{"location":"homeworks/#spring-2024-homeworks","title":"Spring 2024 Homeworks","text":"Homework # Problems Solutions HW1 HW1 Problems HW1 Solutions HW2 HW2 Problems HW2 Solutions HW3 HW3 Problems HW3 Solutions HW4 HW4 Problems HW4 Solutions"},{"location":"homeworks/#spring-2023-homeworks","title":"Spring 2023 Homeworks","text":"Homework # Problems Solutions HW0 HW0 Problems HW0 Solutions HW1 HW1 Problems HW1 Solutions HW2 HW2 Problems HW2 Solutions HW3 HW3 Problems HW3 Solutions HW4 HW4 Problems HW4 Solutions"},{"location":"homeworks/week1/","title":"HW1: Introduction to RL","text":"<p>Welcome to the first homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and workshops.</p>"},{"location":"homeworks/week1/#homework","title":"Homework","text":"<p>You can download the homework questions and the notebook from the following links:</p> <p>HW1 Questions</p> <p>HW1 Notebook</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week1/#explanation","title":"Explanation","text":""},{"location":"homeworks/week1/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f2\u06f8 \u0628\u0647\u0645\u0646 (February 16) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week1/#solution","title":"Solution","text":"<p>HW1 Solution Notebook HW1 Solution Slides</p>"},{"location":"homeworks/week10/","title":"HW10: Exploration Methods","text":"<p>Welcome to the 10th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures, recitation, and workshop.</p>"},{"location":"homeworks/week10/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW10 Questions</p> <p>HW10 Notebook 1 HW10 Notebook 2</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week10/#explanation","title":"Explanation","text":""},{"location":"homeworks/week10/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f1\u06f6 \u062e\u0631\u062f\u0627\u062f (June 6) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week11/","title":"HW11: Imitation &amp; Inverse RL","text":"<p>Welcome to the 11th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures, recitation, and workshop.</p>"},{"location":"homeworks/week11/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW11 Questions</p> <p>HW11 Notebook</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week11/#explanation","title":"Explanation","text":""},{"location":"homeworks/week11/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f1\u06f3 \u062a\u06cc\u0631 (July 4) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week12/","title":"HW12: Offline Methods","text":"<p>Welcome to the 12th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures, recitation, and workshop.</p>"},{"location":"homeworks/week12/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW12 Questions</p> <p>HW12 Notebook</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week12/#explanation","title":"Explanation","text":""},{"location":"homeworks/week12/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f1\u06f3 \u062a\u06cc\u0631 (July 4) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week13/","title":"HW13: Multi-Agent Methods","text":"<p>Welcome to the 13th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures, recitation, and workshop.</p>"},{"location":"homeworks/week13/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW13 Questions</p> <p>HW13 Notebook 1 HW13 Notebook 2</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week13/#explanation","title":"Explanation","text":""},{"location":"homeworks/week13/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f2\u06f0 \u0634\u0647\u0631\u06cc\u0648\u0631 (September 11) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week14/","title":"HW14: Hierarchical &amp; Meta RL","text":"<p>Welcome to the 14th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures, recitation, and workshop.</p>"},{"location":"homeworks/week14/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW14 Questions</p> <p>HW14 Notebook</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week14/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f2\u06f0 \u0634\u0647\u0631\u06cc\u0648\u0631 (September 11) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week2/","title":"HW2: Value-Based Methods","text":"<p>Welcome to the second homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and workshops.</p>"},{"location":"homeworks/week2/#homework","title":"Homework","text":"<p>You can download the homework questions and the notebook from the following links:</p> <p>HW2 Questions</p> <p>HW2 Notebook 1 HW2 Notebook 2</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week2/#explanation","title":"Explanation","text":""},{"location":"homeworks/week2/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f1\u06f2 \u0627\u0633\u0641\u0646\u062f (March 2) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week2/#solution","title":"Solution","text":"<p>HW2 Solution to Questions</p> <p>HW2 Solution Notebook | Part 1: SARSA(n) and Q-Learning(n)</p> <p>HW2 Solution Notebook | Part 2: DQN vs DDQN</p>"},{"location":"homeworks/week3/","title":"HW3: Policy-Based Methods","text":"<p>Welcome to the third homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and workshops.</p>"},{"location":"homeworks/week3/#homework","title":"Homework","text":"<p>You can download the homework questions and the notebook from the following links:</p> <p>HW3 Questions</p> <p>HW3 Notebook 1 HW3 Notebook 2 HW3 Notebook 3 HW3 Notebook 4</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week3/#explanation","title":"Explanation","text":""},{"location":"homeworks/week3/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f1\u06f2 \u0627\u0633\u0641\u0646\u062f (March 2) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week3/#solution","title":"Solution","text":"<p>HW3 Solution to Questions</p> <p>HW3 Solution Notebook | Part 1: REINFORCE vs GA</p> <p>HW3 Solution Notebook | Part 2: REINFORCE in CartPole</p> <p>HW3 Solution Notebook | Part 3: REINFORCE in Mountain Car</p> <p>HW3 Solution Notebook | Part 4: REINFORCE vs DQN</p>"},{"location":"homeworks/week4/","title":"HW4: Advanced Methods","text":"<p>Welcome to the forth homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and workshops.</p>"},{"location":"homeworks/week4/#homework","title":"Homework","text":"<p>You can download the homework questions and the notebook from the following links:</p> <p>HW4 Questions</p> <p>HW4 Notebook 1 HW4 Notebook 2</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week4/#explanation","title":"Explanation","text":""},{"location":"homeworks/week4/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f2\u06f4 \u0627\u0633\u0641\u0646\u062f (March 14) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week4/#solution","title":"Solution","text":"<p>HW4 Solution to Questions</p> <p>HW4 Solution Notebook | Part 1: PPO Continuous</p> <p>HW4 Solution Notebook | Part 2: SAC &amp; DDPG Continuous</p>"},{"location":"homeworks/week5/","title":"HW5: Model-Based Methods","text":"<p>Welcome to the fifth homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and workshops.</p>"},{"location":"homeworks/week5/#homework","title":"Homework","text":"<p>You can download the homework questions and the notebook from the following links:</p> <p>HW5 Questions</p> <p>HW5 Notebook 1 HW5 Notebook 2 HW5 Notebook 3</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week5/#explanation","title":"Explanation","text":""},{"location":"homeworks/week5/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f8 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646 (March 28) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week5/#solution","title":"Solution","text":"<p>HW5 Solution to Questions</p> <p>HW5 Solution Notebook | Part 1: MCTS</p> <p>HW5 Solution Notebook | Part 2: Dyna</p> <p>HW5 Solution Notebook | Part 3: MPC</p>"},{"location":"homeworks/week6/","title":"HW6: Multi-Armed Bandits","text":"<p>Welcome to the sixth homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and workshops.</p>"},{"location":"homeworks/week6/#homework","title":"Homework","text":"<p>You can download the homework questions and the notebook from the following links:</p> <p>HW6 Questions</p> <p>HW6 Notebook</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week6/#explanation","title":"Explanation","text":""},{"location":"homeworks/week6/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f1\u06f7 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646 (April 6) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week6/#solution","title":"Solution","text":"<p>HW6 Solution to Questions</p> <p>HW6 Solution Notebook</p>"},{"location":"homeworks/week7/","title":"HW7: Value-Based Theory","text":"<p>Welcome to the 7th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and recitation.</p>"},{"location":"homeworks/week7/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW7 Questions</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week7/#explanation","title":"Explanation","text":""},{"location":"homeworks/week7/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f2\u06f1 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a (May 11) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week8/","title":"HW8: Policy-Based Theory","text":"<p>Welcome to the 8th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and recitation.</p>"},{"location":"homeworks/week8/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW8 Questions</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week8/#explanation","title":"Explanation","text":""},{"location":"homeworks/week8/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f2\u06f1 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a (May 11) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week9/","title":"HW9: Advanced Theory","text":"<p>Welcome to the 9th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and recitation.</p>"},{"location":"homeworks/week9/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW9 Questions</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week9/#explanation","title":"Explanation","text":""},{"location":"homeworks/week9/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f1\u06f6 \u062e\u0631\u062f\u0627\u062f (June 6) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"journal_club/","title":"Introduction","text":""},{"location":"lectures/","title":"Introduction","text":""},{"location":"lectures/week1/","title":"Week 1: Introduction to RL","text":""},{"location":"lectures/week1/#lecture-1","title":"Lecture 1","text":""},{"location":"lectures/week1/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week1/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week1/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides</p>"},{"location":"lectures/week1/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week1/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week1/#lecture-2","title":"Lecture 2","text":""},{"location":"lectures/week1/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week1/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week1/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week1/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week1/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week10/","title":"Week 10: Exploration Methods","text":""},{"location":"lectures/week10/#lecture-19","title":"Lecture 19","text":""},{"location":"lectures/week10/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week10/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week10/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated) Download Slides (Extra)</p>"},{"location":"lectures/week10/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week10/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week10/#lecture-20","title":"Lecture 20","text":""},{"location":"lectures/week10/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week10/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week10/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated) Download Slides (Extra)</p>"},{"location":"lectures/week10/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week10/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week11/","title":"Week 11: Imitation &amp; Inverse RL","text":""},{"location":"lectures/week11/#lecture-21","title":"Lecture 21","text":""},{"location":"lectures/week11/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week11/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week11/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated) Download Slides (Extra)</p>"},{"location":"lectures/week11/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week11/#lecture-22","title":"Lecture 22","text":""},{"location":"lectures/week11/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week11/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week11/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week12/","title":"Week 12: Offline Methods","text":""},{"location":"lectures/week12/#lecture-23","title":"Lecture 23","text":""},{"location":"lectures/week12/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week12/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week12/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week12/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week12/#lecture-24","title":"Lecture 24","text":""},{"location":"lectures/week12/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week12/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week12/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated) Download Slides (Extra)</p>"},{"location":"lectures/week12/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week13/","title":"Week 13: Multi-Agent Methods","text":""},{"location":"lectures/week13/#lecture-25","title":"Lecture 25","text":""},{"location":"lectures/week13/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week13/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week13/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week13/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week13/#lecture-26","title":"Lecture 26","text":""},{"location":"lectures/week13/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week13/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week13/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week13/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week14/","title":"Week 14: Hierarchical &amp; Meta RL","text":""},{"location":"lectures/week14/#lecture-27","title":"Lecture 27","text":""},{"location":"lectures/week14/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week14/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week14/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated) Download Slides (Extra)</p>"},{"location":"lectures/week14/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week14/#lecture-28","title":"Lecture 28","text":""},{"location":"lectures/week14/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week14/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week14/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week14/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week15/","title":"Week 15: Guest Lectures","text":""},{"location":"lectures/week15/#lecture-29","title":"Lecture 29","text":""},{"location":"lectures/week15/#lecture-30","title":"Lecture 30","text":""},{"location":"lectures/week2/","title":"Week 2: Value-Based Methods","text":""},{"location":"lectures/week2/#lecture-3","title":"Lecture 3","text":""},{"location":"lectures/week2/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week2/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week2/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week2/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week2/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week2/#lecture-4","title":"Lecture 4","text":""},{"location":"lectures/week2/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week2/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week2/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week2/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week2/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week3/","title":"Week 3: Policy-Based Methods","text":""},{"location":"lectures/week3/#lecture-5","title":"Lecture 5","text":""},{"location":"lectures/week3/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week3/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week3/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week3/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week3/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week3/#lecture-6","title":"Lecture 6","text":""},{"location":"lectures/week3/#screen-camera_1","title":"Screen + Camera","text":"<p>This lecture was held online due to the university being closed.</p>"},{"location":"lectures/week3/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week3/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides</p>"},{"location":"lectures/week3/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week3/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week4/","title":"Week 4: Advanced Methods","text":""},{"location":"lectures/week4/#lecture-7","title":"Lecture 7","text":""},{"location":"lectures/week4/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week4/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week4/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week4/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week4/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week4/#lecture-8","title":"Lecture 8","text":""},{"location":"lectures/week4/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week4/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week4/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides</p>"},{"location":"lectures/week4/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week4/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week5/","title":"Week 5: Model-Based Methods","text":""},{"location":"lectures/week5/#lecture-9","title":"Lecture 9","text":""},{"location":"lectures/week5/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week5/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week5/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week5/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week5/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week5/#lecture-10","title":"Lecture 10","text":""},{"location":"lectures/week5/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week5/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week5/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides</p>"},{"location":"lectures/week5/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week5/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week6/","title":"Week 6: Multi-Armed Bandits","text":""},{"location":"lectures/week6/#lecture-11","title":"Lecture 11","text":""},{"location":"lectures/week6/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week6/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides</p>"},{"location":"lectures/week6/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week6/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week6/#lecture-12","title":"Lecture 12","text":""},{"location":"lectures/week6/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week6/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week6/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week6/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week7/","title":"Week 7: Value-Based Theory","text":""},{"location":"lectures/week7/#lecture-13","title":"Lecture 13","text":""},{"location":"lectures/week7/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week7/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week7/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated) Download Slides (Extra)</p>"},{"location":"lectures/week7/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week7/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week7/#lecture-14","title":"Lecture 14","text":""},{"location":"lectures/week7/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week7/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week7/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week7/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week8/","title":"Week 8: Policy-Based Theory","text":""},{"location":"lectures/week8/#lecture-15","title":"Lecture 15","text":""},{"location":"lectures/week8/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week8/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week8/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides</p>"},{"location":"lectures/week8/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week8/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week8/#lecture-16","title":"Lecture 16","text":""},{"location":"lectures/week8/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week8/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week8/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated) Download Slides (Extra)</p>"},{"location":"lectures/week8/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week8/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week9/","title":"Week 9: Advanced Theory","text":""},{"location":"lectures/week9/#lecture-17","title":"Lecture 17","text":""},{"location":"lectures/week9/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week9/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week9/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week9/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week9/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week9/#lecture-18","title":"Lecture 18","text":""},{"location":"lectures/week9/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week9/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week9/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week9/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"poster_session/","title":"Introduction","text":"<p>The purpose of the poster session is for you to dive into a reinforcement learning topic of your choice, enjoy the learning process, and share your enthusiasm with classmates and the wider community! This is your chance to explore something that excites you\u2014whether it's a cutting-edge algorithm, a unique application, or an open research question\u2014and present it in a way that sparks curiosity. So, have fun, keep experimenting, and let your passion shine through!</p>"},{"location":"poster_session/#research-topic-proposal","title":"Research Topic Proposal","text":"<p>Before you begin designing your poster, please submit a concise proposal outlining your chosen reinforcement learning topic. This ensures that all submissions maintain high-quality standards and helps you focus your research effectively.</p> <ul> <li>Template: Use the Proposal Template to structure your submission.  </li> <li>Submission: Upload your completed proposal to Quera (submit here) by June\u00a06.</li> </ul>"},{"location":"poster_session/#main-grading-components","title":"Main Grading Components","text":"<p>Your final grade will be determined by three core components: a video presentation, the poster design itself, and an in-person presentation.</p>"},{"location":"poster_session/#video-presentation-03-points","title":"Video Presentation (0.3\u00a0points)","text":"<p>Record a Full HD video (1920\u00d71080) presenting your final work to someone who has basic knowledge of reinforcement learning. Think of this as a mini-lecture or demo that summarizes your project:</p> <ul> <li>Submission: Upload your video to Google Drive and share the link on Quera (submit here) by June\u00a024.</li> <li>Objective: Provide a concise, engaging overview of your research so that someone can grasp the core ideas in under 12\u00a0minutes.</li> </ul>"},{"location":"poster_session/#full-marks-criteria","title":"Full Marks Criteria","text":"<ul> <li>Clear Motivation &amp; Context:   Start with \u201cWhy does this topic matter?\u201d Give a brief intro to the problem you tackled.</li> <li>Well-Structured Flow: <ol> <li>Introduction (30\u201360\u00a0seconds): State the problem and why it's interesting.  </li> <li>Main Idea / Methods (4\u20136\u00a0minutes): Describe your approach, algorithmic insights, or experimental setup.  </li> <li>Results &amp; Discussion (2\u20133\u00a0minutes): Highlight key findings\u2014charts, figures, or demo clips.  </li> <li>Conclusion &amp; Future Work (1\u20132\u00a0minutes): Summarize takeaways and possible next steps.</li> </ol> </li> <li>Strong Visuals: <ul> <li>Use slides or screen-sharing to show your poster, code snippets, plots, or demos.  </li> <li>Keep text minimal\u2014focus on diagrams, animations, or short bullet points.  </li> </ul> </li> <li>Time Management: <ul> <li>Aim for 8\u201312\u00a0minutes total. Practice so you don't rush or run overtime.</li> </ul> </li> <li>Engaging Delivery: <ul> <li>Be audible and clear\u2014consider using a good microphone or quiet environment.  </li> <li>Maintain an energetic pace; avoid monotone speaking.  </li> <li>Make eye contact (if on camera) and use natural gestures to keep viewers engaged.</li> </ul> </li> </ul> <p>Note: Since your final video will be featured on our YouTube channel, begin with an introductory slide that prominently displays your name, topic, and affiliation. Also, include your online contact details (e.g., website, social media profiles) in the text file containing your video link, so that we can share them under your video presentation and viewers can easily connect with you.</p>"},{"location":"poster_session/#poster-design-03-points","title":"Poster Design (0.3\u00a0points)","text":"<p>Create a Vertical A1 poster (594\u202fmm\u202f\u00d7\u202f841\u202fmm) that visually communicates your project. You can submit either: A vector file (PDF, SVG, etc.), or A high-resolution raster (PNG, TIF) at \u2265\u202f300\u202fDPI.</p> <ul> <li>Submission: Upload your final poster to Google Drive and share the link on Quera (submit here) by June\u00a024.</li> <li>Objective: Use layout, color, and concise text to guide viewers through your research question, methods, and results.</li> </ul>"},{"location":"poster_session/#full-marks-criteria_1","title":"Full Marks Criteria","text":"<ol> <li>Logical Layout: <ul> <li>Divide the poster into clear sections: Title / Authors, Introduction, Methods, Results, Discussion, References.  </li> <li>Follow a top-to-bottom or left-to-right flow so readers can scan easily.</li> </ul> </li> <li>Readable Typography</li> <li>Visual Clarity: <ul> <li>Include diagrams, charts, or screenshots that directly support your narrative.  </li> <li>Keep text boxes and figures aligned and balanced\u2014avoid overcrowding.</li> </ul> </li> <li>Clear Research Question &amp; Motivation: <ul> <li>At the very top or upper-left, state \u201cWhat question are we asking?\u201d in a few sentences.</li> </ul> </li> <li>Concise Methods &amp; Results: <ul> <li>Use bullet points, flowcharts, or algorithm diagrams to explain your approach.  </li> <li>Present key findings using labeled graphs or tables\u2014annotate them so viewers understand at a glance.</li> </ul> </li> <li>Proper Citations &amp; References: <ul> <li>Include citations in a smaller font at the bottom.  </li> <li>Use a consistent citation style (MLA).</li> </ul> </li> <li>Polish &amp; Professionalism: <ul> <li>Check for spelling/grammar errors (proofread!).  </li> <li>Choose a clean color palette (2\u20133 complementary colors).  </li> <li>Leave enough white space so nothing feels cramped.</li> </ul> </li> </ol> <p>Example Posters: You can view last year's submissions here for inspiration.</p>"},{"location":"poster_session/#in-person-presentation-04-points","title":"In-Person Presentation (0.4\u00a0points)","text":"<p>On June\u202f26 or 27, you will stand beside your poster at the poster session, discuss your work with judges and peers, and answer their questions live.</p> <ul> <li>Objective: Demonstrate your deep understanding of the topic and communicate it effectively in a face-to-face setting.</li> </ul>"},{"location":"poster_session/#full-marks-criteria_2","title":"Full Marks Criteria","text":"<ol> <li>Confident Explanation: <ul> <li>Greet viewers with a brief elevator pitch (15\u201320\u00a0seconds): \u201cHi, I'm\u00a0[Name], and this is my project on\u00a0[Topic]\u2014we explore [core idea] because\u00a0[why it matters].\u201d</li> <li>Walk through the poster, pointing at key sections. Avoid reading text verbatim.</li> </ul> </li> <li>Technical Q&amp;A: <ul> <li>Anticipate common questions:  <ul> <li>\u201cWhy did you compare these two algorithms?\u201d  </li> <li>\u201cWhat are the limitations of your approach?\u201d  </li> </ul> </li> <li>If you don't know an answer, it's OK to say, \u201cThat's a good point\u2014here's how I'd investigate it next.\u201d</li> </ul> </li> <li>Demonstrated RL Foundations: <ul> <li>Show that you understand the underlying RL concepts (e.g., Markov Decision Processes, reward design, exploration vs. exploitation).  </li> <li>Be ready to discuss alternatives or extensions.</li> </ul> </li> <li>Engaging the Audience: <ul> <li>Make eye contact, ask rhetorical questions (\u201cImagine training an agent that\u2026\u201d).  </li> <li>Show enthusiasm\u2014smile and be approachable.</li> </ul> </li> <li>Time Management: <ul> <li>Each visitor may only have a few minutes\u2014focus on the big picture first, then dive deeper if they ask.</li> <li>Keep your overall \u201ctour\u201d of the poster to about 10\u00a0minutes, then invite questions.</li> </ul> </li> </ol> <p>Note: Your final score and rank in the competition will be a weighted combination of judges' evaluations and audience feedback.</p>"},{"location":"poster_session/#bonus-grading-components","title":"Bonus Grading Components","text":"<p>If you want to go above and beyond, there are three ways to earn extra points. These are optional but highly encouraged if you enjoy pushing the boundaries of scientific communication.</p>"},{"location":"poster_session/#1-video-presentation-in-english-03-points","title":"1. Video Presentation in English (0.3\u00a0points)","text":"<p>Submit an English-language video of your presentation (same format requirements as above) to demonstrate your ability to communicate RL concepts to a global audience.</p> <ul> <li>Submission: Along with your main video, upload an English version to Google Drive and share the link on Quera by June\u00a024.</li> <li>Objective: Practice scientific English and make your work accessible to non-Persian speakers.</li> </ul>"},{"location":"poster_session/#full-marks-criteria_3","title":"Full Marks Criteria","text":"<ul> <li>Entirely in English: No switching back to another language.  </li> <li>Include Presenter Video: We want to see you speaking (face and voice) while you present your slides or poster.  </li> <li>Clear Pronunciation: Listeners should understand the technical content without major effort.  </li> <li>Fluency &amp; Flow: Avoid lengthy pauses\u2014speak at a natural pace suitable for international viewers.</li> </ul>"},{"location":"poster_session/#2-blog-post-in-rl-journal-club-04-points","title":"2. Blog Post in RL Journal Club (0.4\u00a0points)","text":"<p>Write a blog post about your project for our RL Journal Club blog.</p> <ul> <li>Objective: Publish a polished article that contributes to the broader RL community.</li> <li>Deadline: June\u00a024.</li> </ul>"},{"location":"poster_session/#brief-guideline","title":"Brief Guideline","text":"<ol> <li>Fork the Repository: <ul> <li>Go to the RLJ Club GitHub and click Fork to create your own copy.</li> </ul> </li> <li>Set Up Locally: <ul> <li>Follow instructions for Hugo and the hugo-PaperMod theme.  </li> <li>Make sure you can build the site on your machine.</li> </ul> </li> <li>Write Your Article: <ul> <li>Look at existing posts to understand style and structure.  </li> <li>Write in a clear, engaging tone\u2014imagine you're explaining to someone who is familiar with RL but not an expert in your niche.</li> </ul> </li> <li>Create a Pull Request: <ul> <li>Commit your new Markdown file, push to your fork, and open a Pull Request against the main repo.  </li> <li>Include a brief description (\u201cMy poster session blog post on\u00a0[Topic]\u201d).</li> </ul> </li> </ol>"},{"location":"poster_session/#full-marks-criteria_4","title":"Full Marks Criteria","text":"<ul> <li>Clarity &amp; Coherence: The narrative flows logically; even a non-specialist can follow.  </li> <li>Comprehensive Coverage: Motivation, methods, and key insights are all well explained.  </li> <li>Visuals: At least one figure (poster snippet, plot, or diagram) to illustrate your points.  </li> <li>Professional Polish: Proofread for grammar, spelling, and formatting. Use consistent Markdown conventions.</li> </ul>"},{"location":"poster_session/#3-extra-bonus-competition-03-points","title":"3. Extra Bonus &amp; Competition (\u2265\u202f0.3\u00a0points)","text":"<p>Surprise us! If you find a creative way to elevate your project beyond the requirements, you could earn up to 0.3 extra points (or more) depending on impact and originality.</p> <ul> <li>Objective: Reward exceptional creativity, depth, or extra initiative.</li> </ul>"},{"location":"poster_session/#possible-examples","title":"Possible Examples:","text":"<ul> <li>Top Presentation Award: Judges may select the 3 best presentations in the event\u2014earn additional points if you rank among them.  </li> <li>Novel Experiments / Demos: Implement a brand-new experiment, create a live demo environment, or show off an impressive visualization.  </li> <li>Broad Comparisons: Compare multiple RL papers, algorithms, or frameworks in the same study\u2014demonstrate deeper analysis.  </li> <li>Custom Environment / Benchmark: Design an original environment (e.g., a small game or simulation) and benchmark different RL agents on it.  </li> <li>Interactive Poster: Embed QR codes, short videos, or interactive web demos that viewers can explore on their phones.  </li> <li>Real-World Application: Showcase how DRL can solve a real problem\u2014robotics, finance, games, or any practical domain.  </li> <li>Educational Material: Develop a mini-tutorial, record a short lecture series, or create a well-documented GitHub repo with code and clear README.</li> </ul> <p>Challenge yourself\u2014we love being surprised! If you're unsure whether an idea qualifies, ask early so we can give feedback.</p>"},{"location":"poster_session/#summary-approximate-deadlines","title":"Summary &amp; Approximate Deadlines","text":"<p>Below is a quick reference for key milestones and their weight toward your final grade:</p> Component Points Approximate Deadline Research Proposal 0 June\u00a06, 2025 (11:59\u202fPM) Video Presentation 0.3 June\u00a024, 2025 (11:59\u202fPM) Poster Design 0.3 June\u00a024, 2025 (11:59\u202fPM) In-Person Presentation 0.4 June\u00a026\u201327, 2025 (Event) Bonus\u00a01: English Video Presentation 0.3 June\u00a024, 2025 (11:59\u202fPM) Bonus\u00a02: Blog Post (RL Journal Club) 0.4 June\u00a024, 2025 (11:59\u202fPM) Bonus\u00a03: Extra Creativity 0.3+ June\u00a026\u201327, 2025 (Event) <p>We can't wait to see what you come up with. Let's make this poster session informative, fun, and inspiring for everyone involved\u2014happy researching and good luck!</p>"},{"location":"poster_session/sp24/","title":"Spring 2024","text":"<ul> <li> <p> Advancements in Offline Reinforcement Learning </p> </li> <li> <p> Automated Reinforcement Learning (AutoRL) </p> </li> <li> <p> Causal Bandits </p> </li> <li> <p> Combinatorial Causal Bandits </p> </li> <li> <p> Diffusion+RL </p> </li> <li> <p> Explainable Reinforcement Learning </p> </li> <li> <p> Goal-Conditioned Reinforcement Learning </p> </li> <li> <p> Hierarchical Reinforcement Learning </p> </li> <li> <p> Introduction to RL-based Recommender Systems </p> </li> <li> <p> Learning to Communication in MARL </p> </li> <li> <p> LLM RL Alignment </p> </li> <li> <p> Meta Reinforcement Learning </p> </li> <li> <p> Modular Reinforcement Learning </p> </li> <li> <p> Multi-Task Model-Based Reinforcement Learning </p> </li> <li> <p> Non-Stationary Reinforcement Learning </p> </li> <li> <p> Towards Robust and Safe Reinforcement Learning </p> </li> <li> <p> Unsupervised Reinforcement Learning </p> </li> </ul>"},{"location":"prerequisites/","title":"Introduction","text":""},{"location":"prerequisites/deep_learning/","title":"Deep Learning","text":""},{"location":"prerequisites/game_theory/","title":"Game Theory","text":""},{"location":"prerequisites/information_theory/","title":"Information Theory","text":""},{"location":"prerequisites/linear_algebra/","title":"Linear Algebra","text":""},{"location":"prerequisites/numerical_optimization/","title":"Numerical Optimization","text":""},{"location":"prerequisites/stochastic_processes/","title":"Stochastic Process","text":""},{"location":"recitations/","title":"Introduction","text":""},{"location":"recitations/week1/","title":"Week 1: Introduction to RL","text":""},{"location":"recitations/week1/#screen-record","title":"Screen Record","text":""},{"location":"recitations/week1/#recitation-slides","title":"Recitation Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"recitations/week10/","title":"Week 10: Exploration Methods","text":""},{"location":"recitations/week11/","title":"Week 11: Imitation &amp; Inverse RL","text":""},{"location":"recitations/week12/","title":"Week 12: Offline Methods","text":""},{"location":"recitations/week13/","title":"Week 13: Multi-Agent Methods","text":""},{"location":"recitations/week14/","title":"Week 14: Hierarchical &amp; Meta RL","text":""},{"location":"recitations/week2/","title":"Week 2: Value-Based Methods","text":""},{"location":"recitations/week2/#screen-record","title":"Screen Record","text":""},{"location":"recitations/week2/#recitation-slides","title":"Recitation Slides","text":"<p>Download Slides</p>"},{"location":"recitations/week2/#recitation-notes","title":"Recitation Notes","text":"<p>In this Recitation we will have a brief recap of What you've learnt so far. Let's dive in.</p>"},{"location":"recitations/week2/#contents","title":"Contents","text":"<ul> <li>Graphical Models</li> <li>Morkov Decision Process(MDP) and Markov Property</li> <li>Visualizing MDPs using Graphical Models</li> <li>Return(Sum of Rewards)</li> <li>State Value Function</li> <li>State-Action Value Function</li> <li>Policy Iteration</li> <li>Generalized Policy Iteration</li> <li>Value Iteration</li> <li>Exploration vs Exploitation</li> <li>ON Policy &amp; OFF policy</li> <li>Sampling Methods</li> <li>Monte Carlo</li> <li>Sampling &amp; Bootstrapping Methods</li> <li>N-Step TD</li> </ul>"},{"location":"recitations/week2/#graphical-models","title":"Graphical Models","text":"<ul> <li> <p>Basically graphical model, also known as GM is a very usefull tool to Represent a graphical form of probabilistic relations betwen Random variables.     Simply they are nothing but nodes and edges that are related through some probablity. Assume we have 3 random variable: \\(x_1,x_2,x_3\\) with the joint     probability of \\(P(x_1,x_2,x_3)\\). Their GM will look like this:</p> <p></p> <p>Where the Nodes are random varibales and edges define the relation between them.</p> <p>But How is this actually drawn? Well Let's do it step by step.</p> <p>Step 1: Draw the Random variables as Nodes.</p> <p></p> <p>Step 2: Factorize  \\(P(x_1,x_2,x_3)\\).</p> <p>\\(P(x_1,x_2,x_3) = \ud835\udc43(\ud835\udc65_1 |\ud835\udc65_2,\ud835\udc65_3 )\ud835\udc43(\ud835\udc65_2,\ud835\udc65_3 )=\ud835\udc43(\ud835\udc65_1 |\ud835\udc65_2,\ud835\udc65_3 )\ud835\udc43(\ud835\udc65_2\u2502\ud835\udc65_3 )\ud835\udc43(\ud835\udc65_3)\\)</p> <p>Step 3: From now we will draw the graph based on the factorized probabilty.</p> <p>\\(\ud835\udc43(\ud835\udc65_1 |\ud835\udc65_2,\ud835\udc65_3 )\\) means that \\(x_1\\) conditioned on both \\(\ud835\udc65_2,\ud835\udc65_3\\). to show that we draw an edge from both \\(\ud835\udc65_2,\ud835\udc65_3\\) to \\(\ud835\udc65_1\\).</p> <p></p> <p>Step 4:</p> <p>\\(\ud835\udc43(\ud835\udc65_2\u2502\ud835\udc65_3 )\\) means that \\(x_2\\) is conditioned on \\(x_3\\) so draw an edge from \\(x_3\\) to \\(x_2\\).</p> <p></p> <p>congratulation. Just like that We draw the GM of \\(P(x_1,x_2,x_3)\\). But we're not done yet.</p> <p>What if \\(x_1\\) is independent of \\(x_2\\) given \\(x_3\\)? What does that lead to?</p> <p>the independence  \\(x_1\\) and  \\(x_2\\) given  \\(x_3\\) means that \\(\ud835\udc43(\ud835\udc65_1 |\ud835\udc65_2) = 0\\), since  \\(x_1\\) and  \\(x_2\\) are seperated from the beginning </p> <p>their conditional probabilty is zero. to express that on the graph, we just erase the edge, indicating their relation, meaning that they don't have anything to do with each other! So we have this:</p> <p></p> </li> </ul>"},{"location":"recitations/week2/#morkov-decision-process-mdp-and-markov-property","title":"Morkov Decision Process (MDP) and Markov Property","text":"<ul> <li>We model our Reinforcement Learning Problems with MDPs.variable</li> <li> <p>To do that we divide our problem into some random variable, a simplified MDP consists of State, Action, Next State and Reward.   our Agents current situation or location is called State, at each state agent takes a certain action.   taking the action, agent moves to a new state, which we call Next state, Then receive a Reward based on how good and appropriate action and next state are.</p> </li> <li> <p>An MDP has a key property called Markov Property, Stating that an agent's Next state Only Depends on its Current State and not anything else.</p> </li> </ul> <p>Why did I menitioned it at all? as I said we model RL tasks using MDP so it's worthy to know how to draw their GM.</p>"},{"location":"recitations/week2/#visualizing-mdps-using-graphical-models","title":"Visualizing MDPs using Graphical Models","text":"<ul> <li>Let's say We have an agent. it's spawned at a state denoted as \\(s_0\\), takes the action \\(a_0\\), and moves to \\(s_1\\), then takes the action \\(a_1\\) and goes to \\(s_2\\). Wait a minute!! what did happen to Reward? Well receiving a reward is may or may not happed but the point is that here we assumed reward as a deterministic variable not random (doesn\u2019t have a probabilistic distribution) </li> </ul> <p>the joint Probability and factorized form: \\(\ud835\udc43(\ud835\udc60_0,\ud835\udc4e_0,\ud835\udc60_1,\ud835\udc4e_1,\ud835\udc60_2 )=\ud835\udc43(\ud835\udc60_2| \ud835\udc60_0,\ud835\udc4e_0,\ud835\udc60_1,\ud835\udc4e_1)\ud835\udc43(\ud835\udc4e_1\u2502\ud835\udc60_0,\ud835\udc4e_0,\ud835\udc60_1 )\ud835\udc43(\ud835\udc60_1\u2502\ud835\udc60_0,\ud835\udc4e_0 )\ud835\udc43(\ud835\udc4e_0 |\ud835\udc60_0)P(\ud835\udc60_0)\\)</p> <p>As I mentioned befor, MDP has Morkov property which means I can rewrite some of the terms: \\(s_2\\) only depends on \\(s_1\\) and \\(a_1\\) and not \\(s_0\\) and \\(a_0\\) due to the fact that they belong to the past. Thus we have, \\(\ud835\udc43(\ud835\udc60_2|\ud835\udc60_0,\ud835\udc4e_0,\ud835\udc60_1,\ud835\udc4e_1) = \ud835\udc43(\ud835\udc60_2 | \ud835\udc60_1,\ud835\udc4e_1)\\), Also in \\(\ud835\udc43(\ud835\udc4e_1\u2502\ud835\udc60_0,\ud835\udc4e_0,\ud835\udc60_1)\\), \\(a_1\\) only depends on the current state \\(s_1\\) and not \\(\ud835\udc60_0, \ud835\udc4e_0\\), again  we can write: \\(\ud835\udc43(\ud835\udc4e_1\u2502\ud835\udc60_0,\ud835\udc4e_0,\ud835\udc60_1 )=\ud835\udc43(\ud835\udc4e_1\u2502 \ud835\udc60_1 )\\)</p> <p>after doing the simplifications we get:</p> <p></p>"},{"location":"recitations/week2/#return-sum-of-rewards","title":"Return (Sum of Rewards)","text":"<p>As indicated in the headline Return means sum of rewards over an episode. in mathematic terms:</p> <p>\\(R(\\tau) = r_1 + \\gamma^{1}r_2 + \\gamma^{2}r_3 + ... + \\gamma^{T-1}r_{T}\\) where \\(\\tau\\) shows a whole trajectory starting from t=0 and T time step, episode ends.</p> <p>and if we aim to show the return blongs to certain a time step, then we have:</p> <p>\\(R_t = r_{t+1} + \\gamma^{1}r_{t+2} + \\gamma^{2}r_{t+3} + ... + \\gamma^{t+T-1}r_{t+T}\\)</p> <p>Another notation for \\(R(\\tau)\\) is \\(G\\) and for \\(R_t\\) is \\(G_t\\)</p> <p>We can also calculate the return in a recursive form:</p> <p>\\(G_t = r_{t+1} + \\gamma^{1}r_{t+2} + \\gamma^{2}r_{t+3} + \\gamma^{3}r_{t+4} +... + \\gamma^{t+T-1}r_{t+T}=\\) \\(r_{t+1} + \\gamma[r_{t+2} + \\gamma^{1}r_{t+3} + \\gamma^{2}r_{t+4} +... ]=\\)</p> <p>\\(r_{t+1} + \\gamma \\cdot G_{t+1}\\)</p>"},{"location":"recitations/week2/#state-value-function","title":"State Value Function","text":"<ul> <li>State value function is a measurement, indicating How good a state is among other states in an environment.   You may ask How can a state be better or worse than the others? and How do we quanitize it?   if you look at the following figure, you can see me on a frozen lake which is divided into cells.</li> </ul> <p>the black cells have weak ice, so if I step on them, they break and I die. as you know ice is slippery, so I might slip any moment and go into the wrong direction!! I am litterally Stuck, and let's say I can't go out unless a Helicopter helps me. Considering this fact, if we assume the best cell is the safest. Which one is the best and which one is the worst cell?</p> <p>Obviously the cell \"C\" is the safest because even if I slip, I still have one more chance. But F and D are the worst, I have 3 cells around me, but 2 of them have weak ice. So Any mistake can get me killed. </p> <p>Fair enough. But how do we actually measure these State values? This formula helps us.  </p> <p>What does it say? the state value (a.k.a value) of a state is equal to expectation of the Returns w.r.t the taken Policy.</p> <p>Example: In State A, With the probability of 0.8 we go right and 0.2 we go down. What is the value of state A?</p> <p> </p> <p>In this trajectory assuming \\(\\gamma = 1\\) the Return is as follow: </p> <p>\\(R(\\tau_1) = 1 + 1 + 1 + 1 = 4\\)</p> <p>and in another one we got:</p> <p> </p> <p>\\(R(\\tau_2) = -1 + 1 + 1 + 1 = 2\\)</p> <p>Now Let's Calculate the V(A)</p> <p> </p> <p>But how did we those trajectories and rewards? simple we actually took those actions, whether you're running a simulation or it's an actuall prototype, you have to  try the trajectories. The more trajectory get tested the more percise values.</p> <ul> <li> <p>Heads up: </p> <p>1-The calculated value is not necesserily fully percise, We only tested 2 trajectories, for a fully refined value, we need to try alot more trajectories.</p> <p>2-There were so much simplification in this example, don't get confused if you faced a lot more calculation claiming to be value function. </p> </li> </ul>"},{"location":"recitations/week2/#state-action-value-function","title":"State-Action Value Function","text":"<p>We talked about how to measure a state value, but let's be honest, that's not enough, knowing which state has higher value dosn't necesserily help us find our way. even in the worst situation there might be an action, helping us to success. so we move one step forward and calculate  State-Action Values (a.k.a Q Values), this time we evaluate how good a certain action is, in a certain State.</p> <p>\\(Q(s,a) = \\mathbb{E}_{\\tau \\sim \\pi} [R(\\tau) \\mid S = s, A = a]\\)</p> <p>Looking at the equation, you realize there is a condition on both s and a meaning that, when taking the expectation the first state and actions are fixed but after taking them, feel free to do anything according to Policy \\(\\pi\\), the expectation will cover for you. you can see an example in the next figure.</p> <p> </p> <p>The Agent starts from the Start Cell and takes a fixed action to the Right but after that takes actions according to its Policy.</p> <ul> <li> <p>Heads up: </p> <p>1-The calculated Q Value using this trajectories is not necesserily fully percise, We only tested few trajectories, for a fully refined state-action value, we need to try alot more trajectories.</p> <p>2-There were so much simplification in this example, don't get confused if you faced a lot more complex interpretations and equations, claiming to be state-action value function. </p> <p>3- The same strategy could be used for any other state-action pair.</p> </li> </ul> <p>Summary</p> <p></p> <p> </p>"},{"location":"recitations/week2/#bellman-equations","title":"Bellman equations","text":""},{"location":"recitations/week2/#state-value-function_1","title":"State Value Function","text":"<p>In a Very simplified situation, where The Environment is not Stochastic and Agent's Policy is deterministic, We can Write the Value function as follows:</p> <p>\\(\ud835\udc49(\ud835\udc60)=\ud835\udc45(\ud835\udc60,\ud835\udc4e,\ud835\udc60^\u2032)+\\gamma \\cdot\ud835\udc49(\ud835\udc60^\u2032)\\) Which is Called Bellman Equation for V.</p> <p>For example for the following MDP:</p> <p> </p> <p>Here instead of assigning a specific node to reward we wrote it on the edge.</p> <p>We have: \\(\ud835\udc49(s_2)=\ud835\udc45(\ud835\udc60_2,\ud835\udc4e_2,\ud835\udc60_3)+\\gamma \\cdot\ud835\udc49(\ud835\udc60_3) \\rightarrow \ud835\udc49(s_2)=r_2 +\\gamma \\cdot\ud835\udc49(\ud835\udc60_3)\\)</p> <p>But as I mentioned this is a very simplified MDP, But I guess it's a great time to add some stochasticity to the Environment!</p> <p>When the Environment is stochastic, there is always a chance that you miss even if you're policy is clear like day light to you.</p> <p>for example consider the frozen lake I got stuck in.</p> <p> </p> <p>when I am at state D the obvious choice is to stay where I am or move to Left. But the lake surface is slippery, Despite my intention to move to the left.</p> <p>I may slip and go up or down! this is the Stochacity I'm talking about. in Probabilty We model it as \\(\ud835\udc43(s'|s,a)\\)</p> <p>If the environment is stochastic, we must take account for it. to make that happen We add an Expectaion to the previous Equation.</p> <p>\\(\ud835\udc49^\ud835\udf0b (\ud835\udc60)=\\sum_{\ud835\udc60'}\ud835\udc43(\ud835\udc60' |\ud835\udc60,\ud835\udc4e)[\ud835\udc45(\ud835\udc60,\ud835\udc4e,\ud835\udc60^\u2032 )+\\gamma \\cdot \ud835\udc49^\ud835\udf0b (\ud835\udc60^\u2032 )]\\)</p> <p>In the frozen lake, Assume I only can move in the direction of the intended action, or the sides but I never go backward.  For example Let's Say I am at state A and want to go downward. what is the value of A?</p> <p>\\(\ud835\udc49^\ud835\udf0b (\ud835\udc60)=\ud835\udc43(Start |A,down)[\ud835\udc45(A,down,Start )+\\gamma \\cdot \ud835\udc49^\ud835\udf0b (Start)]\\) \\(+ \ud835\udc43(C |A,down)[\ud835\udc45(A,down,C )+\\gamma \\cdot \ud835\udc49^\ud835\udf0b (C)]\\) \\(+ \ud835\udc43(Black |A,down)[\ud835\udc45(A,down,Black )+\\gamma \\cdot \ud835\udc49^\ud835\udf0b (Black)]\\)</p> <p>Another stochasticity we should be aware of, is Stochasticity of Policy. A policy is not always clear and deterministic, sometimes it's drawn from a probabilistic distribution. We model this as P(a|s). But instead of P notation, it's denoted \\(\\pi(a|s)\\), meaning that given a certain state, there is even small possiblilty for each action to be taken.</p> <p>To have the Bellman equation to account for \\(\\pi(a|s)\\), We rewrite it as follows:</p> <p>\\(\ud835\udc49^\ud835\udf0b (\ud835\udc60)=\\sum_{a}\\pi(a|s)\\sum_{\ud835\udc60'}\ud835\udc43(\ud835\udc60' |\ud835\udc60,\ud835\udc4e)[\ud835\udc45(\ud835\udc60,\ud835\udc4e,\ud835\udc60^\u2032 )+\\gamma \\cdot \ud835\udc49^\ud835\udf0b (\ud835\udc60^\u2032 )]\\)</p> <p>As a suppliment to the previous Example, assume at state A, the moves downward with probability of 0.5 and goes right and leftward, each with probability of 0.25 what is V(A)?</p> <p>Here s = A is fixed but s' and a are random.</p> <p>\\(\ud835\udc49^\ud835\udf0b (\ud835\udc60)=\\pi(down|A)[\ud835\udc43(Start |A,down)[\ud835\udc45(A,down,Start )+\\gamma \\cdot \ud835\udc49^\ud835\udf0b (Start)]\\) \\(+ \ud835\udc43(C |A,down)[\ud835\udc45(A,down,C )+\\gamma \\cdot \ud835\udc49^\ud835\udf0b (C)]\\) \\(+ \ud835\udc43(Black |A,down)[\ud835\udc45(A,down,Black )+\\gamma \\cdot \ud835\udc49^\ud835\udf0b (Black)]]\\) \\(+ \\pi(Right|A)[\ud835\udc43(Start |A,Right)[\ud835\udc45(A,Right,Start )+\\gamma \\cdot \ud835\udc49^\ud835\udf0b (Start)]\\) \\(+ \ud835\udc43(C |A,Right)[\ud835\udc45(A,Right,C )+\\gamma \\cdot \ud835\udc49^\ud835\udf0b (C)]\\) \\(+ \ud835\udc43(Black |A,Right)[\ud835\udc45(A,Right,Black )+\\gamma \\cdot \ud835\udc49^\ud835\udf0b (Black)]]\\) \\(+ \\pi(Left|A)[\ud835\udc43(Start |A,Left)[\ud835\udc45(A,Left,Start )+\\gamma \\cdot \ud835\udc49^\ud835\udf0b (Start)]\\) \\(+ \ud835\udc43(C |A,Left)[\ud835\udc45(A,Left,C )+\\gamma \\cdot \ud835\udc49^\ud835\udf0b (C)]\\) \\(+ \ud835\udc43(Black |A,Left)[\ud835\udc45(A,Left,Black )+\\gamma \\cdot \ud835\udc49^\ud835\udf0b (Black)]]\\)</p> <p>Okay We are done here.</p>"},{"location":"recitations/week2/#state-action-function","title":"State-Action Function","text":"<p>In a Very simplified situation, where The Environment is not Stochastic and Agent's Policy is deterministic, We can Write the Value function as follows:</p> <p>\\(Q(\ud835\udc60,a)=\ud835\udc45(\ud835\udc60,\ud835\udc4e,\ud835\udc60^\u2032)+\\gamma \\cdot Q(\ud835\udc60^\u2032,a')\\) Which is Called Bellman Equation for Q.</p> <p>For example for the following MDP:</p> <p> </p> <p>\\(Q(\ud835\udc60 = s_1,a=a_1)=\ud835\udc45(\ud835\udc60_1,\ud835\udc4e_1,\ud835\udc60_2)+\\gamma \\cdot Q(\ud835\udc60_1,a2) \\rightarrow r_1 +\\gamma \\cdot Q(\ud835\udc60_1,a2)\\)</p> <p>Like before, now we add Environment stochasticity to this equation:</p> <p>\\(Q^\ud835\udf0b (\ud835\udc60,a)=\\sum_{\ud835\udc60'}\ud835\udc43(\ud835\udc60' |\ud835\udc60,\ud835\udc4e)[\ud835\udc45(\ud835\udc60,\ud835\udc4e,\ud835\udc60^\u2032 )+\\gamma \\cdot Q^\ud835\udf0b (\ud835\udc60^\u2032,a^\u2032 )]\\)</p> <p>And finally we include the policy stochasticity:</p> <p>\\(Q^\ud835\udf0b (\ud835\udc60,a)=\\sum_{a}\\pi(a|s)\\sum_{\ud835\udc60'}\ud835\udc43(\ud835\udc60' |\ud835\udc60,\ud835\udc4e)[\ud835\udc45(\ud835\udc60,\ud835\udc4e,\ud835\udc60^\u2032 )+\\gamma \\cdot Q^\ud835\udf0b (\ud835\udc60^\u2032,a^\u2032)]\\) But this Equation is Wrong!!!. Why is that so?</p> <p>because as we discussed when you are talking about Q function the first state and action pair are fixed, thus taking expectation w.r.t a is a big mistake.</p> <p>But we still have to take another expectation. because, even though the first state-action pair is fixed but (\ud835\udc60<sup>\u2032,a</sup>\u2032) pair is still stochastic so we have:</p> <p>\\(Q^\ud835\udf0b (\ud835\udc60,a)=\\sum_{\ud835\udc60'}\ud835\udc43(\ud835\udc60' |\ud835\udc60,\ud835\udc4e)[\ud835\udc45(\ud835\udc60,\ud835\udc4e,\ud835\udc60^\u2032 )+\\gamma \\cdot \\sum_{a}\\pi(a^\u2032|\ud835\udc60^\u2032)Q^\ud835\udf0b (\ud835\udc60^\u2032,a^\u2032)]\\), also we can simplify  \\(\\sum_{a}\\pi(a^\u2032|\ud835\udc60^\u2032)Q^\ud835\udf0b (\ud835\udc60^\u2032,a^\u2032) = v(\ud835\udc60^\u2032)\\)</p> <p>in conclusion the Q couls be written as \\(Q^\ud835\udf0b (\ud835\udc60,a)=\\sum_{\ud835\udc60'}\ud835\udc43(\ud835\udc60' |\ud835\udc60,\ud835\udc4e)[\ud835\udc45(\ud835\udc60,\ud835\udc4e,\ud835\udc60^\u2032 )+\\gamma \\cdot v(\ud835\udc60^\u2032)]\\)</p>"},{"location":"recitations/week2/#policy-iteration","title":"Policy Iteration","text":"<p>In this section we're going to discuss about policy iteration algorithm which is somehow the underneath of evey RL algorithm.</p> <p></p> <p>This indicates that policy iteration consists of two procedure. \\(1^{st}\\) Policy evaluation and \\(2^{nd}\\) Policy Improvement. The process is as follows:</p> <p>1- Initialize a Random policy.</p> <p>2- Evelate the initialized policy till values are converged. (Policy Evaluation) </p> <p>3- Use the calculated values in step 2, to improve the policy.</p> <ul> <li>Repeat steps 2 and 3 untill the policy remains unchanged or in other words, the optimal policy is acquired. </li> </ul> <p>Policy Evaluation</p> <p>We somehow explaned policy evaluation in previous sections, it's just calculating state values, given a certain policy. Let's have a numerical example:</p> <p>SWF Environment Intrudoction</p> <p> </p> <p>That's a very typical Environment for classic RL. Agent can take only Left and Rightward actions at each state and as it's shown in the picture, with 50 percent chance the intended action is done successfully, with 33.33 percent agent stays where ir was and with 16.66 percent it goes backward. the agent receive a (+1) reward only when reaches the goal.</p> <p>The random policy We are going to take is :</p> <p> </p> <p>Which actually is the worst policy for our case.</p> <p>One question, without doing any calculation, judjing by the look, Which state value gets updated first? and why?</p> <p>The answer is State 5! but why? It's the ony state thet receive reward at the first place.</p> <p>now let's do some calculation.</p> <p> </p> <p>Now for State 4 in iteration 1: </p> <p>\\(\ud835\udc49(4)=\ud835\udc43(\ud835\udc60^\u2032=3,\ud835\udc4e=\ud835\udc3f)[\ud835\udc5f+\ud835\udc49(3)]+\ud835\udc43(\ud835\udc60^\u2032=4,\ud835\udc4e=\ud835\udc3f)[\ud835\udc5f+\ud835\udc49(4)] + \ud835\udc43(\ud835\udc60^\u2032=5,\ud835\udc4e=\ud835\udc3f)[\ud835\udc5f+\ud835\udc49(5)]\\)</p> <p>\\(\ud835\udc49(4)=0.5[\ud835\udc5f+0]+0.3333[0+0]+0.1666[0+0]=0.0\\)</p> <p>And after 104 iterations we finally reach the convergence:</p> <p> </p> <p>Looking at the Values table you notice that state 0 and 6 are not updated through the whole process!! Why?</p> <p>Because they are terminal states, meaning that being spawned at them or moving to them, ends the epiosde, so there is no next state (no reward, no next state value) for them to updete their vlaues.</p> <p>Now that we have finished the first step of value iteration, it's Policy Improvement's turn. How do we do that? Take the action that makes the Q(s,a) the maximum:</p> <p></p> <p>But so for we only calulated values (V) not Q!! what should we do? there are 2 soloutions.</p> <p>1- Start over, use Q bellman Eqaution to Recalculate Q and wast your time.</p> <p>2- or use this equation that we discussed eariler: \\(Q^\ud835\udf0b (\ud835\udc60,a)=\\sum_{\ud835\udc60'}\ud835\udc43(\ud835\udc60' |\ud835\udc60,\ud835\udc4e)[\ud835\udc45(\ud835\udc60,\ud835\udc4e,\ud835\udc60^\u2032 )+\\gamma \\cdot v(\ud835\udc60^\u2032)]\\) by using this equation you can calculate the Q(s,a) from optimal V(s).</p> <p>\\(\ud835\udc44(\ud835\udc60=5,\ud835\udc4e=\ud835\udc45)=\ud835\udc43_{56}^\ud835\udc45 (\ud835\udc5f+\ud835\udc49(6))+\ud835\udc43_{54}^\ud835\udc45( \ud835\udc5f+\ud835\udc49(4))+\ud835\udc43_{55}^\ud835\udc45 (\ud835\udc5f+\ud835\udc49(5)) =\\) \\(0.5(1+0)+0.1666(0+0.1099)+0.3333(0+0.3324)= 0.6290\\)</p> <p>\\(\ud835\udc44(\ud835\udc60=5,\ud835\udc4e=\ud835\udc3f)=\ud835\udc43_{56}^L (\ud835\udc5f+\ud835\udc49(6))+\ud835\udc43_{54}^L( \ud835\udc5f+\ud835\udc49(4))+\ud835\udc43_{55}^L (\ud835\udc5f+\ud835\udc49(5))=\\) \\(0.1666(1+0) + 0.5(0+0.1099)+0.3333(0+0.3324)= 0.3322\\)</p> <p>\\(\ud835\udc44(\ud835\udc60=4,\ud835\udc4e=\ud835\udc45)=\ud835\udc43_{45}^\ud835\udc45(\ud835\udc5f+\ud835\udc49(5))+\ud835\udc43_{44}^\ud835\udc45 (\ud835\udc5f+\ud835\udc49(4))+\ud835\udc43_{43}^\ud835\udc45 (\ud835\udc5f+\ud835\udc49(3))=\\) \\(0.5(0+0.3324)+0.3333(0+0.1099)+0.1666(0+0.0357)= 0.2088\\)</p> <p>\\(\ud835\udc44(\ud835\udc60=4,\ud835\udc4e=L)=\ud835\udc43_{45}^L(\ud835\udc5f+\ud835\udc49(5))+\ud835\udc43_{44}^L (\ud835\udc5f+\ud835\udc49(4))+\ud835\udc43_{43}^L (\ud835\udc5f+\ud835\udc49(3))=\\) \\(0.1666(0+0.3324)+0.3333(0+0.1099)+0.5(0+0.0357)= 0.1099\\)</p> <p>Just like that you can calculate the Q(s,a), after that go back to step 2 and repeat the same procedure till reaching the convergence in policy.</p>"},{"location":"recitations/week2/#generalized-policy-iteration","title":"Generalized Policy Iteration","text":"<p>So far, we've discussed policy iteration. Now, let's talk about generalized policy iteration (GPI). What is it?  Simply put, GPI states that all RL algorithms inherently involve policy iteration. They all have some form of policy evaluation, though the way they evaluate the policy may vary. Likewise, after evaluation, they all perform policy improvement in some way.</p>"},{"location":"recitations/week2/#value-iteration","title":"Value Iteration","text":"<ul> <li> <p>There was a huge problem with Policy iteration. it takes too long for values to converge, and only after that you are able to update your policy. So we use value iteration.</p> </li> <li> <p>In value iteration we don't wait for values to converge, instead we update the policy after each iteration. </p> </li> </ul> <p>Let's get started. Assume the same SWF world with the same initialized random policy and set all V(s) = 0.</p> <p>\\(\ud835\udc44(\ud835\udc60=5,\ud835\udc4e=\ud835\udc45)=\ud835\udc43_{56}^\ud835\udc45 (\ud835\udc5f+\ud835\udc49(6))+\ud835\udc43_{54}^\ud835\udc45(\ud835\udc5f+\ud835\udc49(4))+\ud835\udc43_{55}^\ud835\udc45 (\ud835\udc5f+\ud835\udc49(5)) =\\) \\(0.5(1+0)+0.1666(0+0)+0.3333(0+0)= 0.5\\)</p> <p>\\(\ud835\udc44(\ud835\udc60=5,\ud835\udc4e=\ud835\udc3f)=\ud835\udc43_{56}^L (\ud835\udc5f+\ud835\udc49(6))+\ud835\udc43_{54}^L(\ud835\udc5f+\ud835\udc49(4))+\ud835\udc43_{55}^L (\ud835\udc5f+\ud835\udc49(5))=\\) \\(0.1666(1+0) + 0.5(0+0)+0.3333(0+0) = 0.1666 \u2245 0.17\\)</p> <p>The other Q values remain Zero. Now update the Policy using the same method for policy improvement!!!</p> <p>\\(Argmax_a(\ud835\udc44(\ud835\udc60=5,\ud835\udc4e=\ud835\udc45),\ud835\udc44(\ud835\udc60=5,\ud835\udc4e=L)) = (a = R)\\)  Since other values remain unchanged we leave them we the initial Policy.</p> <p></p> <p>For the second iteration first calculate the new values.</p> <p>\\(\ud835\udc49(5)=\ud835\udc43(\ud835\udc60^\u2032=6,\ud835\udc4e=\ud835\udc45)[\ud835\udc5f+\ud835\udc49(6)] + \ud835\udc43(\ud835\udc60^\u2032=5,\ud835\udc4e=\ud835\udc45)[\ud835\udc5f+\ud835\udc49(5)] + \ud835\udc43(\ud835\udc60^\u2032=4,\ud835\udc4e=\ud835\udc45)[\ud835\udc5f+\ud835\udc49(4)]\\) \\(\ud835\udc49(5)=0.5[1+0] + 0.3333[0+0] + 0.1666[0+0]=0.5\\)</p> <p>\\(\ud835\udc49(4)=\ud835\udc43(\ud835\udc60^\u2032=5,\ud835\udc4e=\ud835\udc3f)[\ud835\udc5f+\ud835\udc49(5)] + \ud835\udc43(\ud835\udc60^\u2032=4,\ud835\udc4e=\ud835\udc3f)[\ud835\udc5f+\ud835\udc49(4)] + \ud835\udc43(\ud835\udc60^\u2032=3,\ud835\udc4e=\ud835\udc3f)[\ud835\udc5f+\ud835\udc49(3)]\\) \\(\ud835\udc49(4)=0.5[0+0] + 0.3333[0+0] + 0.1666[0+0] = 0.0\\)</p> <p>The other values remain zero like V(4).</p> <p>Now calculate the Q values based on new values.</p> <p>\\(\ud835\udc44(\ud835\udc60=5,\ud835\udc4e=\ud835\udc45)=\ud835\udc43_{56}^\ud835\udc45 (\ud835\udc5f+\ud835\udc49(6))+\ud835\udc43_{55}^\ud835\udc45 (\ud835\udc5f+\ud835\udc49(5))+\ud835\udc43_{54}^\ud835\udc45 (\ud835\udc5f+\ud835\udc49(4))=\\) \\(0.5(1+0)+0.3333(0+0.5)+0.1666(0+0) = 0.6665 \u2245 0.67\\)</p> <p>\\(\ud835\udc44(\ud835\udc60=5,\ud835\udc4e=\ud835\udc45)=\ud835\udc43_{56}^L (\ud835\udc5f+\ud835\udc49(6))+\ud835\udc43_{54}^L (\ud835\udc5f+\ud835\udc49(4))+\ud835\udc43_{55}^L (\ud835\udc5f+\ud835\udc49(5))=\\) \\(0.1666(1+0)+0.5(0+0)+0.3333(0+0.5)= 0.3333 \u22450.33\\)</p> <p>\\(\ud835\udc44(\ud835\udc60=4,\ud835\udc4e=\ud835\udc45)=\ud835\udc43_{45}^\ud835\udc45 (\ud835\udc5f+\ud835\udc49(5))+\ud835\udc43_{44}^\ud835\udc45 (\ud835\udc5f+\ud835\udc49(4))+\ud835\udc43_{43}^\ud835\udc45 (\ud835\udc5f+\ud835\udc49(3))=\\) \\(0.5( 0+0.5)+0.3333(0+0)+0.1666(0+0)=0.25\\)</p> <p>\\(\ud835\udc44(\ud835\udc60=4,\ud835\udc4e=\ud835\udc3f)=\ud835\udc43_{45}^\ud835\udc3f (\ud835\udc5f+\ud835\udc49(5))+\ud835\udc43_{44}^\ud835\udc3f (\ud835\udc5f+\ud835\udc49(4))+\ud835\udc43_{43}^\ud835\udc3f (\ud835\udc5f+\ud835\udc49(3))=\\) \\(0.1666(0+0.5)+0.3333 (0+0)+0.3333(0+0)= 0.0833 \u2245 0.08\\) The other Q values remain Zero.</p> <p>Now Again, Policy Imrovement:</p> <ul> <li>\\(Argmax_a(\ud835\udc44(\ud835\udc60=5,\ud835\udc4e=\ud835\udc45),\ud835\udc44(\ud835\udc60=5,\ud835\udc4e=L)) = (a = R)\\) </li> <li>\\(Argmax_a(\ud835\udc44(\ud835\udc60=4,\ud835\udc4e=\ud835\udc45),\ud835\udc44(\ud835\udc60=4,\ud835\udc4e=L)) = (a = R)\\)</li> </ul> <p>The new policy looks like:</p> <p></p> <p>Notice that:</p> <ul> <li>in value iteration there is no need to reach the optimal Value to update the policy.</li> <li>You can update the policy with each iteration</li> </ul> <p>By repeating this loop after 122 iteration we finally convergence:</p> <p></p>"},{"location":"recitations/week2/#exploration-vs-exploitation","title":"Exploration vs Exploitation","text":"<p>Assume you're walking school to your house. there is one hour long path that you take everyday. suddenly you decide to take a new path to shorten the walkong duration. you don't know about the new path, is it faster? is it safe? you even may get lost. this is a risk. it can raise you high up or break you.</p> <p>this is the concept of Exploration vs Exploitation!! when you take the usual path it's Exploitation, meaning you're using your current knowledge, that same old  policy that wrks, but taking the new path is Exploration, you're trying to find new things.</p>"},{"location":"recitations/week2/#e-soft-policy","title":"E-Soft Policy","text":"<p>Does the same thing When dealing with RL problems. Also known as \\(\\epsilon \\text{ } \\text{greedy policy}\\),  </p> <p>\\(\\epsilon\\) is the value smaller or equal to than 1, it controls the probabilty of which Eploitation is done. usually starts with a large value (close to 1) and   over time decreases to a pre-set value (close to 0). </p> <p>It states that the best action is selected with probability of \\(1-\\epsilon + \\frac{\\epsilon}{|A(s_t)|}\\) and other actions each have tha chance of   \\(\\frac{\\epsilon}{|A(s_t)|}\\) to be selected. \\(|A(s_t)|\\) is the number of actions in the action space.</p> <p>Example: If  \\(\\epsilon = 0.1\\) and \\(|A(s_t)| = 4\\) thus \\(1-\\epsilon + \\frac{\\epsilon}{|A(s_t)|} = 1 - 0.1 + \\frac{0.1}{4} = 0.925\\) with probability of 0.925 We  choose the best action and other 3 actions each have 0.025 chance to be selected. </p> <p>\\(\\epsilon \\text{ } \\text{greedy}\\) is a simpler form of the \\(\\epsilon \\text{ } \\text{Soft policy}\\):</p> <p></p> <p>In this form with the chance of \\(\\epsilon\\) we choose a randome action (all actions can be taken) and with the chance of \\(1 - \\epsilon\\) we choose the best action.</p>"},{"location":"recitations/week2/#on-policy-off-policy","title":"ON Policy &amp; OFF Policy","text":"<p>An algorithm is off-policy if it updates its policy using data collected from a different policy, rather than the one being improved. it means that we have two set of policies. one which is called behavior policy, is used to gather date from the environment. and the target policy is the  one getting improved by this data.</p> <p>An On policy algorithm uses the same policy that is being improved to gather data.  Further explanation will be done in next sections.</p>"},{"location":"recitations/week2/#sampling-methods","title":"Sampling Methods","text":"<p>Prior to this section, We utilized our methods in environments that we knew their everything about them. and by everything I mean: transition functions or in another words we knew how does their stochasticity work.</p> <p>But in reallity we don't know them! thus we can't calculate values which means we can't improve policies. as a result we utilize sampling methods which gather data from the environment and use them to estimate the values.</p>"},{"location":"recitations/week2/#monte-carlo","title":"Monte Carlo","text":"<p>Monte Carlo is one of these methods. How does it work? basically agent is initialized somewhere in the environment and starts exploring the world and gathering these data: \\((s_0,a_0,r_0,s_1,a_1,r_1,s_2,...,s_{T-1},a_{T-1},r_{T-1},s_T)\\) Then using these data we Estimate Values, Remember the Value function formula:</p> <p></p> <p>We shall Estimate this function By taking the mean of the returns:</p> <p>\\(V(s_t) = \\sum_{t'=t}^T r_t/N(s_t)\\)  Where \\(N(s_t)\\) is the number of episodes that \\(s_t\\) has been visited in them.</p> <p>Okay now let's have an example:</p> <p></p> <p>In this example which includes two episodes the Agent starts from State 12 and reaches the Goal. We Want to calculate the Value of state 12. in the left episode the agent receive a -10 by stepping into the state 21 and receive a +10 by reaching the state 23 which is our goal.</p> <p>\\(R(\\tau_1) = -10 + 10 = 0\\)</p> <p>and in the right one, agent only receive a +10 by reaching the state 23 which is our goal. so</p> <p>\\(R(\\tau_2) = 10\\)</p> <p>Also let's consider another episode:</p> <p></p> <p>In this onr the agent has stepped into state 12 twice, and we should calculate its Return. There are two ways to do that.</p>"},{"location":"recitations/week2/#first-visit-monte-carlo","title":"First Visit Monte Carlo","text":"<p>first, count the state 12 just once:</p> <p>\\(R(\\tau_3) = - 10 - 10 + 10 = -10\\)</p> <p>So \\(N(s_t = 12) = 3\\) and \\(V(s_t = 12) = \\frac{R(\\tau_1) + R(\\tau_2) + R(\\tau_3)}{N(s_t = 12)} =\\frac{0 + 10 - 10}{3} = 0\\)</p> <p>This way of calculating the value is called [First Visit Monte Carlo]. meaning we only charge the agent for the visited states once.</p>"},{"location":"recitations/week2/#every-visit-monte-carlo","title":"Every Visit Monte Carlo","text":"<p>But there is another way too, we charge the agent each time visits a state. in this case for episode 3:</p> <p>\\(R(\\tau_3) = -10 - 10 + 10 = -10\\) and \\(R(\\tau_4) = +10\\). as an explanation, agent starts at state 12 goes to state 21, steps into 22 and gets back to 12 then countinues its path to the goal.</p> <p>this is the \\(\\tau_3\\) and we charged the agent because of the first time.it visited the state 12. also as stated, agent gets back to the state 12, so this the second time it's visiting state 12 so we charge it again. which only recives a +10 reward in its path. so \\(R(\\tau_4) = +10\\)</p> <p>thus So \\(N(s_t = 12) = 4\\) and \\(V(s_t = 12) = \\frac{R(\\tau_1) + R(\\tau_2) + R(\\tau_3) + R(\\tau_4)}{N(s_t = 12)} =\\frac{0 + 10 - 10 + 10}{4} = 2.5\\)</p> <p>This way of calculating the value is called [Every Visit Monte Carlo].</p>"},{"location":"recitations/week2/#incremental-mean","title":"Incremental Mean","text":"<p>We all know the usual way to calculate mean of numbers, but in our case, keeping the track of them in a loop, for multiple states is not really an efficent way. so we introduce Incremental Mean.</p> <p>\\(Mean^{new} = Mean^{old} + Step Size[New Estimated Value - Mean^{old}]\\) </p> <p>Here StepSize is like a learning rate, it controls the effect of the new Estimated value</p>"},{"location":"recitations/week2/#countiue-to-monte-carlo","title":"Countiue to Monte Carlo","text":"<p>So now we want to calculate the Value using incremental mean. we rewrite the incremental mean for value like this:</p> <p>\\(Value(s_t)^{new} = Value(s_t)^{old} + Step Size[G_t - V(s_t)^{old}]\\) </p>"},{"location":"recitations/week2/#monte-carlo-control-first-visit-pseudocode","title":"Monte Carlo Control-First visit Pseudocode","text":"<p>The process of estimating the (Q or V) is called Monte Carlo Prediction. The full Procedure that helps us to have a better policy is Monte Carlo control.</p> <p>Monte Carlo Control-First visit </p> <p>As you can see after Estimating the New Values The  \\(\\epsilon \\text{ } \\text{greedy}\\) is used to improve the policy and collect better data in the next episode. Since the policy Generating the Episode and the policy that is being improved, both are  \\(\\epsilon \\text{ } \\text{greedy}\\) this is an ON policy MC.</p>"},{"location":"recitations/week2/#sampling-and-bootstrapping","title":"Sampling and Bootstrapping","text":"<p>If you take a close look at Monte Carlo example, you find out, that to calulate the values I needed to finish the episode. this is not quite desired. if there was a way to calculate the the Value without the need to finish the episode, that would have been great. So now we meet Bootstrapping</p> <p></p> <p>In this image the left one is like Mone Carlo, Starts in an initial state and we keep track of it,till reaches the end. but the right one is Bootstrapping, as I said in Bootstrapping we don't wait till the end. in this case we calculate the new value, the moment we receive the new reward and went to a new state. with this Introduction, I'am going to introduce you two Bootstrapping methods:</p> <ul> <li> <p>SARSA</p> </li> <li> <p>Q Learning</p> </li> </ul> <p>SARSA and Q Learning are a subsections of Temporal Difference Learning method(TD Learning Method). as It's called TD Learning, We introduce two key Equations for TD Learning:</p>"},{"location":"recitations/week2/#td-target","title":"TD Target","text":"<ul> <li>\\(\\text{TD Target} = r_{t+1} + \\gamma \\cdot V(s_{t+1})\\) This equation is basically the new value estimation for \\(V(s_{t})\\) which in Monte Carlo was Estimated by \\(G_t\\)</li> </ul>"},{"location":"recitations/week2/#td-error","title":"TD ERROR","text":"<ul> <li>\\(\\text{TD Error} = \\text{TD Target} - V(s_t)^{old}\\)  This is the error we aim to minimize it, the correct minimization of this means that we have reached a right estimation of the values.</li> </ul> <p>Value update formula for TD:</p>"},{"location":"recitations/week2/#update-formula","title":"Update Formula","text":"<p>\\(V(s_t)^{new} = V(s_t)^{old} + StepSize[\\text{TD Target} - V(s_t)^{old}]\\) </p>"},{"location":"recitations/week2/#td-pseudocode","title":"TD Pseudocode","text":"<p>As mentioned in TD Learning we use Bootstrapping. the following Pseudocode explanes how we update the Values in a loop.</p> <p></p>"},{"location":"recitations/week2/#sarsa","title":"SARSA","text":"<p>As I said earlier knowing the Values, alone doesn't help. we need Q values to have a better undrestnding of the policy!! in SARSA we calculate Qs instead od Vs.</p> <p>so form of the previously mentioned equations will change a little bit.</p> <ul> <li> <p>\\(\\text{TD Target}_{SARSA} = r_{t+1} + \\gamma \\cdot Q(s_{t+1},a_{t+1})\\) </p> </li> <li> <p>\\(\\text{TD Error}_{SARSA} = \\text{TD Target}_{SARSA} - Q(s_t,a_t)^{old}\\) </p> </li> <li> <p>\\(Q(s_t,a_t)^{new} =Q(s_t,a_t)^{old} + StepSize[\\text{TD Target}_{SARSA} - Q(s_t,a_t)^{old}]\\) </p> </li> </ul>"},{"location":"recitations/week2/#sarsa-pseudocode","title":"SARSA Pseudocode","text":"<p>Notice that SARSA uses \\(\\epsilon \\text{ } \\text{greedy policy}\\) both in taking actions and updating the Q values. this means that SARSA is an on policy method. since the policy gathering data and the policy, being improved are both  \\(\\epsilon \\text{ } \\text{greedy}\\)</p>"},{"location":"recitations/week2/#q-learning","title":"Q Learning","text":"<p>Q Learning is Quite like SARSA with a little difference: it has a max term in TD Target.</p> <ul> <li> <p>\\(\\text{TD Target}_{Q Learning} = r_{t+1} + \\gamma \\cdot max_a[Q(s_{t+1},a_{t+1})]\\) </p> </li> <li> <p>\\(\\text{TD Error}_{Q Learning} = \\text{TD Target}_{Q Learning} - Q(s_t,a_t)^{old}\\) </p> </li> <li> <p>\\(Q(s_t,a_t)^{new} =Q(s_t,a_t)^{old} + StepSize[\\text{TD Target}_{Q Learning} - Q(s_t,a_t)^{old}]\\) </p> </li> </ul>"},{"location":"recitations/week2/#q-learning-example","title":"Q Learning Example","text":"<p>Example: Consider this mouse, trying to get cheese and not getting. the Reward function is explaned in the figure. \\(StepSize = 0.1\\) and \\(\\gamma = 0.99\\)</p> <p>Now Step by Step We are about to update Q(s,a) using Q learning.</p> <p></p> <p>Time Step 1</p> <p>Step 1: Initialize Q arbitrarily (e.g, Q(s,a) =0 for All \\(\\in S\\) and \\(a \\in A(s)\\) and Q(terminal state,.) = 0)</p> <p></p> <p>Step 2: Choose and perform an Action from \\(\\epsilon-greedy\\) policy. Assuming that starting \\(\\epsilon\\) is 1, I take a random action to right.</p> <p></p> <p>Step 3: Update \\(\ud835\udc44(s_\ud835\udc61, a_\ud835\udc61)\\)</p> <p>\\(Q(s_t,a_t)^{new} =Q(s_t,a_t)^{old} + StepSize[r_{t+1} + \\gamma \\cdot max_a[Q(s_{t+1},a_{t+1})] - Q(s_t,a_t)^{old}]\\) </p> <p>\\(Q(\\text{Initial State, Right}) = 0 + 0.1[1 + 0.99\\cdot(0) - 0]  = 0.1\\)</p> <p></p> <p>Time Step 2</p> <p>Step 1: Choose Action and perform. I take a random action again, since epsilon=0.99 is big.\u00a0(Notice we decay epsilon a little bit because, as the training progress, we want less and less exploration). I took the action Down.\u00a0This is not a good action since it leads me to the poison.</p> <p></p> <p>Because I ate poison, I got \\(R_{t+1}\\)=\u221210, and I died. \\(\\rightarrow\\) Episode Ended</p> <p>\\(Q(\\text{State 2, Right}) = 0 + 0.1[-10 + 0.99\\cdot(0) - 0]  = -1\\)</p> <p></p> <p>To finally gain converge and Right policy, you should Apply this procedure much more. </p>"},{"location":"recitations/week2/#q-learning-pseudocode","title":"Q Learning Pseudocode","text":"<p>Notice that Q Learning uses \\(\\epsilon \\text{ } \\text{greedy policy}\\) just in taking actions and when it comes to updating the Q values it's greedy policy. This means Q learning is an off policy method.</p>"},{"location":"recitations/week2/#a-brief-comparison-between-td-and-mc","title":"A brief Comparison between TD and MC","text":""},{"location":"recitations/week2/#n-step-td-learning","title":"N Step TD Learning","text":"<p>Take a good look at this figure:</p> <p></p> <p>Prior to this section We only talked about 1 Step TD, where ONLY TooK ONE Action and recieved Reward, That is known as TD(0). But we can take more steps and gather Explicit Rewards insread of just estimating the next state values. as we increase the taken steps, we move from TD(0) to TD(1), Where TD one is the same as Monte Carlo, meaning it waits till the end of the episode.</p> <p>So according to this description, we should calculate the N Step Returns.</p> <p></p>"},{"location":"recitations/week2/#n-step-sarsa-pseudocode","title":"N Step SARSA Pseudocode","text":"<p>Based this introduction, Now you can undrestand SARSA Pseudocode:</p> <p></p>"},{"location":"recitations/week2/#references","title":"References","text":"<ul> <li>Sutton &amp; Barto Book: Reinforcement Learning</li> <li>Robotch Academy: Slides of Reinforcement Learning Course</li> <li>Grokking Deep Reinforcement Learning Book</li> <li>Hugging Face RL course</li> </ul>"},{"location":"recitations/week2/#authors","title":"Author(s)","text":"<ul> <li> <p>Reza GhaderiZadeh</p> <p>Teaching Assistant</p> <p>r.ghaderi2001@gmail.com</p> <p> </p> </li> </ul>"},{"location":"recitations/week3/","title":"Week 3: Policy-Based Methods","text":""},{"location":"recitations/week3/#screen-record","title":"Screen Record","text":""},{"location":"recitations/week3/#recitation-notes","title":"Recitation Notes","text":""},{"location":"recitations/week3/#types-of-reinforcement-learning-methods","title":"Types of Reinforcement Learning Methods","text":"<p>Reinforcement Learning (RL) can be broadly classified into two main categories: Model-Based and Model-Free methods. These methods differ in how they approach the problem of decision-making and learning in environments with uncertainty.</p>"},{"location":"recitations/week3/#model-based-reinforcement-learning","title":"Model-Based Reinforcement Learning","text":"<p>In Model-Based RL, the agent attempts to learn or is provided with a model of the environment. This model represents the environment's dynamics, including the transition function (how the environment reacts to actions) and the reward function (the reward the agent receives for an action in a state). The agent can plan ahead by simulating possible future trajectories using this model.</p> <p>Examples of Model-Based RL Methods:</p> <ul> <li>Dyna-Style Methods</li> <li>Model Predictive Control (MPC)</li> </ul> <p>Advantages of Model-Based RL:</p> <ul> <li>Uses planning to reduce interaction with the environment.</li> <li>Converges faster when a reliable model is available.</li> </ul> <p>Disadvantages of Model-Based RL:</p> <ul> <li>Requires an accurate model, which may be hard to obtain.</li> <li>Planning can be computationally expensive.</li> </ul>"},{"location":"recitations/week3/#model-free-reinforcement-learning","title":"Model-Free Reinforcement Learning","text":"<p>In Model-Free RL, the agent does not learn a model of the environment. Instead, it directly learns the value function or policy through interaction with the environment. This is done by observing the outcomes of actions taken in various states and using these experiences to improve the agent's decisions.</p>"},{"location":"recitations/week3/#value-based-methods","title":"Value-Based Methods","text":"<p>Value-Based methods focus on learning the value function, which estimates the expected return (or cumulative reward) from a given state or state-action pair. The agent typically learns this value function and selects actions based on it to maximize expected return.</p> <p>Examples of Value-Based RL Methods:</p> <ul> <li>Q-Learning</li> <li>SARSA</li> <li>DQN</li> </ul> <p>Advantages of Value-Based Methods:</p> <ul> <li>Simple to implement and effective when the action space is discrete.</li> <li>Can work with both discrete and continuous state spaces.</li> </ul> <p>Disadvantages of Value-Based Methods:</p> <ul> <li>Only applicable to discrete action spaces.</li> <li>May struggle in environments with complex dynamics or high-dimensional states.</li> </ul>"},{"location":"recitations/week3/#policy-based-methods","title":"Policy-Based Methods","text":"<p>In Policy-Based methods, the agent learns a policy directly, which is a mapping from states to actions. The goal is to optimize the policy in a way that maximizes expected return, rather than estimating values for state-action pairs.</p> <p>Examples of Policy-Based RL Methods:</p> <ul> <li>REINFORCE</li> <li>Proximal Policy Optimization (PPO)</li> <li>Trust Region Policy Optimization (TRPO)</li> </ul> <p>Advantages of Policy-Based Methods:</p> <ul> <li>Can handle continuous action spaces.</li> <li>Suitable for learning complex policies, especially in high-dimensional environments.</li> </ul> <p>Disadvantages of Policy-Based Methods:</p> <ul> <li>Requires a large amount of data for effective training.</li> <li>Computationally expensive due to gradient estimation and optimization.</li> </ul>"},{"location":"recitations/week3/#actor-critic-methods","title":"Actor-Critic Methods","text":"<p>Actor-Critic methods combine both value-based and policy-based approaches. These methods maintain two components:</p> <ul> <li>Actor: The policy function, which is responsible for deciding which action to take given the current state.</li> <li>Critic: The value function, which evaluates the actions taken by the actor based on their expected return (e.g., using a value function or action-value function).</li> </ul> <p>Examples of Actor-Critic Methods:</p> <ul> <li>A2C (Advantage Actor-Critic)</li> <li>A3C (Asynchronous Advantage Actor-Critic)</li> <li>DDPG (Deep Deterministic Policy Gradient)</li> </ul> <p>Advantages of Actor-Critic Methods:</p> <ul> <li>Combines value-based and policy-based approaches.</li> <li>Works well in continuous action spaces.</li> <li>Uses value-based feedback to improve policy stability.</li> </ul> <p>Disadvantages of Actor-Critic Methods:</p> <ul> <li>More complex to implement and tune.</li> <li>Requires careful balancing between actor and critic to ensure stability.</li> </ul> <p>Summary of RL Methods:</p> <ul> <li>Model-Based RL: Learns or uses a model of the environment to plan and optimize actions.</li> <li>Model-Free RL:<ul> <li>Value-Based Methods: Learn a value function to guide action selection (e.g., Q-Learning, SARSA, DQN).</li> <li>Policy-Based Methods: Learn a direct policy that maps states to actions (e.g., REINFORCE, PPO, TRPO).</li> <li>Actor-Critic Methods: Combine both value-based and policy-based approaches (e.g., A2C, A3C, DDPG).</li> </ul> </li> </ul>"},{"location":"recitations/week3/#policy-based-methods_1","title":"Policy-Based Methods","text":"<p>Policy-based methods are a class of reinforcement learning (RL) algorithms where the goal is to directly learn a policy that maximizes cumulative rewards, as opposed to learning a value function. These methods are advantageous in environments with large or continuous action spaces, where value-based methods (such as Q-learning) struggle.</p> <p>Policy search methods aim to find an optimal policy by directly optimizing its parameters. There are two main approaches: evolutionary methods, such as Genetic Algorithms (GA), and gradient-based methods, such as Policy Gradient.</p>"},{"location":"recitations/week3/#evolutionary-methods-eg-genetic-algorithm","title":"Evolutionary Methods (e.g., Genetic Algorithm)","text":"<p>Evolutionary methods optimize policy parameters by searching through a population of candidate policies. These methods do not rely on gradient information but instead evolve policies through selection, mutation, and crossover.</p> <ul> <li>A population of policies is initialized randomly.</li> <li>Each policy is evaluated based on its performance (fitness).</li> <li>The best-performing policies are selected and modified through mutation and crossover to create a new generation.</li> <li>This process continues for multiple generations, gradually improving the policy.</li> </ul> <p>Since evolutionary methods do not use gradients, they are useful for optimizing policies in environments where the objective function is not differentiable or well-defined.</p>"},{"location":"recitations/week3/#policy-gradient-methods","title":"Policy Gradient Methods","text":"<p>Policy Gradient methods take a different approach by defining an objective function, which is the expected return, and optimizing it using gradient ascent. Instead of searching randomly, these methods directly adjust the policy parameters in the direction that increases the expected reward.</p> <ul> <li>The policy is parameterized (e.g., as a neural network).</li> <li>The expected return is used as an objective function.</li> <li>The gradient of this objective is computed with respect to the policy parameters.</li> <li>The parameters are updated using gradient ascent to maximize expected return.</li> </ul> <p>Policy Gradient methods are efficient when the objective function is differentiable and provide a more structured way of reaching optimal policies compared to evolutionary methods.</p>"},{"location":"recitations/week3/#key-differences","title":"Key Differences","text":"<ul> <li>Optimization Approach: Evolutionary methods search for optimal policies using a population-based approach, while Policy Gradient methods optimize a defined objective function using gradients.</li> <li>Use of Gradients: Evolutionary methods do not require gradients, making them useful in non-differentiable RL environments, whereas Policy Gradient methods rely on computing gradients to optimize the policy.</li> <li>Convergence Behavior: Policy Gradient methods typically converge more smoothly as they directly optimize an objective, whereas evolutionary methods rely on population-based exploration, which can lead to slower convergence.</li> <li>Exploration vs. Exploitation: Evolutionary methods naturally encourage more exploration by maintaining a diverse population of policies, while Policy Gradient methods refine a single policy through experience.</li> </ul> <p>Both methods have advantages and are chosen based on the problem requirements. Evolutionary methods are useful for complex RL problems where gradient information is unavailable or unreliable, while Policy Gradient methods are more structured and efficient when gradients can be computed.</p>"},{"location":"recitations/week3/#sudo-code-example-for-ga-based-policy-search","title":"Sudo Code Example for GA-Based Policy Search","text":"<p>Below is an example pseudocode for a genetic algorithm applied to policy search:</p> <pre><code>Algorithm: Genetic Algorithm for Policy Optimization\n1. Initialize a population of random policies:\n   population = initialize_population(population_size)\n2. For generation = 1 to num_generations:\n   a. Evaluate fitness of each policy:\n      For policy in population:\n         fitness = evaluate_policy(policy)\n         store fitness score\n   b. Select top-performing policies:\n      selected_policies = select_top_policies(population, fitness_scores)\n   c. Generate next generation:\n      next_generation = []\n      While len(next_generation) &lt; population_size:\n         parent1, parent2 = select_parents(selected_policies)\n         child1, child2 = crossover(parent1, parent2)\n         child1 = mutate(child1)\n         child2 = mutate(child2)\n         next_generation.append(child1, child2)\n      population = next_generation\n3. Return best-performing policy:\n   best_policy = select_best_policy(population, fitness_scores)\n</code></pre>"},{"location":"recitations/week3/#biased-vs-unbiased-estimation","title":"Biased vs. Unbiased Estimation","text":"<p>The biased formula for the sample variance \\(S^2\\) is given by:</p> \\[ S^2_{\\text{biased}} = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\overline{X})^2 \\] <p>This is an underestimation of the true population variance \\(\\sigma^2\\) because it does not account for the degrees of freedom in estimation. Instead, the unbiased estimator is:</p> \\[ S^2_{\\text{unbiased}} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\overline{X})^2. \\] <p>To see why the biased version underestimates the true variance, recall the expectation:</p> \\[ \\mathbb{E}[S^2_{\\text{biased}}] = \\left( 1 - \\frac{1}{n} \\right) \\sigma^2 &lt; \\sigma^2. \\]"},{"location":"recitations/week3/#variance-of-estimation","title":"Variance of Estimation","text":"<p>Variance in estimation quantifies how much an estimator's output fluctuates across different samples from the same population. For an estimator \\(\\hat{\\theta}\\) of a parameter \\(\\theta\\), the variance is defined as:</p> \\[ \\text{Var}(\\hat{\\theta}) = E\\left[(\\hat{\\theta} - E[\\hat{\\theta}])^2\\right] \\] <p>Where: - \\(\\hat{\\theta}\\) is the estimator of the parameter \\(\\theta\\), - \\(E[\\hat{\\theta}]\\) is the expected value of the estimator.</p>"},{"location":"recitations/week3/#monte-carlo-estimators-in-reinforcement-learning","title":"Monte Carlo Estimators in Reinforcement Learning","text":"<p>A Monte Carlo estimator is a method used to approximate the expected value of a function \\(f(X)\\) over a random variable \\(X\\) with a given probability distribution \\(p(X)\\). The true expectation is:</p> \\[ E[f(X)] = \\int f(x) p(x) \\, dx \\] <p>Instead, we use Monte Carlo estimation by drawing \\(N\\) independent samples \\(X_1, X_2, \\dots, X_N\\) from \\(p(X)\\) and computing:</p> \\[ \\hat{\\mu}_{MC} = \\frac{1}{N} \\sum_{i=1}^N f(X_i) \\]"},{"location":"recitations/week3/#policy-gradient-in-reinforcement-learning","title":"Policy Gradient in Reinforcement Learning","text":"<p>Policy gradient methods directly optimize the policy \\(\\pi_{\\theta}(a | s)\\) by adjusting the parameters \\(\\theta\\) using gradient ascent on expected cumulative rewards:</p> \\[ J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ R(\\tau) \\right] \\] <p>where \\(\\tau = (s_0, a_0, s_1, a_1, \\dots)\\) represents a trajectory following policy \\(\\pi_{\\theta}\\), and \\(R(\\tau)\\) is the cumulative reward. Using the log likelihood ratio trick, the policy gradient theorem gives:</p> \\[ \\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) R_t \\right] \\]"},{"location":"recitations/week3/#unbiased-estimation-and-high-variance-in-policy-gradient","title":"Unbiased Estimation and High Variance in Policy Gradient","text":"<p>Monte Carlo estimators of policy gradients are unbiased, meaning:</p> \\[\\mathbb{E} \\left[ \\nabla_{\\theta} J(\\theta) \\right] = \\nabla_{\\theta} J(\\theta)\\] <p>However, they often suffer from high variance, which makes learning unstable. The variance of the Monte Carlo estimate can be large due to randomness in the trajectory sampling process. To reduce variance, we commonly use:</p> <ul> <li>Baseline: Subtracting a baseline function \\(b(s)\\), which does not affect the expectation but reduces variance:</li> </ul> \\[\\nabla_{\\theta} J(\\theta) = \\mathbb{E} \\left[ \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) (R_t - b(s_t)) \\right]\\] <ul> <li> <p>Actor-Critic Methods: Using a learned value function \\(V(s)\\) as a baseline.</p> </li> <li> <p>Variance Reduction Techniques: Techniques like reward normalization, advantage estimation, and bootstrapping help stabilize learning.</p> </li> </ul> <p>Despite these techniques, policy gradient methods remain sensitive to high variance, necessitating careful tuning and large batch sizes.</p>"},{"location":"recitations/week3/#reinforce-algorithm","title":"REINFORCE Algorithm","text":"<p>Algorithm: REINFORCE: Monte-Carlo Policy-Gradient Control (Episodic)</p> <ol> <li>Input: Differentiable policy parameterization \\(\\pi(a | s, \\theta)\\)</li> <li>Algorithm parameter: Step size \\(\\alpha &gt; 0\\)</li> <li>Initialize: Policy parameters \\(\\theta \\in \\mathbb{R}^d\\) (e.g., to 0)</li> <li>Loop forever (for each episode):<ul> <li>Generate an episode \\(S_0, A_0, R_1, \\dots, S_T, A_{T-1}, R_T\\) following \\(\\pi(\\cdot | \\cdot, \\theta)\\)</li> <li>For each step \\(t = 0, 1, \\dots, T-1\\):<ul> <li>Compute return:  \\(G \\gets \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_k\\)</li> <li>Update policy parameters:  \\(\\theta \\gets \\theta + \\alpha \\gamma^t G \\nabla \\ln \\pi(A_t | S_t, \\theta)\\)</li> </ul> </li> </ul> </li> </ol>"},{"location":"recitations/week3/#using-reinforce-in-continuous-action-spaces","title":"Using REINFORCE in Continuous Action Spaces","text":"<p>The REINFORCE algorithm is a policy gradient method that can be applied to continuous action spaces by parameterizing the policy as a continuous distribution over actions, rather than using discrete action choices. In continuous action spaces, the policy is typically represented as a probability distribution, such as a Gaussian distribution, where the mean and variance of the distribution are learned through the parameters \\(\\theta\\).</p>"},{"location":"recitations/week3/#policy-representation","title":"Policy Representation","text":"<p>Let the policy \\(\\pi_{\\theta}(a_t | s_t)\\) be represented by a parametric family of probability distributions. In the case of a continuous action space, one common choice is a Gaussian distribution:</p> \\[\\pi_{\\theta}(a_t | s_t) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(a_t - \\mu(s_t))^2}{2\\sigma^2} \\right)\\] <p>where:</p> <ul> <li>\\(\\mu(s_t)\\) is the mean of the action distribution, which is a function of the state \\(s_t\\) parameterized by \\(\\theta\\).</li> <li>\\(\\sigma^2\\) is the variance of the action distribution, which may also be a function of the state or treated as a fixed constant.</li> </ul> <p>The policy outputs a distribution over actions, and the agent samples actions \\(a_t\\) from this distribution during each step.</p>"},{"location":"recitations/week3/#gradient-estimation","title":"Gradient Estimation","text":"<p>For continuous action spaces, the gradient of the objective function (expected return) with respect to the policy parameters \\(\\theta\\) is given by:</p> \\[\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^{T} G_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) \\right]\\] <p>where:</p> <ul> <li>\\(\\tau = (s_0, a_0, s_1, a_1, \\dots)\\) is a trajectory sampled from the policy \\(\\pi_{\\theta}\\).</li> <li>\\(G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k\\) is the return (cumulative discounted reward) from time step \\(t\\) onward.</li> <li>\\(\\log \\pi_{\\theta}(a_t | s_t)\\) is the log probability of selecting action \\(a_t\\) given state \\(s_t\\) under policy \\(\\pi_{\\theta}\\).</li> </ul> <p>The policy parameters \\(\\theta\\) are updated using gradient ascent:</p> \\[\\theta \\gets \\theta + \\alpha \\sum_{t=0}^{T} G_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t)\\] <p>For a Gaussian policy, where the action distribution is parameterized as:</p> \\[\\pi_{\\theta}(a_t | s_t) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(a_t - \\mu_{\\theta}(s_t))^2}{2\\sigma^2} \\right),\\] <p>the log-probability is:</p> \\[\\log \\pi_{\\theta}(a_t | s_t) = -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(a_t - \\mu_{\\theta}(s_t))^2}{2\\sigma^2}.\\] <p>Taking the gradient of the log-probability with respect to \\(\\theta\\) results in:</p> \\[\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) = \\frac{a_t - \\mu_{\\theta}(s_t)}{\\sigma^2} \\nabla_{\\theta} \\mu_{\\theta}(s_t).\\] <p>If \\(\\sigma\\) is also learned, the gradient with respect to \\(\\sigma\\) is:</p> \\[\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) = \\frac{(a_t - \\mu_{\\theta}(s_t))^2 - \\sigma^2}{\\sigma^3} \\nabla_{\\theta} \\sigma.\\]"},{"location":"recitations/week3/#algorithm-overview-in-continuous-action-space","title":"Algorithm Overview in Continuous Action Space","text":"<p>The steps of the REINFORCE algorithm applied to a continuous action space are as follows:</p> <ol> <li>Initialize the policy parameters \\(\\theta\\) (e.g., for a Gaussian policy, initialize the weights that determine the mean \\(\\mu(s_t)\\) and variance \\(\\sigma^2\\)).</li> <li> <p>For each episode:</p> <ul> <li>Initialize the state \\(s_0\\).</li> <li>For each time step:<ul> <li>Sample an action \\(a_t\\) from the policy \\(\\pi_{\\theta}(a_t | s_t)\\).</li> <li>Execute action \\(a_t\\), observe reward \\(r_t\\) and next state \\(s_{t+1}\\).</li> <li>Compute the return \\(G_t\\) (e.g., the cumulative discounted reward).</li> <li> <p>Update the policy parameters using the gradient of the log-probability:</p> <p>\\(\\theta \\leftarrow \\theta + \\alpha G_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t)\\)</p> </li> </ul> </li> </ul> </li> <li> <p>Repeat for multiple episodes to improve the policy.</p> </li> </ol>"},{"location":"recitations/week3/#challenges-in-continuous-action-spaces","title":"Challenges in Continuous Action Spaces","text":"<ul> <li> <p>Exploration: In continuous action spaces, exploration can be more challenging because the agent must explore an infinite number of possible actions. One strategy to encourage exploration is to maintain some randomness in the policy (e.g., keeping the variance \\(\\sigma^2\\) large).</p> </li> <li> <p>Variance of Gradient Estimates: Policy gradient methods, including REINFORCE, can have high variance in the gradient estimates, especially in continuous action spaces. Techniques like baseline functions or actor-critic methods can help reduce this variance.</p> </li> </ul>"},{"location":"recitations/week3/#authors","title":"Author(s)","text":"<ul> <li> <p>SeyyedAli MirGhasemi</p> <p>Teaching Assistant</p> <p>sam717269@gmail.com</p> <p> </p> </li> </ul>"},{"location":"recitations/week4/","title":"Week 4: Advanced Methods","text":""},{"location":"recitations/week4/#screen-record","title":"Screen Record","text":""},{"location":"recitations/week4/#recitation-notes","title":"Recitation Notes","text":""},{"location":"recitations/week4/#actor_critic","title":"Actor_Critic","text":"<p>The variance of policymethods can originate from two sources:</p> <ol> <li>high variance in the cumulative reward estimate and </li> <li>high variance in the gradient estimate. </li> </ol> <p>For both problems, a solution has been developed: bootstrapping for better reward estimates and baseline subtraction to lower the variance of gradient estimates.</p> <p>In the next seciton we will review the concepts bootstrapping (reward to go), using baseline (Advantage value) and Generalized Advantage Estimation (GAE).</p>"},{"location":"recitations/week4/#reward-to-go","title":"Reward to Go:","text":"<p>A cumulative reward from state \\(s_t\\) to the end of the episode by applying policy \\(\\pi_\\theta\\).</p> <p>As mentioned earlier, in the policy gradient method, we update our policy weights with the learning rate \\(\\alpha\\) as follows:</p> \\[ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta), \\] <p>where</p> \\[ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t})\\cdot r(s_{i,t},a_{i,t}). \\] <p>In this equation, the term \\(r(s_{i,t}, a_{i,t})\\) is the primary source of variance and noise. We use the causality trick to mitigate this issue by multiplying the policy gradient at state \\(s_t\\) with its future rewards. It is important to note that the policy at state \\(s_t\\) can only affect future rewards, not past ones. The causality trick is represented as follows:</p> \\[ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1}\\bigg( \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t})\\bigg) \\bigg( \\sum^T_{t=1}r(s_{i,t},a_{i,t}) \\bigg) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t})\\bigg( \\sum^T_{t'=t}r(s_{i,t'},a_{i,t'}) \\bigg). \\] <p>The term \\(\\sum^T_{t'=t}r(s_{i,t'},a_{i,t'})\\) is known as reward to go, which is calculated in a Monte Carlo manner. It represents the total expected reward from a given state by applying policy \\(\\pi_\\theta\\), starting from time \\(t\\) to the end of the episode.</p> <p>To further reduce variance, we can approximate the reward to go with the Q-value, which conveys a similar meaning. Thus, we can rewrite \\(\\nabla_\\theta J(\\theta)\\) as:</p> \\[ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t}) Q(s_{i,t},a_{i,t}). \\]"},{"location":"recitations/week4/#advantage-value","title":"Advantage Value:","text":"<p>Measures how much an action is better than the average of other actions in a given state.</p> <p></p>"},{"location":"recitations/week4/#why-use-the-advantage-value","title":"Why Use the Advantage Value?","text":"<p>We can further reduce variance by subtracting a baseline from \\(Q(s_{i,t}, a_{i,t})\\) without altering the expectation of \\(\\nabla_\\theta J(\\theta)\\), making it an unbiased estimator:</p> \\[ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t}) \\bigg( Q(s_{i,t},a_{i,t}) - b_t \\bigg). \\] <p>A reasonable choice for the baseline is the expected reward. Although it is not optimal, it significantly reduces variance.</p> <p>We define:</p> \\[ Q(s_{i,t},a_{i,t}) = \\sum_{t'=t}^T E_{\\pi_\\theta}[r(s_{t'}, a_{t'})|s_t,a_t]. \\] <p>To ensure the baseline is independent of the action taken, we compute the expectation of \\(Q(s_{i,t}, a_{i,t})\\) over all actions sampled from the policy:</p> \\[ E_{a_t \\sim \\pi_\\theta(a_{i,t}|s_{i,t})} [Q(s_{i,t},a_{i,t})] = V(s_t) = b_t. \\] <p>Thus, the variance-reduced policy gradient equation becomes:</p> \\[ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t}) \\bigg( Q(s_{i,t},a_{i,t}) - V(s_t) \\bigg). \\] <p>We define the advantage function as:</p> \\[ A(s_t,a_t) = Q(s_{i,t},a_{i,t}) - V(s_t). \\]"},{"location":"recitations/week4/#understanding-the-advantage-function","title":"Understanding the Advantage Function","text":"<p>Consider a penalty shootout game to illustrate the concept of the advantage function and Q-values in reinforcement learning.</p> <p></p>"},{"location":"recitations/week4/#game-setup","title":"# Game Setup:","text":"<ul> <li>A goalie always jumps to the right to block the shot.</li> <li>A kicker can shoot either left or right with equal probability (0.5 each), defining the kicker's policy \\(\\pi_k\\).</li> </ul> <p>The reward matrix for the game is:</p> Kicker / Goalie Right (jumps right) Left (jumps left) Right (shoots right) 0,1 1,0 Left (shoots left) 1,0 0,1"},{"location":"recitations/week4/#expected-reward","title":"# Expected Reward:","text":"<p>Since the kicker selects left and right with equal probability, the expected reward is:</p> \\[ V^{\\pi_k}(s_t) = 0.5 \\times 1 + 0.5 \\times 0 = 0.5. \\]"},{"location":"recitations/week4/#q-value-calculation","title":"# Q-Value Calculation:","text":"<p>The Q-value is expressed as:</p> \\[ Q^{\\pi_k}(s_{i,t},a_{i,t}) = V^{\\pi_k}(s_t) + A^{\\pi_k}(s_t,a_t). \\] <ul> <li>If the kicker shoots right, the shot is always saved (\\(Q^{\\pi_k}(s_{i,t},r) = 0\\)).</li> <li>If the kicker shoots left, the shot is always successful (\\(Q^{\\pi_k}(s_{i,t},l) = 1\\)).</li> </ul>"},{"location":"recitations/week4/#advantage-calculation","title":"# Advantage Calculation:","text":"<p>The advantage function \\(A^{\\pi_k}(s_t,a_t)\\) measures how much better or worse an action is compared to the expected reward.</p> <ul> <li>If the kicker shoots left, he scores (reward = 1), which is 0.5 more than the expected reward \\(V^{\\pi_k}(s_t)\\). Thus, the advantage of shooting left is:</li> </ul> \\[ 1 = 0.5 + A^{\\pi_k}(s_t,l) \\Rightarrow A^{\\pi_k}(s_t,l) = 0.5. \\] <ul> <li>If the kicker shoots right, he fails (reward = 0), which is 0.5 less than the expected reward. Thus, the advantage of shooting right is:</li> </ul> \\[ 0 = 0.5 + A^{\\pi_k}(s_t,r) \\Rightarrow A^{\\pi_k}(s_t,r) = -0.5. \\]"},{"location":"recitations/week4/#estimating-the-advantage-value","title":"Estimating the Advantage Value","text":"<p>Instead of maintaining separate networks for estimating \\(V(s_t)\\) and \\(Q(s_{i,t}, a_{i,t})\\), we approximate \\(Q(s_{i,t}, a_{i,t})\\) using \\(V(s_t)\\):</p> \\[ Q(s_{i,t},a_{i,t}) = r(s_t, a_t) + \\sum_{t'=t+1}^T E_{\\pi_\\theta}[r(s_{t'}, a_{t'})|s_t,a_t] \\approx r(s_t, a_t) + V(s_{t+1}). \\] <p>Thus, we estimate the advantage function as:</p> \\[ A(s_{i,t},a_{i,t}) \\approx r(s_t, a_t) + V(s_{t+1}) - V(s_t). \\] <p>We can also, consider the advantage function with discount factor as:</p> \\[ A(s_{i,t},a_{i,t}) \\approx r(s_t, a_t) + \\gamma V(s_{t+1}) - V(s_t). \\] <p>To train the value estimator, we use Monte Carlo estimation.</p>"},{"location":"recitations/week4/#generalized-advantage-estimation-gae","title":"Generalized Advantage Estimation (GAE)","text":"<p>To have a good  balance between variance and bias, we can use the concept of GAE, which is firstly introduced in High-Dimensional Continuous Control Using Generalized Advantage Estimation. </p> <p>At the first, we define \\(\\hat{A}^{(k)}(s_{i,t},a_{i,t})\\) to understand this the GAE concept.</p> \\[ \\hat{A}^{(k)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \\dots + \\gamma^{k-1}r(s_{t+k-1}, a_{t+k-1}) + \\gamma^k V(s_{t+k})- V(s_t). \\] <p>So, we can write the \\(\\hat{A}^{(k)}(s_{i,t},a_{i,t})\\) for \\(k \\in \\{1, \\infty\\}\\) as:</p> \\[ \\hat{A}^{(1)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \\gamma V(s_{t+1}) - V(s_t) \\] \\[ \\hat{A}^{(2)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \\gamma r(s_{t+1}, a_{t+1}) + \\gamma^2 V(s_{t+2}) - V(s_t) \\] \\[ .\\\\ .\\\\ .\\\\ \\] \\[ \\hat{A}^{(\\infty)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \\gamma r(s_{t+1}, a_{t+1}) + \\gamma^2 r(s_{t+2}, a_{t+2})+ \\dots - V(s_t)\\\\ \\] <p>\\(\\hat{A}^{(1)}(s_{i,t},a_{i,t})\\) is high bias, low variance, whilst \\(\\hat{A}^{(\\infty)}(s_{i,t},a_{i,t})\\) is unbiased, high variance.</p> <p>We take a weighted average of all \\(\\hat{A}^{(k)}(s_{i,t},a_{i,t})\\) for \\(k \\in \\{1, \\infty\\}\\) with weight \\(w_k = \\lambda^{k-1}\\) to balance bias and variance. This is called Generalized Advantage Estimation (GAE). </p> \\[ \\hat{A}^{(GAE)}(s_{i,t},a_{i,t}) = \\frac{\\sum_{k =1}^T  w_k \\hat{A}^{(k)}(s_{i,t},a_{i,t})}{\\sum_k w_k}= \\frac{\\sum_{k =1}^T \\lambda^{k-1} \\hat{A}^{(k)}(s_{i,t},a_{i,t})}{\\sum_k w_k} \\]"},{"location":"recitations/week4/#actor_critic-algorihtms","title":"Actor_Critic Algorihtms","text":""},{"location":"recitations/week4/#batch-actor-critic-algorithm","title":"Batch actor-critic algorithm","text":"<p>The first algorithm is: Actor-Critic with Bootstrapping and Baseline Subtraction In this algorithm, the simulator runs for an entire episode before updating the policy.</p> <p>Batch actor-critic algorithm:</p> <ol> <li>for each episode do:</li> <li> for each step do:</li> <li>\u2003\u2003Take action \\(a_t \\sim \\pi_{\\theta}(a_t | s_t)\\), get \\((s_t,a_t,s'_t,r_t)\\).</li> <li>\u2003Fit \\(\\hat{V}(s_t)\\) with sampled rewards.</li> <li>\u2003Evaluate the advantage function: \\(A({s_t, a_t})\\)</li> <li>\u2003Compute the policy gradient: \\(\\nabla_{\\theta} J(\\theta) \\approx \\sum_{i} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_i | s_i) A({s_t})\\)</li> <li>\u2003Update the policy parameters:  \\(\\theta \\gets \\theta + \\alpha \\nabla_{\\theta} J(\\theta)\\)</li> </ol> <p>Running full episodes for a single update is inefficient as it requires a significant amount of time. To address this issue, the online actor-critic algorithm is proposed.</p>"},{"location":"recitations/week4/#online-actor-critic-algorithm","title":"Online actor-critic algorithm","text":"<p>In this algorithm, we take an action in the environment and immediately apply an update using that action.</p> <p>Online actor-critic algorithm</p> <ol> <li>for each episode do:</li> <li> for each step do:</li> <li>\u2003\u2003Take action \\(a_t \\sim \\pi_{\\theta}(a_t | s_t)\\), get \\((s_t,a_t,s'_t,r_t)\\).</li> <li>\u2003\u2003Fit \\(\\hat{V}(s_t)\\) with the sampled reward.</li> <li>\u2003\u2003Evaluate the advantage function: \\(A({s,a})\\)</li> <li>\u2003\u2003Compute the policy gradient: \\(\\nabla_{\\theta} J(\\theta) \\approx  \\nabla_{\\theta} \\log \\pi_{\\theta}(a | s) A({s,a})\\)</li> <li>\u2003\u2003Update the policy parameters: \\(\\theta \\gets \\theta + \\alpha \\nabla_{\\theta} J(\\theta)\\)</li> </ol> <p>Training neural networks with a batch size of 1 leads to high variance, making the training process unstable.</p> <p>To mitigate this issue, two main solutions are commonly used: 1. Parallel Actor-Critic (Online) 2. Off-Policy Actor-Critic</p>"},{"location":"recitations/week4/#parallel-actor-critic-online","title":"Parallel Actor-Critic (Online)","text":"<p>Many high-performance implementations are based on the actor critic approach. For large problems, the algorithm is typically parallelized and implemented on a large cluster computer.</p> <p></p> <p>To reduce variance, multiple actors are used to update the policy. There are two main approaches:</p> <ul> <li>Synchronized Parallel Actor-Critic: All actors run synchronously, and updates are applied simultaneously. However, this introduces synchronization overhead, making it impractical in many cases.</li> <li>Asynchronous Parallel Actor-Critic: Each actor applies its updates independently, reducing synchronization constraints and improving computational efficiency. It also, uses asynchronous (parallel and distributed) gradient descent for optimization of deep neural network controllers.</li> </ul>"},{"location":"recitations/week4/#off-policy-actor-critic-algorithm","title":"Off-Policy Actor-Critic Algorithm","text":"<p>In the off-policy approach, we maintain a replay buffer to store past experiences, allowing us to train the model using previously collected data rather than relying solely on the most recent experience.</p> <p></p> <p>Off-policy actor-critic algorithm:</p> <ol> <li>for each episode do:</li> <li> for multiple steps do:</li> <li>\u2003\u2003Take action \\(a \\sim \\pi_{\\theta}(a | s)\\), get \\((s,a,s',r)\\), store in \\(\\mathcal{R}\\).</li> <li>\u2003Sample a batch \\(\\{s_i, a_i, r_i, s'_i \\}\\) for buffer \\(\\mathcal{R}\\).</li> <li>\u2003Fit \\(\\hat{Q}^{\\pi}(s_i, a_i)\\) for each \\(s_i, a_i\\).</li> <li>\u2003Compute the policy gradient: \\(\\nabla_{\\theta} J(\\theta) \\approx \\frac{1}{N} \\sum_{i} \\nabla_{\\theta} \\log \\pi_{\\theta}(a^{\\pi}_i | s_i) \\hat{Q}^{\\pi}(s_i, a^{\\pi}_i)\\)</li> <li>\u2003Update the policy parameters: \\(\\theta \\gets \\theta + \\alpha \\nabla_{\\theta} J(\\theta)\\)</li> </ol> <p>To work with off-policy methods, we use the Q-value instead of the V-value in step 3. In step 4, rather than using the advantage function, we directly use \\(\\hat{Q}^{\\pi}(s_i, a^{\\pi}_i)\\), where \\(a^{\\pi}_i\\)  is sampled from the policy \\(\\pi\\). By using the Q-value instead of the advantage function, we do not encounter the high-variance problem typically associated with single-step updates. This is because we sample a batch from the replay buffer, which inherently reduces variance. As a result, there is no need to compute an explicit advantage function for variance reduction.</p>"},{"location":"recitations/week4/#issues-with-standard-policy-gradient-methods","title":"Issues with Standard Policy Gradient Methods","text":"<p>Earlier policy gradient methods, such as Vanilla Policy Gradient (VPG) or REINFORCE, suffer from high variance and instability in training. A key problem is that large updates to the policy can lead to drastic performance degradation.</p> <p>To address these issues, Trust Region Policy Optimization (TRPO) was introduced, enforcing a constraint on how much the policy can change in a single update. However, TRPO is computationally expensive because it requires solving a constrained optimization problem. PPO is a simpler and more efficient alternative to TRPO, designed to ensure stable policy updates without requiring complex constraints.</p>"},{"location":"recitations/week4/#proximal-policy-optimization-ppo","title":"Proximal Policy Optimization (PPO)","text":""},{"location":"recitations/week4/#the-intuition-behind-ppo","title":"The intuition behind PPO","text":"<p>The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability of the policy by limiting the change you make to the policy at each training epoch: we want to avoid having too large policy updates.  why?</p> <ol> <li>We know empirically that smaller policy updates during training are more likely to converge to an optimal solution.</li> <li>If we change the policy too much, we may end up with a bad policy that cannot be improved.</li> </ol> <p></p> <p>soruce: Unit 8, of the Deep Reinforcement Learning Class with Hugging Face</p> <p>Therefore, in order not to allow the current policy to change much compared to the previous policy, we limit the ratio of these two policies to  \\([1 - \\epsilon, 1 + \\epsilon]\\).</p>"},{"location":"recitations/week4/#the-clipped-surrogate-objective","title":"the Clipped Surrogate Objective","text":"\\[ L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip} \\left( r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_t \\right) \\right] \\]"},{"location":"recitations/week4/#the-ratio-function","title":"The ratio Function","text":"\\[ r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\] <p>\\(r_{\\theta}\\) denotes the probability ratio between the current and old policy. if \\(r_{\\theta} &gt; 1\\), then the probability of doing action \\(a_t\\) at \\(s_t\\) in current policy is higher than the old policy and vice versa.</p> <p>So this probability ratio is an easy way to estimate the divergence between old and current policy.</p>"},{"location":"recitations/week4/#the-clipped-part","title":"The clipped part","text":"\\[ \\text{clip} \\left( r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_t \\] <p>If the current policy is updated significantly, such that the new policy parameters \\(\\theta'\\)  diverge greatly from the previous ones, the probability ratio between the new and old policies is clipped to the bounds  \\(1 - \\epsilon\\), \\(1 + \\epsilon\\). At this point, the derivative of the objective function becomes zero, effectively preventing further updates. </p>"},{"location":"recitations/week4/#the-unclipped-part","title":"The unclipped part","text":"\\[ r_t(\\theta) \\hat{A}_t \\] <p>In the context of optimization, if the initial starting point is not ideal\u2014i.e., if the probability ratio between the new and old policies is outside the range of \\(1 - \\epsilon\\) and \\(1 + \\epsilon\\)\u2014the ratio is clipped to these bounds. This clipping results in the derivative of the objective function becoming zero, meaning no gradient is available for updates. </p> <p>In this formulation, the optimization is performed with respect to the new policy parameters \\(\\theta'\\), and \\(A\\) represents the advantage function, which indicates how much better or worse the action performed is compared to the average return.</p> <ul> <li>Case 1: Positive Advantage (\\(A &gt; 0\\))</li> </ul> <p>if the Advantage \\(A\\) is positive (indicating that the action taken has a higher return than the expected return), and \\(\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} &lt; 1-\\epsilon\\) , the unclipped part is less than the clipped part and then it is minimized, so we have gradient to update the policy. This allows the policy to increase the probability of the action, aiming for the ratio to reach \\(1 + \\epsilon\\) without violating the clipping constraint.</p> <ul> <li>Case 2: Negative Advantage (\\(A &lt; 0\\))</li> </ul> <p>On the other hand, if the Advantage \\(A\\) is negative (meaning the action taken is worse than the average return), and \\(\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} &gt; 1+\\epsilon\\), the unclipped objective is again minimized and the gradient is non-zero, leading to an update. In this case, since the Advantage is negative, the policy is adjusted to reduce the probability of selecting that action, bringing the ratio closer to the boundary (\\(1-\\epsilon\\)), while ensuring that the new policy does not deviate too much from the old one.</p>"},{"location":"recitations/week4/#visualize-the-clipped-surrogate-objective","title":"Visualize the Clipped Surrogate Objective","text":""},{"location":"recitations/week4/#ppo-pseudocode","title":"PPO pseudocode","text":""},{"location":"recitations/week4/#algorithm-1-ppo-clip","title":"Algorithm 1: PPO-Clip","text":"<ol> <li>Input: initial policy parameters \\(\\theta_0\\), initial value function parameters \\(\\phi_0\\) </li> <li>for \\(k = 0, 1, 2, \\dots\\) do </li> <li>\u2003 Collect set of trajectories \\(\\mathcal{D}_k = \\{\\tau_i\\}\\) by running policy \\(\\pi_k = \\pi(\\theta_k)\\) in the environment.  </li> <li>\u2003 Compute rewards-to-go \\(\\hat{R}_t\\).  </li> <li>\u2003 Compute advantage estimates, \\(\\hat{A}_t\\) (using any method of advantage estimation) based on the current value function \\(V_{\\phi_k}\\).  </li> <li> <p>\u2003 Update the policy by maximizing the PPO-Clip objective:  </p> \\[ \\theta_{k+1} = \\arg \\max_{\\theta} \\frac{1}{|\\mathcal{D}_k| T} \\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^{T} \\min \\left( \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_k}(a_t | s_t)} A^{\\pi_{\\theta_k}}(s_t, a_t), \\, g(\\epsilon, A^{\\pi_{\\theta_k}}(s_t, a_t)) \\right) \\] <p>typically via stochastic gradient ascent with Adam.  </p> </li> <li> <p>\u2003 Fit value function by regression on mean-squared error:  </p> \\[ \\phi_{k+1} = \\arg \\min_{\\phi} \\frac{1}{|\\mathcal{D}_k| T} \\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^{T} \\left( V_{\\phi}(s_t) - \\hat{R}_t \\right)^2 \\] </li> <li> <p>end for</p> </li> </ol>"},{"location":"recitations/week4/#challenges-of-ppo-algorithm","title":"Challenges of PPO algorithm","text":"<p>PPO requires a significant amount of interactions with the environment to converge. This can be problematic in real-world applications where data is expensive or difficult to collect. in fact it is a sample inefficient algorithm.</p> <ol> <li> <p>Hyperparameter Sensitivity:   PPO requires careful tuning of hyperparameters such as the clipping parameter (epsilon), learning rate, and the number of updates per iteration. Poorly chosen hyperparameters can lead to suboptimal performance or even failure to converge.</p> </li> <li> <p>Sample Efficiency: Although PPO is more sample-efficient than some other policy gradient methods, it still requires a significant amount of data to achieve good performance. This can be problematic in environments where data collection is expensive or time-consuming.</p> </li> <li> <p>Exploration-Exploitation Tradeoff: PPO, like other policy gradient methods, can struggle with balancing exploration and exploitation. It may prematurely converge to suboptimal policies if it fails to explore sufficiently.</p> </li> </ol>"},{"location":"recitations/week4/#helpful-links","title":"Helpful links","text":"<p>Unit 8, of the Deep Reinforcement Learning Class with Hugging Face</p> <p>Proximal Policy Optimization (PPO) Explained</p> <p>Proximal Policy Optimization (PPO) - How to train Large Language Models</p> <p>Proximal Policy Optimization</p> <p>Understanding PPO: A Game-Changer in AI Decision-Making Explained for RL Newcomers</p> <p>Proximal Policy Optimization (PPO) - Explained</p>"},{"location":"recitations/week4/#deep-deterministic-policy-gradients-ddpg","title":"Deep Deterministic Policy Gradients (DDPG)","text":""},{"location":"recitations/week4/#handling-continuous-action-spaces","title":"Handling Continuous Action Spaces","text":""},{"location":"recitations/week4/#why-is-this-a-problem","title":"Why is this a problem?","text":"<p>In discrete action spaces, methods like DQN (Deep Q-Networks) can use an action-value function \\(Q(s, a)\\) to select the best action. However, in continuous action spaces, selecting the optimal action requires solving a high-dimensional optimization problem at every step, which is computationally expensive.</p>"},{"location":"recitations/week4/#how-does-ddpg-solve-it","title":"How does DDPG solve it?","text":"<p>DDPG uses a deterministic policy network \\(\\pi(s)\\), which directly maps states to actions, eliminating the need for iterative optimization over action values.</p>"},{"location":"recitations/week4/#ddpg-architecture","title":"DDPG Architecture","text":"<p>DDPG uses four neural networks: - A Q network - A deterministic policy network - A target Q network - A target policy network </p> <p>The Q network and policy network are similar to Advantage Actor-Critic (A2C), but in DDPG, the Actor directly maps states to actions (the output of the network directly represents the action) instead of outputting a probability distribution over a discrete action space.</p> <p>The target networks are time-delayed copies of their original networks that slowly track the learned networks. Using these target value networks greatly improves stability in learning.  </p>"},{"location":"recitations/week4/#why-use-target-networks","title":"Why Use Target Networks?","text":"<p>In methods without target networks, the update equations of the network depend on the network's own calculated values, making it prone to divergence.  </p> <p>For example, if we update the Q-values directly using the current network, errors can compound, leading to instability. The target networks help mitigate this issue by providing more stable targets for updates. </p> \\[     Q(s_t, a_t) \\leftarrow r_t + \\gamma Q(s_{t+1}, \\arg\\max_{a'} Q(s_{t+1}, a')) \\]"},{"location":"recitations/week4/#breakdown-of-ddpg-components","title":"Breakdown of DDPG Components","text":"<ol> <li>Experience Replay </li> <li>Actor &amp; Critic Network Updates </li> <li>Target Network Updates </li> <li>Exploration </li> </ol>"},{"location":"recitations/week4/#replay-buffer","title":"Replay Buffer","text":"<p>As used in Deep Q-Learning and other RL algorithms, DDPG also utilizes a replay buffer to store experience tuples:  </p> \\[ (state, action, reward, next\\_state) \\] <p>These tuples are stored in a finite-sized cache (replay buffer). During training, random mini-batches are sampled from this buffer to update the value and policy networks.</p> <ul> <li> <p>Why Use Experience Replay?</p> <p>In optimization tasks, we want data to be independently distributed. However, in an on-policy learning process, the collected data is highly correlated.  </p> </li> </ul> <p>By storing experience in a replay buffer and sampling random mini-batches for training, we break correlations and improve learning stability.</p>"},{"location":"recitations/week4/#actor-policy-critic-value-network-updates","title":"Actor (Policy) &amp; Critic (Value) Network Updates","text":"<ul> <li>Value Network (Critic) Update  </li> </ul> <p>The value network is updated similarly to Q-learning. The updated Q-value is obtained using the Bellman equation:</p> \\[ y_i = r_i + \\gamma Q' \\left (s_{i+1}, \\mu' (s_{i+1}|\\theta^{\\mu'})|\\theta^{Q'} \\right) \\] <p>However, in DDPG, the next-state Q-values are calculated using the target Q network and target policy network.  </p> <p>Then, we minimize the mean squared error (MSE) loss between the updated Q-value and the original Q-value:</p> \\[ \\mathcal{L} = \\frac{1}{N} \\sum_{i} \\left(y_i -Q(s_i, a_i|\\theta^Q) \\right)^2 \\] <p>Note: The original Q-value is calculated using the learned Q-network, not the target Q-network.</p> <ul> <li>Policy Network (Actor) Update  </li> </ul> <p>The policy function aims to maximize the expected return:</p> \\[ J(\\theta) = \\mathbb{E} \\left[ Q(s, a) \\mid s = s_t, a_t = \\mu(s_t) \\right] \\] <p>The policy loss is computed by differentiating the objective function with respect to the policy parameters:</p> \\[ \\nabla_{\\theta^\\mu} J(\\theta) = \\nabla_a Q(s, a) |_{a = \\mu(s)} \\nabla_{\\theta^\\mu} \\mu(s|\\theta^\\mu) \\] <p>But since we are updating the policy in an off-policy way with batches of experience, we take the mean of the sum of gradients calculated from the mini-batch:</p> \\[ \\nabla_{\\theta_\\mu} J(\\theta) \\approx \\frac{1}{N} \\sum_i \\left[  \\left. \\nabla_a Q(s, a \\mid \\theta^Q) \\right|_{s = s_i, a = \\mu(s_i)}  \\nabla_{\\theta_\\mu} \\mu(s \\mid \\theta^\\mu) \\Big|_{s = s_i}  \\right] \\]"},{"location":"recitations/week4/#target-network-updates","title":"Target Network Updates","text":"<p>The target networks are updated via soft updates instead of direct copying:</p> \\[ \\theta^{Q'} \\leftarrow \\tau \\theta^{Q} + (1 - \\tau) \\theta^{Q'} \\] \\[ \\theta^{\\mu'} \\leftarrow \\tau \\theta^{\\mu} + (1 - \\tau) \\theta^{\\mu'} \\] <p>where \\(\\tau\\) is a small value , ensuring smooth updates that prevent instability.</p>"},{"location":"recitations/week4/#exploration","title":"Exploration","text":"<p>In RL for discrete action spaces, exploration is often done using epsilon-greedy or Boltzmann exploration.  </p> <p>However, in continuous action spaces, exploration is done by adding noise to the action itself.  </p>"},{"location":"recitations/week4/#ornstein-uhlenbeck-process","title":"Ornstein-Uhlenbeck Process","text":"<p>The DDPG paper proposes adding Ornstein-Uhlenbeck (OU) noise to the actions.  </p> <p>The OU Process generates temporally correlated noise, preventing the noise from canceling out or \"freezing\" the action dynamics.</p> \\[ \\mu^{'}(s_t) = \\mu(s_t|\\theta^\\mu_t) + \\mathcal{N} \\] <p></p> <p></p>"},{"location":"recitations/week4/#ddpg-pseudocode-a-step-by-step-breakdown","title":"DDPG Pseudocode: A Step-by-Step Breakdown","text":""},{"location":"recitations/week4/#algorithm-1-ddpg-algorithm","title":"Algorithm 1: DDPG Algorithm","text":"<p>Randomly initialize critic network \\( Q(s, a | \\theta^Q) \\) and actor \\( \\mu(s | \\theta^\\mu) \\) with weights \\( \\theta^Q \\) and \\( \\theta^\\mu \\). Initialize target networks \\( Q' \\) and \\( \\mu' \\) with weights \\( \\theta^{Q'} \\gets \\theta^Q, \\quad \\theta^{\\mu'} \\gets \\theta^\\mu \\). Initialize replay buffer \\( R \\).  </p> <p>for episode = 1 to \\( M \\) do \u00a0\u00a0Initialize a random process \\( \\mathcal{N} \\) for action exploration. \u00a0\u00a0Receive initial observation state \\( s_1 \\). for \\( t = 1 \\) to \\( T \\) do \u00a0\u00a0\u00a0\u00a0Select action \\( a_t = \\mu(s_t | \\theta^\\mu) + \\mathcal{N}_t \\) according to the current policy and exploration noise. \u00a0\u00a0\u00a0\u00a0Execute action \\( a_t \\) and observe reward \\( r_t \\) and new state \\( s_{t+1} \\). \u00a0\u00a0\u00a0\u00a0Store transition \\( (s_t, a_t, r_t, s_{t+1}) \\) in \\( R \\). \u00a0\u00a0\u00a0\u00a0Sample a random minibatch of \\( N \\) transitions \\( (s_i, a_i, r_i, s_{i+1}) \\) from \\( R \\). \u00a0\u00a0\u00a0\u00a0Set:  </p> \\[   y_i = r_i + \\gamma Q'(s_{i+1}, \\mu'(s_{i+1} | \\theta^{\\mu'}) | \\theta^{Q'})   \\] <p>\u00a0\u00a0\u00a0\u00a0Update critic by minimizing the loss:  </p> \\[   L = \\frac{1}{N} \\sum_i (y_i - Q(s_i, a_i | \\theta^Q))^2   \\] <p>\u00a0\u00a0\u00a0\u00a0Update actor policy using the sampled policy gradient: </p> \\[   \\nabla_{\\theta^\\mu} J \\approx \\frac{1}{N} \\sum_i \\nabla_a Q(s, a | \\theta^Q) |_{s=s_i, a=\\mu(s_i)} \\nabla_{\\theta^\\mu} \\mu(s | \\theta^\\mu) |_{s_i}   \\] <p>\u00a0\u00a0\u00a0\u00a0Update the target networks:  </p> \\[   \\theta^{Q'} \\gets \\tau \\theta^Q + (1 - \\tau) \\theta^{Q'}   \\] \\[   \\theta^{\\mu'} \\gets \\tau \\theta^\\mu + (1 - \\tau) \\theta^{\\mu'}   \\] <p> end for end for </p>"},{"location":"recitations/week4/#soft-actor-critic-sac","title":"Soft Actor-Critic (SAC)","text":""},{"location":"recitations/week4/#challenges-and-motivation-of-sac","title":"Challenges and motivation of SAC","text":"<ol> <li>Previous Off-policy methods like DDPG often struggle with exploration , leading to suboptimal policies. SAC overcomes this by introducing entropy maximization, which encourages the agent to explore more efficiently.</li> <li>Sample inefficiency is a major issue in on-policy algorithms like Proximal Policy Optimization (PPO), which require a large number of interactions with the environment. SAC, being an off-policy algorithm, reuses past experiences stored in a replay buffer, making it significantly more sample-efficient.</li> <li>Another challenge is instability in learning, as methods like DDPG can suffer from overestimation of Q-values. SAC mitigates this by employing twin Q-functions (similar to TD3) and incorporating entropy regularization, leading to more stable and robust learning.</li> </ol> <p>In essence, SAC seeks to maximize the entropy in policy, in addition to the expected reward from the environment. The entropy in policy can be interpreted as randomness in the policy.</p>"},{"location":"recitations/week4/#what-is-entropy","title":"what is entropy?","text":"<p>We can think of entropy as how unpredictable a random variable is. If a random variable always takes a single value then it has zero entropy because it\u2019s not unpredictable at all. If a random variable can be any Real Number with equal probability then it has very high entropy as it is very unpredictable.</p> <p></p> <p>probability distributions with low entropy have a tendency to greedily sample certain values, as the probability mass is distributed relatively unevenly.</p>"},{"location":"recitations/week4/#maximum-entropy-reinforcement-learning","title":"Maximum Entropy Reinforcement Learning","text":"<p>In Maximum Entropy RL, the agent tries to optimize the policy to choose the right action that can receive the highest sum of reward and long term sum of entropy. This enables the agent to explore more and avoid converging to local optima.</p> <p><code>reason</code>: We want a high entropy in our policy to encourage the policy to assign equal probabilities to actions that have same or nearly equal Q-values(allow the policy to capture multiple modes of good policies), and also to ensure that it does not collapse into repeatedly selecting a particular action that could exploit some inconsistency in the approximated Q function. Therefore, SAC overcomes the  problem by encouraging the policy network to explore and not assign a very high probability to any one part of the range of actions.</p> <p>The objective function of the Maximum entropy RL is as shown below:</p> \\[ J(\\pi_{\\theta}) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) + \\alpha H(\\pi(\\cdot | s_t)) \\right] \\] <p>and the optimal policy is:</p> \\[ \\pi^* = \\arg\\max_{\\pi_{\\theta}} \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) + \\alpha H(\\pi(\\cdot | s_t)) \\right] \\] <p>\\(\\alpha\\) is the temperature parameter that balances between exploration and exploitation.</p>"},{"location":"recitations/week4/#overcoming-exploration-bias-in-multimodal-q-functions","title":"Overcoming Exploration Bias in Multimodal Q-Functions","text":"<p>Here we want to explain the concept of a multimodal Q-function in reinforcement learning (RL), where the Q-value function, \\(Q(s, a)\\), represents the expected cumulative reward for taking action \\(a\\) in state \\(s\\). </p> <p>In this context, the robot is in an initial state and has two possible passages to follow, which result in a bimodal Q-function (a function with two peaks). These peaks correspond to two different action choices, each leading to different potential outcomes.</p>"},{"location":"recitations/week4/#standard-rl-approach","title":"Standard RL Approach","text":"<p>The grey curve represents the Q-function, which has two peaks, indicating two promising actions.A conventional RL approach typically assumes a unimodal (single-peaked) policy distribution, represented by the red curve.</p> <p>This policy distribution is modeled as a Gaussian \\(\\mathcal{N}(\\mu(s_t), \\Sigma)\\) centered around the highest Q-value peak.</p> <p>This setup results in exploration bias where the agent primarily explores around the highest peak and ignores the lower peak entirely.</p>"},{"location":"recitations/week4/#improved-exploration-strategy","title":"Improved Exploration Strategy","text":"<p>Instead of using a unimodal Gaussian policy, a Boltzmann-weighted policy is used. The policy distribution (green shaded area) is proportional to \\(\\exp(Q(s_t, a_t))\\), meaning actions are sampled based on their Q-values. </p> <p>This approach allows the agent to explore multiple high-reward actions and avoids the bias of ignoring one passage. As a result, the agent recognizes both options, increasing the chance of finding the optimal path.</p> <p></p> <p>This concept is relevant for actor-critic RL methods like Soft Actor-Critic (SAC), which uses entropy to encourage diverse exploration.</p>"},{"location":"recitations/week4/#soft-policy","title":"Soft Policy","text":"<ul> <li>Soft policy </li> </ul> \\[ J(\\pi) = \\sum_{t=0}^{T} \\mathbb{E}_{(s_t, a_t)\\sim\\rho_{\\pi}} \\left[ r(s_t, a_t) + \\alpha\\mathcal{H}(\\pi(\\cdot | s_t)) \\right] \\] <p>With new objective function we need to define Value funciton and Q-value funciton again. </p> <ul> <li>Soft Q-value funciton</li> </ul> \\[ Q(s_t, a_t) = r(s_t, a_t) + \\gamma\\mathbb{E}_{s_{t+1}\\sim p}\\left[ V(s_{t+1}) \\right] \\] <ul> <li>Soft Value function</li> </ul> \\[ V(s_t) = \\mathbb{E}_{a_t\\sim \\pi} \\left[Q(s_t, a_t) - \\text{log}\\space\\pi(a_t|s_t)\\right] \\]"},{"location":"recitations/week4/#soft-policy-iteration","title":"Soft policy iteration","text":"<p>Soft Policy Iteration is an entropy-regularized version of classical policy iteration, which consists of:</p> <ol> <li>Soft Policy Evaluation: Estimating the soft Q-value function under the current policy.</li> <li>Soft Policy Improvement:  Updating the policy to maximize the soft Q-value function,incorporating entropy regularization.</li> </ol> <p>This process iteratively improves the policy while balancing exploration and exploitation.</p>"},{"location":"recitations/week4/#soft-policy-evaluation-critic-update","title":"Soft Policy Evaluation (Critic Update)","text":"<p>The goal of soft policy evaluation is to compute the expected return of a given policy \\(\\pi\\) under the maximum entropy objective, which modifies the standard Bellman equation by adding an entropy term. (SAC explicitly learns the Q-function for the current policy)</p> <p>The soft Q-value function for a policy \\(\\pi\\) is updated using a modified Bellman operator \\(T^{\\pi}\\):</p> \\[ T^\\pi Q(s_t, a_t) = r(s_t, a_t) + \\gamma \\mathbb{E}_{s_{t+1} \\sim p} [V(s_{t+1})] \\] <p>with substitution of \\(V\\) we have :</p> \\[ T^\\pi Q(s_t, a_t) = r(s_t, a_t) + \\gamma \\mathbb{E}_{\\substack{s_{t+1} \\sim p \\\\ a_{t+1} \\sim \\pi}} [Q(s_{t+1,}, a_{t+1}) - \\text{log}\\space\\pi(a_{t+1}|s_{t+1})] \\] <p>Key Result: Soft Bellman Backup Convergence </p> <p>Theorem: By repeatedly applying the operator \\(T^\\pi\\), the Q-value function converges to the true soft Q-value function for policy \\(\\pi\\):  </p> \\[ Q_k \\to Q^\\pi \\text{ as } k \\to \\infty \\] <p>Thus, we can estimate \\(Q^\\pi\\) iteratively.</p>"},{"location":"recitations/week4/#soft-policy-improvement-actor-update","title":"Soft Policy Improvement (Actor Update)","text":"<p>Once the Q-function is learned, we need to improve the policy using a gradient-based update. This means:</p> <ul> <li>Instead of directly maximizing Q-values, the policy is updated to optimize a modified objective that balances reward maximization and exploration.</li> <li>The update is off-policy, meaning the policy can be trained using past experiences stored in a replay buffer, rather than requiring fresh samples like on-policy methods (e.g., PPO).</li> </ul> <p>The update is based on an exponential function of the Q-values:</p> \\[ \\pi^*(a | s) \\propto \\exp (Q^\\pi(s, a)) \\] <p>which means the optimal policy is obtained by normalizing \\(\\exp (Q^\\pi(s, a))\\) over all actions:</p> \\[ \\pi^*(a | s) = \\frac{\\exp (Q^\\pi(s, a))}{Z^\\pi(s)} \\] <p>where \\(Z^\\pi(s)\\) is the partition function that ensures the distribution sums to 1.</p> <p>For the policy improvement step, we update the policy distribution towards the softmax distribution for the current Q function.</p> \\[ \\pi_{\\text{new}} = \\arg \\min_{\\pi' \\in \\Pi} D_{\\text{KL}} \\left( \\pi'(\\cdot | s) \\, \\bigg|\\bigg| \\, \\frac{\\exp (Q^\\pi(s, \\cdot))}{Z^\\pi(s)} \\right) \\] <p>Key Result: Soft Policy Improvement Theorem The new policy \\(\\pi_{\\text{new}}\\) obtained via this update improves the expected soft return:</p> \\[ Q^{\\pi_{\\text{new}}}(s, a) \\geq Q^{\\pi_{\\text{old}}}(s, a) \\quad \\forall (s, a) \\] <p>Thus, iterating this process leads to a better policy.</p>"},{"location":"recitations/week4/#convergence-of-soft-policy-iteration","title":"Convergence of Soft Policy Iteration","text":"<p>By alternating between soft policy evaluation and soft policy improvement, soft policy iteration converges to an optimal maximum entropy policy within the policy class \\(\\Pi\\):</p> \\[ \\pi^* = \\arg \\max_{\\pi \\in \\Pi} \\sum_t \\mathbb{E}[r_t + \\alpha H(\\pi(\\cdot | s_t))] \\] <p>However, this exact method is only feasible in the tabular setting. For continuous control, we approximate it using function approximators.</p>"},{"location":"recitations/week4/#soft-actor-critic","title":"Soft Actor-Critic","text":"<p>For complex learning domains with high-dimensional and/or continuous state-action spaces, it is mostly impossible to find exact solutions for the MDP. Thus, we must leverage function approximation (i.e. neural networks) to find a practical approximation to soft policy iteration. then we use stochastic gradient descent (SGD) to update parameters of these networks.</p> <p>we model the value functions as expressive neural networks, and the policy as a Gaussian distribution over the action space with the mean and covariance given as neural network outputs with the current state as input.</p>"},{"location":"recitations/week4/#soft-value-function-v_psis","title":"Soft Value function (\\(V_{\\psi}(s)\\))","text":"<p>A separate soft value function which helps in stabilising the training process. The soft value function approximator minimizes the squared residual error as follows:</p> \\[ J_V(\\psi) = \\mathbb{E}_{s_{t} \\sim \\mathcal{D}} \\left[ \\frac{1}{2} \\left( V_{\\psi}(s_t) - \\mathbb{E}_{a \\sim \\pi_{\\phi}} [Q_{\\theta}(s_t, a_t) - \\log \\pi_{\\phi}(a_t | s_t)] \\right)^2 \\right] \\] <p>It means the learning of the state-value function, \\(V\\) is done by minimizing the squared difference between the prediction of the value network and expected prediction of Q-function with the entropy of the policy, \\(\\pi\\). - \\(D\\) is the distribution of previously sampled states and actions, or a replay buffer.</p> <p>Gradient Update for \\(V_{\\psi}(s)\\)</p> \\[ \\hat{\\nabla}_{\\psi} J_V(\\psi) = \\nabla_{\\psi} V_{\\psi}(s_t) \\left( V_{\\psi}(s_t) - Q_{\\theta}(s_t, a_t) + \\log \\pi_{\\phi}(a_t | s_t) \\right) \\] <p>where the actions are sampled according to the current policy, instead of the replay buffer.</p>"},{"location":"recitations/week4/#soft-q-funciton-q_thetas-a","title":"Soft Q-funciton (\\(Q_{\\theta}(s, a)\\))","text":"<p>We minimize the soft Q-function parameters by using the soft Bellman residual provided here:</p> \\[ J_Q(\\theta) = \\mathbb{E}_{(s_{t}, a_t) \\sim \\mathcal{D}} \\left[ \\frac{1}{2} \\left( Q_{\\theta}(s_t, a_t) - \\hat{Q}(s_t, a_t)\\right)^2 \\right] \\] <p>with : </p> \\[ \\hat{Q}(s_t, a_t) = r(s_t, a_t) + \\gamma \\space \\mathbb{E}_{s_{t+1} \\sim p} [V_{\\bar{\\psi}}(s_{t+1})] \\] <p>Gradient Update for \\(Q_{\\theta}\\):</p> \\[ \\hat{\\nabla}_\\theta J_Q(\\theta) = \\nabla_{\\theta} Q_{\\theta}(s_t, a_t) \\left( Q_{\\theta}(s_t, a_t) - r(s_t, a_t) - \\gamma V_{\\bar{\\psi}}(s_{t+1}) \\right) \\] <p>A target value function \\(V_{\\bar{\\psi}}\\) (exponentially moving average of \\(V_{\\psi}\\)) is used to stabilize training.</p> <p>More Explanation About Target Networks: <p>The use of target networks is motivated by a problem in training \\(V\\) network. If you go back to the objective functions in the Theory section, you will find that the target for the \\(Q\\) network training depends on the \\(V\\) Network and the target for the \\(V\\) Network depends on the \\(Q\\) network (this makes sense because we are trying to enforce Bellman Consistency between the two functions). Because of this, the \\(V\\) network has a target that\u2019s indirectly dependent on itself which means that the \\(V\\) network\u2019s target depends on the same parameters we are trying to train. This makes training very unstable.</p> <p>The solution is to use a set of parameters which comes close to the parameters of the main \\(V\\) network, but with a time delay. Thus we create a second network which lags the main network called the target network. There are two ways to go about this.</p> <ol> <li>The first way is to have the target network copied over from the main network regularly after a set number of steps -&gt; Periodic Hard Update</li> </ol> \\[ \\theta^{-} \\leftarrow \\theta \\] <ol> <li>The other way is to update the target network by Polyak averaging (a kind of moving averaging) itself and the main network. -&gt; Soft Update</li> </ol> \\[ \\theta^{-} \\leftarrow \\tau \\theta + (1-\\tau) \\theta^{-} \\] <p> </p> <p>source: concept target network in category reinforcement learning</p>"},{"location":"recitations/week4/#policy-network-pi_phias","title":"Policy network (\\(\\pi_{\\phi}(a|s)\\))","text":"<p>The policy \\(\\pi_{\\phi}(a | s)\\) is updated using the soft policy improvement step, minimizing the KL-divergence:</p> \\[ J_{\\pi}(\\phi) = \\mathbb{E}_{s_t \\sim \\mathcal{D}} \\left[ D_{\\text{KL}} \\left( \\pi_{\\phi}(\\cdot | s_t) \\bigg\\| \\frac{\\exp(Q_{\\theta}(s_t, \\cdot))}{Z_{\\theta}(s_t)} \\right) \\right] \\] <p>Instead of solving this directly, SAC reparameterizes the policy using:</p> \\[ a_t = f_{\\phi}(\\epsilon_t; s_t) \\] <p>This trick is used to make sure that sampling from the policy is a differentiable process so that there are no problems in backpropagating the errors.  \\(\\epsilon_t\\) is random noise vector sampled from fixed distribution (e.g., Spherical Gaussian).</p> <p>Why Reparameterization is Needed? <p>In reinforcement learning, the policy \\(\\pi(a | s)\\) often outputs a probability distribution over actions rather than deterministic actions. The standard way to sample an action is:</p> \\[ a_t \\sim \\pi_{\\phi}(a_t | s_t) \\] <p>However, this sampling operation blocks gradient flow during backpropagation, preventing efficient training using stochastic gradient descent (SGD).</p> <p>Instead of directly sampling \\(a_t\\) from \\(\\pi_{\\phi}(a_t | s_t)\\), we transform a simple noise variable into an action:</p> \\[ a_t = f_{\\phi}(\\epsilon_t, s_t) \\] <ul> <li>\\(\\epsilon_t \\sim \\mathcal{N}(0, I)\\) is sampled from a fixed noise distribution (e.g., a Gaussian).</li> <li>\\(f_{\\phi}(\\epsilon_t, s_t)\\) is a differentiable function (often a neural network) that maps noise to an action.</li> </ul> <p>For a Gaussian policy in SAC, the action is computed as:</p> \\[ a_t = \\mu_{\\phi}(s_t) + \\sigma_{\\phi}(s_t) \\cdot \\epsilon_t \\] <p>So instead of sampling from \\(\\mathcal{N}(\\mu, \\sigma^2)\\) directly, we sample from a fixed standard normal and transform it using a differentiable function.This makes the policy differentiable, allowing gradients to flow through \\(\\mu_{\\phi}\\) and \\(\\sigma_{\\phi}\\).</p> <p> </p> <ul> <li>Continuous Action Generation</li> </ul> <p>In a continuous action space soft actor-critic agent, the neural network in the actor takes the current observation and generates two outputs, one for the mean and the other for the standard deviation. To select an action, the actor randomly selects an unbounded action from this Gaussian distribution. If the soft actor-critic agent needs to generate bounded actions, the actor applies tanh and scaling operations to the action sampled from the Gaussian distribution.</p> <p>During training, the agent uses the unbounded Gaussian distribution to calculate the entropy of the policy for the given observation.</p> <p></p> <ul> <li>Discrete Action Generation</li> </ul> <p>In a discrete action space soft actor-critic agent, the actor takes the current observation and generates a categorical distribution, in which each possible action is associated with a probability. Since each action that belongs to the finite set is already assumed feasible, no bounding is needed.</p> <p>During training, the agent uses the categorical distribution to calculate the entropy of the policy for the given observation.</p> <p>if we rewrite the equation we have:</p> \\[ J_{\\pi}(\\phi) = \\mathbb{E}_{s_t \\sim \\mathcal{D}, \\epsilon_t \\sim \\mathcal{N}} \\left[ \\text{log}\\space\\pi_{\\phi} \\left(f_{\\phi}(\\epsilon_t; s_t) | s_t \\right) - Q_{\\theta}(s_t,f_{\\phi}(\\epsilon_t; s_t) \\right] \\] <p>where \\(\\pi_{\\phi}\\) is defined implicitly in terms of \\(f_{\\phi}\\), and we have noted that the partition function is independent of \\(\\phi\\) and can thus be omitted.</p> <p>Policy Gradient Update</p> \\[ \\hat{\\nabla}_{\\phi} J_{\\pi}(\\phi) = \\nabla_{\\phi} \\log \\pi_{\\phi}(a_t | s_t) + \\left( \\nabla_{a_t} \\log \\pi_{\\phi}(a_t | s_t)- \\nabla_{a_t} Q_{\\theta}(s_t, a_t) \\right) \\nabla_{\\phi} f_{\\phi}(\\epsilon_t; s_t) \\]"},{"location":"recitations/week4/#sac-pseudocode","title":"SAC pseudocode","text":""},{"location":"recitations/week4/#algorithm-1-soft-actor-critic","title":"Algorithm 1: Soft Actor-Critic","text":"<p>Initialize parameter vectors \\(\\psi, \\bar{\\psi}, \\theta, \\phi\\)</p> <p>for each iteration do for each environment step do \\(a_t \\sim \\pi_{\\phi}(a_t | s_t)\\) \\(s_{t+1} \\sim p(s_{t+1} | s_t, a_t)\\) \\(\\mathcal{D} \\gets \\mathcal{D} \\cup \\{(s_t, a_t, r(s_t, a_t), s_{t+1})\\}\\) end for </p> <p> for each gradient step do \\(\\psi \\gets \\psi - \\lambda \\hat{\\nabla}_{\\psi} J_V(\\psi)\\) \\(\\theta_i \\gets \\theta_i - \\lambda_Q \\hat{\\nabla}_{\\theta_i} J_Q(\\theta_i) \\quad \\text{for } i \\in \\{1,2\\}\\) \\(\\phi \\gets \\phi - \\lambda_{\\pi} \\hat{\\nabla}_{\\phi} J_{\\pi}(\\phi)\\) \\(\\bar{\\psi} \\gets \\tau \\psi + (1 - \\tau) \\bar{\\psi}\\) end for end for</p>"},{"location":"recitations/week4/#authors","title":"Author(s)","text":"<ul> <li> <p>Ahmad Karami</p> <p>Teaching Assistant</p> <p>ahmad.karami77@yahoo.com</p> <p> </p> </li> <li> <p>Hamidreza Ebrahimpour</p> <p>Teaching Assistant</p> <p>ebrahimpour.7879@gmail.com</p> <p> </p> </li> </ul>"},{"location":"recitations/week5/","title":"Week 5: Model-Based Methods","text":""},{"location":"recitations/week5/#screen-record","title":"Screen Record","text":""},{"location":"recitations/week5/#recitation-notes","title":"Recitation Notes","text":""},{"location":"recitations/week5/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Week 5: Model-Based Methods</li> <li>Lecture Notes on Model-Based Reinforcement Learning</li> <li>Table of Contents</li> <li>1. Introduction to Model-Based Reinforcement Learning</li> <li>2. Stochastic Optimization<ul> <li>2.1 Problem Formulation</li> <li>2.2 Sample-Based Methods</li> <li>2.3 Example Problem: Fitting a Linear Regression Model for Transition Prediction</li> <li>Iteration 1<ul> <li>1) Predictions \\&amp; Errors</li> <li>2) Gradients</li> <li>3) Update</li> </ul> </li> <li>Iteration 2<ul> <li>1) Predictions \\&amp; Errors</li> <li>2) Gradients</li> <li>3) Update</li> </ul> </li> <li>Result</li> </ul> </li> <li>3. Cross-Entropy Method (CEM)<ul> <li>3.1 Algorithmic Steps</li> <li>3.2 Intuition and Variants</li> <li>3.3 Example Problem: 1D Action Optimization</li> </ul> </li> <li>4. Monte Carlo Tree Search (MCTS)<ul> <li>4.1 Basic Components</li> <li>4.2 PUCT / UCB for Trees</li> <li>4.3 Example Problem: MCTS for a Simple Game (Tic-Tac-Toe)</li> <li>Game Overview</li> <li>Notation for Game States</li> <li>Simulation #1<ul> <li>Step 1: Selection (from the root)</li> <li>Step 2: Expansion</li> <li>Step 3: Simulation (Rollout)</li> </ul> </li> <li>Step 4: Backpropagation</li> <li>Simulation #2<ul> <li>Step 1: Selection (from root)</li> <li>Step 2: Expansion</li> <li>Step 3: Simulation (Rollout)</li> <li>Step 4: Backpropagation</li> </ul> </li> <li>After Two Simulations</li> </ul> </li> <li>5. Model Predictive Control (MPC)<ul> <li>5.1 General Framework</li> <li>5.2 MPC in Reinforcement Learning</li> <li>5.3 Example Problem: Double Integrator System</li> </ul> </li> <li>6. Uncertainty Estimation<ul> <li>6.1 Sources of Uncertainty</li> <li>6.2 Methods of Estimation</li> <li>6.3 Implications for Model-Based RL</li> <li>6.4 Example Problem: Gaussian Process for Next-State Prediction</li> </ul> </li> <li>7. Dyna-Style Algorithms<ul> <li>7.1 Sutton\u2019s Dyna Architecture</li> <li>7.2 Integrating Planning, Acting, and Learning</li> <li>7.3 Example Problem: Dyna-Q in a 3-State Chain Environment</li> </ul> </li> <li>8. References</li> </ul>"},{"location":"recitations/week5/#1-introduction-to-model-based-reinforcement-learning","title":"1. Introduction to Model-Based Reinforcement Learning","text":"<p>In reinforcement learning, an agent interacts with an environment modeled (or approximated) as a Markov Decision Process \\(\\langle \\mathcal{S}, \\mathcal{A}, P, R, \\gamma \\rangle\\). Model-based RL involves explicitly learning or using a model of the environment\u2014transition dynamics \\(P(s'|s,a)\\) and/or reward \\(R(s,a)\\)\u2014to plan actions. </p> <p>Why Model-Based? - Sample Efficiency: If collecting data is expensive, using a model for planning can reduce the necessary real-world interactions. - Exploration: A model allows the agent to simulate potential scenarios offline and direct exploration more effectively. - Robustness and Safety: In safety-critical applications (e.g., robotics, autonomous driving), planning with a model can help avoid catastrophic outcomes during training.  </p> <p>In the sections that follow, we will delve into foundational methods and demonstrate their use in simple, illustrative problems.</p>"},{"location":"recitations/week5/#2-stochastic-optimization","title":"2. Stochastic Optimization","text":""},{"location":"recitations/week5/#21-problem-formulation","title":"2.1 Problem Formulation","text":"<p>A stochastic optimization problem aims to optimize an objective function under uncertainty:</p> \\[\\min_{\\theta} \\; \\mathbb{E}_{x \\sim \\mathcal{D}}[ f(\\theta, x) ],\\] <p>where - \\(\\theta\\) are parameters (could be neural network weights, policy parameters, etc.), - \\(\\mathcal{D}\\) is a (possibly unknown) data distribution or environment dynamics, - \\(f(\\theta, x)\\) is a cost (or negative reward) function.</p> <p>In model-based RL, such problems appear when we: 1. Train a model \\(\\hat{P}_\\theta(s'|s,a)\\) to predict transitions by minimizing some loss \\(\\mathcal{L}(\\theta)\\). 2. Optimize a policy using predicted trajectories.</p>"},{"location":"recitations/week5/#22-sample-based-methods","title":"2.2 Sample-Based Methods","text":"<p>Because \\(\\mathcal{D}\\) or \\(f(\\theta, x)\\) might be complex or high-dimensional, sample-based approaches are common:</p> <ol> <li>Stochastic Gradient Descent (SGD): </li> <li>Evaluate \\(\\nabla_\\theta f(\\theta, x^{(i)})\\) on mini-batches of samples from \\(\\mathcal{D}\\).  </li> <li> <p>Update \\(\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta f(\\theta, x^{(i)})\\).</p> </li> <li> <p>Population-Based / Evolutionary Algorithms:</p> </li> <li>Maintain a population of candidate solutions \\(\\{\\theta_1, \\theta_2, \\ldots\\}\\).  </li> <li> <p>Evaluate fitness and use selection, crossover, mutation to evolve better solutions.</p> </li> <li> <p>Simulated Annealing:</p> </li> <li>Iteratively propose a new solution and accept/reject based on a temperature parameter that decreases over time, allowing for occasional acceptance of worse solutions to escape local minima.</li> </ol>"},{"location":"recitations/week5/#23-example-problem-fitting-a-linear-regression-model-for-transition-prediction","title":"2.3 Example Problem: Fitting a Linear Regression Model for Transition Prediction","text":"<p>Problem Setup - We have a small environment: state \\(s\\in \\mathbb{R}\\) is 1D, action \\(a\\in \\{-1, 1\\}\\). - The true environment dynamics is \\(s_{t+1} = s_t + 0.5 \\, a + \\epsilon_t\\), where \\(\\epsilon_t \\sim \\mathcal{N}(0, 0.1^2)\\). - We collect \\(N=100\\) transitions \\(\\{(s^{(i)}, a^{(i)}, s'^{(i)})\\}_{i=1}^N\\). - We want to fit a linear model: \\(\\hat{s}_{t+1} = w_0 + w_1 s_t + w_2 a_t\\).  </p> <p>Cost Function </p> \\[\\mathcal{L}(w_0, w_1, w_2) = \\sum_{i=1}^N \\bigl(s'^{(i)} - (w_0 + w_1 s^{(i)} + w_2 a^{(i)}) \\bigr)^2.\\] <p>Stochastic Gradient Descent Approach </p> <ol> <li>Initialize parameters \\(\\theta = (w_0, w_1, w_2)\\) randomly.  </li> <li> <p>Loop until convergence:</p> <ul> <li>Sample a mini-batch \\(\\mathcal{B}\\) of transitions from the dataset.</li> <li> <p>Compute the gradient:</p> \\[\\nabla_\\theta \\mathcal{L}_\\mathcal{B}(\\theta) = \\sum_{(s,a,s') \\in \\mathcal{B}} 2 \\, (s' - \\hat{s}) \\, (-1) \\nabla_\\theta \\hat{s},\\] <p>where \\(\\hat{s} = w_0 + w_1 s + w_2 a\\).</p> </li> <li> <p>Update:</p> \\[\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta \\mathcal{L}_\\mathcal{B}(\\theta).\\] </li> </ul> </li> </ol>"},{"location":"recitations/week5/#iteration-1","title":"Iteration 1","text":""},{"location":"recitations/week5/#1-predictions-errors","title":"# 1) Predictions &amp; Errors","text":"<ol> <li>\\(\\hat{s}^{(1)} = 0.20 + 0.90\\cdot0.0 + 0.30\\cdot(+1)=0.50\\); error \\(e^{(1)}=0.55-0.50=0.05.\\) </li> <li>\\(\\hat{s}^{(2)} = 0.20 + 0.90\\cdot1.0 + 0.30\\cdot(-1)=0.80\\); error \\(e^{(2)}=0.40-0.80=-0.40.\\)</li> </ol>"},{"location":"recitations/week5/#2-gradients","title":"# 2) Gradients","text":"<ul> <li>Point #1: \\(\\nabla = -2e^{(1)}[1,\\,s,\\,a] = -2\\cdot0.05\\,[1,\\,0.0,\\,+1] = [-0.10,0,-0.10].\\) </li> <li>Point #2: \\(\\nabla = -2e^{(2)}[1,\\,s,\\,a] = -2\\cdot(-0.40)\\,[1,\\,1.0,\\,-1] = [+0.80,+0.80,-0.80].\\)</li> </ul> <p>Sum: \\([0.70,\\,0.80,\\,-0.90].\\)</p>"},{"location":"recitations/week5/#3-update","title":"# 3) Update","text":"\\[ (w_0,w_1,w_2) \\leftarrow (0.20,0.90,0.30)\\;-\\;0.1\\times(0.70,\\,0.80,\\,-0.90) $$ $$ = (0.13,\\,0.82,\\,0.39). \\]"},{"location":"recitations/week5/#iteration-2","title":"Iteration 2","text":""},{"location":"recitations/week5/#1-predictions-errors_1","title":"# 1) Predictions &amp; Errors","text":"<ol> <li>\\(\\hat{s}^{(1)}=0.13 + 0.82\\cdot0.0 + 0.39\\cdot(+1)=0.52\\); \\(e^{(1)}=0.55-0.52=0.03.\\) </li> <li>\\(\\hat{s}^{(2)}=0.13 + 0.82\\cdot1.0 + 0.39\\cdot(-1)=0.56\\); \\(e^{(2)}=0.40-0.56=-0.16.\\)</li> </ol>"},{"location":"recitations/week5/#2-gradients_1","title":"# 2) Gradients","text":"<ul> <li>Point #1: \\([-2\\cdot0.03,\\,-2\\cdot0.03\\cdot0,\\,-2\\cdot0.03\\cdot1]=[-0.06,\\,0,\\,-0.06].\\) </li> <li>Point #2: \\([-2\\cdot(-0.16),\\,-2\\cdot(-0.16)\\cdot1,\\,-2\\cdot(-0.16)\\cdot(-1)]=[+0.32,+0.32,-0.32].\\)</li> </ul> <p>Sum: \\([0.26,\\,0.32,\\,-0.38].\\)</p>"},{"location":"recitations/week5/#3-update_1","title":"# 3) Update","text":"\\[ (w_0,w_1,w_2)\\leftarrow(0.13,\\,0.82,\\,0.39)-0.1\\times(0.26,\\,0.32,\\,-0.38) $$ $$ = (0.104,\\,0.788,\\,0.428). \\]"},{"location":"recitations/week5/#result","title":"Result","text":"<p>After two steps, \\(\\theta\\) moved from \\((0.20,\\,0.90,\\,0.30)\\) to \\((0.104,\\,0.788,\\,0.428)\\). With more iterations (and more data), the model converges near \\((0,1.0,0.5)\\), reflecting the true dynamics \\(s_{t+1} = s_t + 0.5\\,a + \\epsilon_t\\).</p>"},{"location":"recitations/week5/#3-cross-entropy-method-cem","title":"3. Cross-Entropy Method (CEM)","text":"<p>The Cross-Entropy Method (CEM) is a population-based algorithm that iteratively refines a sampling distribution over possible solutions, focusing on an \u201celite\u201d set of the highest-performing samples.</p>"},{"location":"recitations/week5/#31-algorithmic-steps","title":"3.1 Algorithmic Steps","text":"<ol> <li>Parameterize a Distribution: Let \\(q(\\theta \\mid \\phi)\\) be a distribution over solutions \\(\\theta\\). Often, we use a multivariate Gaussian with parameters \\(\\phi = \\{\\mu, \\Sigma\\}\\).  </li> <li>Sample Solutions: Draw \\(\\{\\theta_1,\\ldots,\\theta_M\\}\\) from \\(q(\\theta|\\phi)\\).  </li> <li>Evaluate Solutions: Compute an objective \\(J(\\theta_i)\\) for each sample (e.g., cumulative reward, negative cost).  </li> <li>Select Elites: Pick the top \\(K\\) samples (or a top percentage).  </li> <li>Update Distribution: Update \\(\\phi\\) (e.g., \\(\\mu, \\Sigma\\)) to fit the elite set.  </li> <li>Iterate: Continue sampling from the updated distribution until convergence or a maximum iteration limit.</li> </ol>"},{"location":"recitations/week5/#32-intuition-and-variants","title":"3.2 Intuition and Variants","text":"<ul> <li>Intuition: By repeatedly sampling solutions and focusing on the best ones, we \u201czoom in\u201d on promising regions.  </li> <li>Quantile Selection: Instead of picking the top \\(K\\), one can pick all solutions above a certain performance threshold.  </li> <li>Regularization: Add a fraction of the old mean/covariance to stabilize updates (avoiding collapsing to a single point).</li> </ul>"},{"location":"recitations/week5/#33-example-problem-1d-action-optimization","title":"3.3 Example Problem: 1D Action Optimization","text":"<p>Scenario </p> <ul> <li>We have a 1D system with state \\(s_0 = 0\\).  </li> <li>We can choose a single action \\(a\\in\\mathbb{R}\\) that transitions the system to \\(s_1 = s_0 + a\\). Then a reward is given by \\(r(s_1) = -(s_1 - 2)^2\\).  </li> <li>We want to find the action \\(a^*\\) that maximizes the reward (equivalently minimizes the negative reward):</li> </ul> \\[a^* = \\arg\\max_a \\; - (0 + a - 2)^2.\\] <p>The optimal solution is obviously \\(a^*=2\\).</p> <p>But let's see how CEM would handle this without an analytical solution.</p> <p>CEM Steps 1. Initialize a Gaussian distribution for \\(a\\): \\(\\mu=0\\), \\(\\sigma^2=4\\). 2. Sample 20 actions: \\(\\{a_1,\\ldots,a_{20}\\}\\). 3. Evaluate each action: \\(J(a_i)= - (a_i - 2)^2.\\) 4. Select Elites: Suppose we pick the top 5 actions. 5. Update \\(\\mu,\\sigma\\) to the mean and std of these top 5 actions. 6. Iterate: After a few iterations, \\(\\mu\\) converges near \\(2\\).</p> <p>A table might look like:</p> Iteration Sample Actions Best 5 (Elites) New Mean (\u03bc) New Std (\u03c3) 1 \\(\\{-4, 1.2, 3.1, ...\\}\\) \\(\\{2.9, 3.1, 1.8, 2.4, 1.6\\}\\) 2.36 0.53 2 \\(\\{1.9, 2.6, 2.1, ...\\}\\) \\(\\{2.0, 2.1, 2.4, 2.6, 1.9\\}\\) 2.2 0.24 ... ... ... ... ... <p>The method \u201chomes in\u201d on the correct action \\(a^*\\approx 2\\).</p>"},{"location":"recitations/week5/#4-monte-carlo-tree-search-mcts","title":"4. Monte Carlo Tree Search (MCTS)","text":"<p>Monte Carlo Tree Search is a powerful method for decision-making in discrete action spaces (notably games). It builds a partial search tree by random simulations (rollouts) and updates action values based on the simulation outcomes.</p>"},{"location":"recitations/week5/#41-basic-components","title":"4.1 Basic Components","text":"<ol> <li>Selection: Traverse down the tree from the root state, choosing actions that balance exploration and exploitation (e.g., UCB).  </li> <li>Expansion: When reaching a leaf node, expand one or more children.  </li> <li>Simulation (Rollout): From the new child node, simulate a default policy until a terminal state (or a depth limit) is reached.  </li> <li>Backpropagation: Propagate the final simulation outcome (win/loss or reward) back up the tree, updating visit counts and action-value estimates.</li> </ol>"},{"location":"recitations/week5/#42-puct-ucb-for-trees","title":"4.2 PUCT / UCB for Trees","text":"<p>A common selection formula is:</p> \\[\\text{UCT}(s,a) = \\bar{Q}(s,a) + c \\sqrt{\\frac{\\ln N(s)}{N(s,a)}}.\\] <ul> <li>\\(\\bar{Q}(s,a)\\): average return of choosing \\(a\\) at \\(s\\).  </li> <li>\\(N(s)\\): visits to state \\(s\\).  </li> <li>\\(N(s,a)\\): visits to the action \\(a\\) from state \\(s\\).  </li> <li>\\(c\\): exploration constant.</li> </ul> <p>In AlphaZero, a variation called PUCT (Polynomial Upper Confidence Trees) adds a prior probability \\(\\pi(a|s)\\) from a policy network:</p> \\[\\text{PUCT}(s,a) = \\bar{Q}(s,a) + c \\,\\pi(a\\mid s)\\,\\frac{\\sqrt{N(s)}}{1+N(s,a)}.\\]"},{"location":"recitations/week5/#43-example-problem-mcts-for-a-simple-game-tic-tac-toe","title":"4.3 Example Problem: MCTS for a Simple Game (Tic-Tac-Toe)","text":"<p>To illustrate how Monte Carlo Tree Search (MCTS) proceeds in a small game like Tic-Tac-Toe, let\u2019s walk step-by-step through two simulated MCTS iterations. We will track how a single simulation (playout) is done each time, and how the search tree is incrementally updated. </p>"},{"location":"recitations/week5/#game-overview","title":"Game Overview","text":"<ul> <li>Board: 3x3 grid.</li> <li>Players: </li> <li>X (the MCTS agent we are training), who always goes first.</li> <li>O (the opponent), which we will assume plays randomly in this example.</li> <li>Rewards (from X\u2019s perspective):</li> <li>+1 if X eventually wins,</li> <li>-1 if O wins,</li> <li>0 for a draw.</li> </ul> <p>We will show how: 1. The Selection step chooses which moves to explore based on visit statistics. 2. The Expansion step adds new nodes (child states) to the tree. 3. The Simulation (rollout) step plays randomly until a terminal board state is reached. 4. The Backpropagation step updates the node statistics.</p> <p>Note: Real MCTS typically runs many (hundreds or thousands) of simulations per move. We\u2019ll illustrate just two simulations for clarity.</p>"},{"location":"recitations/week5/#notation-for-game-states","title":"Notation for Game States","text":"<p>We\u2019ll represent the board states with a simple ASCII layout. For example:</p> <pre><code>```\n1 | 2 | 3 \n---+---+--- \n4 | 5 | 6 \n---+---+--- \n7 | 8 | 9\n```\n</code></pre> <ul> <li>Positions \\(1,2,3,4,5,6,7,8,9\\) can be empty, 'X', or 'O'.</li> <li>The root state (empty board) has all 9 positions empty.</li> </ul>"},{"location":"recitations/week5/#simulation-1","title":"Simulation #1","text":""},{"location":"recitations/week5/#step-1-selection-from-the-root","title":"# Step 1: Selection (from the root)","text":"<ul> <li>Root State: Empty board. We have no Q-values (no prior visits).  </li> <li>MCTS typically breaks ties randomly when no node has a better value or visit count.  </li> <li>Possible moves for X: 9 (positions 1 through 9).  </li> </ul> <p>Let's assume the selection step picks the center move (position 5) for X. - Because all moves are identical from the MCTS perspective at the start, we choose position 5 arbitrarily (or randomly among the 9).</p>"},{"location":"recitations/week5/#step-2-expansion","title":"# Step 2: Expansion","text":"<ul> <li>We\u2019ve now arrived at the child node corresponding to X having placed a mark in position 5.  </li> <li> <p>Board now (child node):</p> <p><code>1 | 2 | 3 ---+---+--- 4 | X | 6 ---+---+--- 7 | 8 | 9</code></p> </li> <li> <p>We expand from this node. In basic MCTS, we typically expand one child for the next player (O).  </p> </li> <li>So let's add a single child for O\u2019s move. But O can choose any of the 8 empty positions.  </li> </ul> <p>For simplicity, say the expansion picks O\u2019s move at position 1.</p>"},{"location":"recitations/week5/#step-3-simulation-rollout","title":"# Step 3: Simulation (Rollout)","text":"<p>We now have a partially specified board:</p> <pre><code>```\nO | 2 | 3 \n---+---+--- \n4 | X | 6 \n---+---+---\n7 | 8 | 9\n```\n</code></pre> <p>From here, the simulation step means: 1. We keep playing until the game ends (win/loss/draw), 2. Both players choose moves randomly (in a real MCTS rollout, we typically do a \u201cdefault policy\u201d or random play).</p> <p>Let\u2019s illustrate a possible random sequence:</p> <ol> <li> <p>X\u2019s random move: positions available are \\(\\{2,3,4,6,7,8,9\\}\\). Suppose X picks position 9.  </p> <p><code>O | 2 | 3 ---+---+--- 4 | X | 6 ---+---+--- 7 | 8 | X</code></p> </li> <li> <p>O picks randomly from the remaining \\(\\{2,3,4,6,7,8\\}\\). Suppose O goes in position 2.</p> <p><code>O | O | 3 ---+---+--- 4 | X | 6 ---+---+--- 7 | 8 | X</code></p> </li> <li> <p>X picks from \\(\\{3,4,6,7,8\\}\\). Suppose X picks position 3.  </p> <p><code>O | O | X ---+---+--- 4 | X | 6 ---+---+--- 7 | 8 | X</code></p> </li> <li> <p>Check if X wins: Right now, no row/column/diagonal is complete.  </p> </li> <li> <p>O picks from \\(\\{4,6,7,8\\}\\). Suppose O picks position 6.  </p> <p><code>O | O | X ---+---+--- 4 | X | O ---+---+--- 7 | 8 | X</code></p> </li> <li> <p>No win yet.  </p> </li> <li> <p>X picks from \\(\\{4,7,8\\}\\). Suppose X picks position 7.  </p> <p><code>O | O | X ---+---+--- 4 | X | O ---+---+--- X | 8 | X</code></p> </li> <li> <p>Now X has positions {3,5,7,9}. That is a winning combination on a diagonal? Actually, 3-5-7 is not a diagonal; the diagonals are (1,5,9) and (3,5,7). </p> </li> <li>Check: 3,5,7 does indeed form a diagonal (top-right to bottom-left). So X has a win.</li> </ol> <p>Hence, the random rollout ends with X winning at step 5. - Return from X\u2019s perspective: +1</p>"},{"location":"recitations/week5/#step-4-backpropagation","title":"Step 4: Backpropagation","text":"<p>We backpropagate the reward (+1 for an X win) up the search tree:</p> <ol> <li>Child Node (O at pos 1): </li> <li>Visit count: \\(N = 1\\)</li> <li>Sum of returns: \\(W = +1\\)  (since from X\u2019s perspective, that rollout was a success)</li> <li> <p>Average Q-value: \\(\\bar{Q} = +1/1 = +1\\)</p> </li> <li> <p>Parent Node (X at pos 5):</p> </li> <li>Visit count: \\(N = 1\\)</li> <li>Sum of returns: \\(W = +1\\)</li> <li> <p>\\(\\bar{Q} = +1\\)</p> </li> <li> <p>Root Node (empty board):</p> </li> <li>Visit count: \\(N = 1\\)</li> <li>Sum of returns: \\(W = +1\\)</li> <li>\\(\\bar{Q} = +1\\)</li> </ol> <p>(We typically store separate counts for each edge or node, but conceptually the result is the same: each visited node has +1 to its total returns.)</p> <p>Thus, after one simulation, the move \u201cX at center\u201d has 100% success rate in our limited data. The child move \u201cO at pos 1\u201d also has an average value of +1 (though that\u2019s from X\u2019s perspective, which might seem counterintuitive for O\u2019s move, but in MCTS we track the perspective of the player to move at that node).</p>"},{"location":"recitations/week5/#simulation-2","title":"Simulation #2","text":"<p>Let\u2019s now do a second MCTS simulation. We start again at the root (empty board).</p>"},{"location":"recitations/week5/#step-1-selection-from-root","title":"# Step 1: Selection (from root)","text":"<p>Now, the root node has a single visited child: \u201cX at pos 5\u201d with a Q-value of +1. - All other 8 moves are unvisited (Q=0, visits=0). - With typical MCTS selection criteria (like UCB), we might still explore an unvisited move.    However, because \u201cX at pos 5\u201d has a very high (maximum) average return so far, let\u2019s assume the selection again picks \u201cX at pos 5\u201d.</p>"},{"location":"recitations/week5/#step-2-expansion_1","title":"# Step 2: Expansion","text":"<p>From the state with \u201cX at center,\u201d we already have an expanded child \u201cO at pos 1.\u201d - But O could also move to positions 2,3,4,6,7,8,9. - Typically, MCTS expansions add at least one unvisited move for O. - Let\u2019s say we expand \u201cO at pos 9\u201d this time as a new child node.</p>"},{"location":"recitations/week5/#step-3-simulation-rollout_1","title":"# Step 3: Simulation (Rollout)","text":"<p>From the state:</p> <pre><code>   1 | 2 | 3\n  ---+---+---\n   4 | X | 6\n  ---+---+---\n   7 | 8 | O\n</code></pre> <p>(That is X in 5, O in 9.)</p> <p>We do another random rollout:</p> <ol> <li>X picks from \\(\\{1,2,3,4,6,7,8\\}\\). Suppose X picks position 1.    <code>X | 2 | 3    ---+---+---    4 | X | 6    ---+---+---    7 | 8 | O</code></li> <li>O picks from \\(\\{2,3,4,6,7,8\\}\\). Suppose O picks position 2.    <code>X | O | 3    ---+---+---    4 | X | 6    ---+---+---    7 | 8 | O</code></li> <li>X picks from \\(\\{3,4,6,7,8\\}\\). Suppose X picks position 6.    <code>X | O | 3    ---+---+---    4 | X | X    ---+---+---    7 | 8 | O</code></li> <li>O picks from \\(\\{3,4,7,8\\}\\). Suppose O picks position 4.    <code>X | O | 3    ---+---+---    O | X | X    ---+---+---    7 | 8 | O</code></li> <li>X picks from \\(\\{3,7,8\\}\\). Suppose X picks 3:    <code>X | O | X    ---+---+---    O | X | X    ---+---+---    7 | 8 | O</code></li> <li>Check if X has 3 in a row: Not yet (positions X has are {1,3,5,6} \u2013 no row/column/diagonal complete).</li> <li>O picks from \\(\\{7,8\\}\\). Suppose O picks 7:    <code>X | O | X    ---+---+---    O | X | X    ---+---+---    O | 8 | O</code></li> <li>O has positions {2,4,7,9}. Check if O wins: <ul> <li>2,4,7 is not a line, 4,7,9 is not a line, 2,7,9 is not a line, etc. So no win yet.</li> </ul> </li> <li>X picks the last available spot 8:    <code>X | O | X    ---+---+---    O | X | X    ---+---+---    O | X | O</code></li> <li>Now the board is full. Check final lines:<ul> <li>X\u2019s positions: {1,3,5,6,8} </li> <li>No winning triple among those positions (1,3,5) not a line, 3,5,6 not a line, etc.</li> <li>O\u2019s positions: {2,4,7,9}</li> <li>Also no line. </li> </ul> </li> <li>It\u2019s a draw.</li> </ol> <p>Result from X\u2019s perspective: 0 (draw).</p>"},{"location":"recitations/week5/#step-4-backpropagation_1","title":"# Step 4: Backpropagation","text":"<p>We backpropagate the draw reward (0) up the tree.</p> <ul> <li>Child node: \u201cO at pos 9.\u201d  </li> <li>Visits: \\(N = 1\\) </li> <li>Sum of returns: \\(W = 0\\) </li> <li> <p>\\(\\bar{Q} = 0\\)</p> </li> <li> <p>Parent node: \u201cX at pos 5.\u201d  </p> </li> <li>Previously: \\(\\bar{Q} = +1\\), \\(N=1\\), sum of returns \\(W=+1\\).  </li> <li>Now we add one more visit with reward 0: new \\(N=2\\), new sum \\(W = +1 + 0 = +1\\).  </li> <li> <p>Updated average \\(\\bar{Q} = \\frac{+1}{2} = +0.5\\).</p> </li> <li> <p>Root node:  </p> </li> <li>Previously: \\(\\bar{Q} = +1\\), \\(N=1\\), sum of returns = +1.  </li> <li>Now: \\(N=2\\), sum of returns = +1+0 = +1.  </li> <li>\\(\\bar{Q} = 1/2 = +0.5\\).</li> </ul>"},{"location":"recitations/week5/#after-two-simulations","title":"After Two Simulations","text":"<p>Here is a simplified view of the search tree and stats:</p> <ul> <li>Root node (empty board):  </li> <li>Visits: \\(N=2\\), Q-value: \\(+0.5\\).  </li> <li> <p>Has 9 possible children (8 unvisited, 1 visited).  </p> </li> <li> <p>Child (X at pos 5):</p> <ul> <li>Visits: 2, \\(\\bar{Q}=+0.5\\).</li> <li>Children (O\u2019s moves):</li> <li>\u201cO at pos 1\u201d: Visits=1, \\(\\bar{Q}=+1\\).  </li> <li>\u201cO at pos 9\u201d: Visits=1, \\(\\bar{Q}=0\\).  </li> <li>Others unvisited.</li> </ul> </li> </ul> <p>In real MCTS, we keep running more simulations. Because \u201cX at pos 5\u201d has a higher average return so far than unvisited moves, it will continue to be selected often. Within that node, O\u2019s possible moves get expanded one by one. Over many simulations, the tree grows, and the average returns converge to reflect the probability of winning from each state.</p>"},{"location":"recitations/week5/#5-model-predictive-control-mpc","title":"5. Model Predictive Control (MPC)","text":"<p>Model Predictive Control (MPC), also known as Receding Horizon Control, has a long history in industrial process control. It optimizes over a finite horizon at each time step, applies the first action, then re-plans.</p>"},{"location":"recitations/week5/#51-general-framework","title":"5.1 General Framework","text":"<ol> <li>Predictive Model \\(\\hat{P}\\): We assume we have a model \\(\\hat{P}(s_{t+1}|s_t,a_t)\\).  </li> <li> <p>Finite-Horizon Optimization:</p> \\[\\min_{a_0, \\dots, a_{H-1}} \\sum_{t=0}^{H-1} c(s_t, a_t) \\quad \\text{subject to} \\; s_{t+1} = \\hat{P}(s_t,a_t),\\] <p>plus an optional terminal cost or constraint. 3. Execute the First Action: Apply \\(a_0^*\\) to the real system. 4. Observe New State: Shift the horizon window and repeat.</p> </li> </ol>"},{"location":"recitations/week5/#52-mpc-in-reinforcement-learning","title":"5.2 MPC in Reinforcement Learning","text":"<ul> <li>In RL contexts, MPC can be used if we have a learned model \\(\\hat{P}_\\theta\\).  </li> <li>At each step, we solve an optimal control problem using the learned model for a short horizon.  </li> <li>This is especially common in continuous control tasks (e.g., robot arms, drones), where well-tuned MPC can be more stable than naive policy-based approaches.</li> </ul>"},{"location":"recitations/week5/#53-example-problem-double-integrator-system","title":"5.3 Example Problem: Double Integrator System","text":"<p>System - Continuous state: \\((x, \\dot{x})\\). - Action: acceleration \\(u\\). - Dynamics:  </p> \\[\\begin{cases} x_{t+1} = x_t + \\dot{x}_t \\Delta t \\\\ \\dot{x}_{t+1} = \\dot{x}_t + u_t \\Delta t \\end{cases}\\] <p>Goal: Regulate to \\((x, \\dot{x}) = (0,0)\\) with minimal cost:</p> \\[c(x, \\dot{x}, u) = x^2 + (\\dot{x})^2 + \\lambda u^2.\\] <p>MPC Steps 1. Horizon = \\(H\\) (e.g., 5 steps). 2. At each time:    - Solve \\(\\min \\sum_{t=0}^{H-1} \\left[ x_t^2 + (\\dot{x}_t)^2 + \\lambda u_t^2 \\right]\\).    - Subject to the above discrete dynamics.    - Use a standard solver or even the Cross-Entropy Method to find \\(\\{u_0,\\dots,u_{H-1}\\}\\).    - Apply only \\(u_0\\).    - Observe new state \\((x_1,\\dot{x}_1)\\).    - Repeat.  </p> <p>Over time, this receding horizon approach will bring the double integrator to the origin while balancing control effort.</p>"},{"location":"recitations/week5/#6-uncertainty-estimation","title":"6. Uncertainty Estimation","text":"<p>When learning a model of the environment, we often have uncertainty about model parameters or about inherent stochasticity. Accounting for this uncertainty can greatly improve planning and exploration.</p>"},{"location":"recitations/week5/#61-sources-of-uncertainty","title":"6.1 Sources of Uncertainty","text":"<ol> <li>Epistemic (model) uncertainty: Due to limited data or model expressiveness.  </li> <li>Aleatoric (intrinsic) uncertainty: Irreducible randomness in the environment (e.g., sensor noise).</li> </ol>"},{"location":"recitations/week5/#62-methods-of-estimation","title":"6.2 Methods of Estimation","text":"<ul> <li>Bayesian Neural Networks: Place a prior over weights, approximate posterior with VI or MCMC.  </li> <li>Ensembles: Train multiple networks on bootstrapped data; measure variance across predictions.  </li> <li>Gaussian Processes (GPs): Provide predictive means and variances with a kernel-based prior.</li> </ul>"},{"location":"recitations/week5/#63-implications-for-model-based-rl","title":"6.3 Implications for Model-Based RL","text":"<ul> <li>Exploration: Target states/actions where uncertainty is high to gather more data.  </li> <li>Risk-Sensitivity: Adjust the policy if high variance could lead to catastrophic failures.  </li> <li>Conservative Model-Based Planning: If uncertain, the model can yield higher cost or lower reward estimates to encourage caution.</li> </ul>"},{"location":"recitations/week5/#64-example-problem-gaussian-process-for-next-state-prediction","title":"6.4 Example Problem: Gaussian Process for Next-State Prediction","text":"<p>Scenario - 1D environment with unknown dynamics: \\(s_{t+1} = g(s_t) + \\epsilon\\). We suspect \\(g(\\cdot)\\) is smooth. - Collect data: \\(\\{(s^{(i)}, s'^{(i)})\\}\\). - Use a Gaussian Process (GP) with a kernel \\(k(s, s')\\) to predict \\(s_{t+1}\\).</p> <p>GP Training 1. Choose Kernel: e.g., RBF \\(k(s,s')=\\exp\\left(-\\frac{(s-s')^2}{2l^2}\\right)\\). 2. Compute Posterior: \\(p(s'_*|s_*, X, y) = \\mathcal{N}(\\bar{\\mu}, \\bar{\\sigma}^2)\\), where \\(\\bar{\\mu},\\bar{\\sigma}^2\\) come from GP regression formulas.</p> <p>Outcome - We get not just a prediction \\(\\bar{\\mu}\\) for next-state but also a standard deviation \\(\\bar{\\sigma}\\). - In planning, we can account for \\(\\bar{\\sigma}\\) by preferring actions with lower predicted variance or exploring high-variance regions.</p>"},{"location":"recitations/week5/#7-dyna-style-algorithms","title":"7. Dyna-Style Algorithms","text":"<p>Dyna is a classic framework by Richard Sutton that integrates: - Direct Reinforcement Learning from real experience, - Model Learning of transitions/rewards, - Planning using the learned model.</p>"},{"location":"recitations/week5/#71-suttons-dyna-architecture","title":"7.1 Sutton\u2019s Dyna Architecture","text":"<ol> <li>Experience: Interact with the environment, collecting transitions \\((s,a,r,s')\\).  </li> <li>Model: Update the model $ \\hat{P}(s'|s,a) $, $ \\hat{R}(s,a) $.  </li> <li>Replay / Planning: Sample \u201cimaginary\u201d transitions from \\(\\hat{P}\\) to update the policy or value function.  </li> </ol>"},{"location":"recitations/week5/#72-integrating-planning-acting-and-learning","title":"7.2 Integrating Planning, Acting, and Learning","text":"<ul> <li>Each real-world step triggers k planning updates.  </li> <li>This can dramatically increase data efficiency because each real transition can spawn many synthetic updates.</li> </ul>"},{"location":"recitations/week5/#73-example-problem-dyna-q-in-a-3-state-chain-environment","title":"7.3 Example Problem: Dyna-Q in a 3-State Chain Environment","text":"<p>Environment Setup </p> <ul> <li>States: \\(S_0, S_1, S_2\\).  </li> <li>Actions: left (L), right (R).  </li> <li>Transitions:  <ul> <li>From \\(S_0\\): R leads to \\(S_1\\), L does nothing.  </li> <li>From \\(S_1\\): R leads to \\(S_2\\), L leads back to \\(S_0\\).  </li> <li>From \\(S_2\\): terminal (or loops with 0 reward if we want a continuing environment).  </li> </ul> </li> <li>Rewards: +1 upon reaching \\(S_2\\). Otherwise 0.  </li> </ul> <p>Dyna-Q Algorithm </p> <pre><code>Initialize Q(s,a) arbitrarily\nInitialize model M(s,a) # store transitions and rewards\nalpha = 0.1\ngamma = 0.99\nnum_episodes = 100\nk = 5 # number of planning updates each step\n\nfor episode in range(num_episodes):\n    s = S_0\n    while s != S_2:\n        # 1. Choose action\n        a = epsilon_greedy(Q[s, :])\n\n        # 2. Observe real transition\n        s_next, r = environment_step(s,a)\n\n        # 3. Update Q with real transition\n        Q[s,a] = Q[s,a] + alpha * (r + gamma * max(Q[s_next,:]) - Q[s,a])\n\n        # 4. Update the model\n        M.store(s,a, s_next, r)\n\n        # 5. Planning (k steps)\n        for i in range(k):\n            s_rand, a_rand = M.sample_previously_visited()\n            s_sim, r_sim = M.predict(s_rand, a_rand)\n            Q[s_rand,a_rand] = Q[s_rand,a_rand] + alpha * (\n                r_sim + gamma * max(Q[s_sim,:]) - Q[s_rand,a_rand]\n            )\n\n        # 6. Move on\n        s = s_next\n</code></pre> <p>Intuition - Each real transition is used both to directly update Q and to improve the model. - Then k planning steps use the model to \u201challucinate\u201d transitions, effectively multiplying the benefit of each real step. - The agent learns the optimal policy (right-right from \\(S_0\\) to reach \\(S_2\\)) in fewer real interactions than a purely model-free approach.</p>"},{"location":"recitations/week5/#8-references","title":"8. References","text":"<p>Below is a comprehensive list of references mentioned, plus additional readings for deeper insights.</p> <ol> <li> <p>General Reinforcement Learning</p> <ul> <li>Sutton, R.S. &amp; Barto, A.G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press.</li> <li>Bertsekas, D. (2012). Dynamic Programming and Optimal Control. Athena Scientific.</li> <li>Spall, J.C. (2003). Introduction to Stochastic Search and Optimization. Wiley.</li> </ul> </li> <li> <p>Stochastic Optimization</p> <ul> <li>Spall, J.C. (2003). Introduction to Stochastic Search and Optimization. Wiley.</li> <li>Bertsekas, D. &amp; Tsitsiklis, J. (1996). Neuro-Dynamic Programming. Athena Scientific.</li> </ul> </li> <li> <p>Cross-Entropy Method</p> <ul> <li>Rubinstein, R.Y. &amp; Kroese, D.P. (2004). The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning. Springer.</li> <li>De Boer, P.-T., Kroese, D.P., Mannor, S., &amp; Rubinstein, R.Y. (2005). \u201cA tutorial on the cross-entropy method\u201d. Annals of Operations Research, 134(1), 19\u201367.</li> </ul> </li> <li> <p>Monte Carlo Tree Search (MCTS)</p> <ul> <li>Kocsis, L. &amp; Szepesv\u00e1ri, C. (2006). \u201cBandit based Monte-Carlo Planning\u201d. In ECML.</li> <li>Coulom, R. (2007). \u201cEfficient Selectivity and Backup Operators in Monte-Carlo Tree Search\u201d. In Computers and Games.</li> <li>Silver, D. et al. (2016). \u201cMastering the game of Go with deep neural networks and tree search\u201d. Nature, 529(7587), 484\u2013489.</li> <li>Silver, D. et al. (2017). \u201cMastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\u201d. arXiv preprint arXiv:1712.01815.</li> </ul> </li> <li> <p>Model Predictive Control (MPC)</p> <ul> <li>Camacho, E.F. &amp; Bordons, C. (2004). Model Predictive Control. Springer.</li> <li>Williams, G. et al. (2017). \u201cInformation Theoretic MPC for Model-Based Reinforcement Learning\u201d. In ICRA.</li> <li>Bertsekas, D. (2012). Dynamic Programming and Optimal Control. Athena Scientific.</li> </ul> </li> <li> <p>Uncertainty Estimation</p> <ul> <li>Deisenroth, M.P. &amp; Rasmussen, C.E. (2011). \u201cPILCO: A Model-Based and Data-Efficient Approach to Policy Search\u201d. In ICML.</li> <li>Blundell, C. et al. (2015). \u201cWeight Uncertainty in Neural Networks\u201d. In ICML.</li> <li>Lakshminarayanan, B. et al. (2017). \u201cSimple and Scalable Predictive Uncertainty Estimation using Deep Ensembles\u201d. In NIPS.</li> <li>Rasmussen, C.E. &amp; Williams, C.K.I. (2006). Gaussian Processes for Machine Learning. MIT Press.</li> </ul> </li> <li> <p>Dyna-Style Algorithms</p> <ul> <li>Sutton, R.S. (1991). \u201cDyna, an integrated architecture for learning, planning, and reacting\u201d. SIGART Bulletin, 2(4), 160\u2013163.</li> <li>Sutton, R.S. &amp; Barto, A.G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press.</li> </ul> </li> </ol>"},{"location":"recitations/week5/#authors","title":"Author(s)","text":"<ul> <li> <p>Naser Kazemi</p> <p>Teaching Assistant</p> <p>naserkazemi2002@gmail.com</p> <p> </p> </li> </ul>"},{"location":"recitations/week6/","title":"Week 6: Multi-Armed Bandits","text":""},{"location":"recitations/week6/#screen-record","title":"Screen Record","text":""},{"location":"recitations/week6/#recitation-notes","title":"Recitation Notes","text":""},{"location":"recitations/week6/#1-definition-of-the-problem","title":"1. Definition of the Problem","text":"<p>The multi-armed bandit problem is a very simple model that we can investigate to better understand the exploration/exploitation tradeoff. In this MDP, we have no state, but only actions and a reward function, i.e. \\((\\mathcal{A, R})\\). Here, \\(\\mathcal{A}\\) is a finite set of actions (\"bandit arms\"), while \\(\\mathcal{R}\\) is a distribution over rewards for actions: \\(\\mathcal{R}^a(r) = \\Pr[R = r | A = a]\\). At each step, then, the agent selects an action \\(A_t \\in \\mathcal{A}\\) and the environment generates a reward (\"payout\") \\(R_t \\sim \\mathcal{R}^{A_t}\\). The goal, as always, is to maximize the cumulative reward \\(\\sum_{\\tau=1}^t R_{\\tau}\\).</p> <p></p> <p>We can define a few more functions and variables in this setup. The action-value (Q-value) of an action is the mean reward for that action:</p> \\[q(a) = \\mathbb{E}[R | A = a].\\] <p>Furthermore, there exists one optimal value \\(v_{\\star}\\), which is the Q-value of the best action \\(a^{\\star}\\):</p> \\[v_\\star = q(a^\\star) = \\max_{a \\in \\mathcal{A}} q(a).\\] <p>The difficulty lies in the fact that the agent does not initially know the reward distributions of the arms. It must balance exploration (gathering information about unknown arms) and exploitation (choosing the best-known arm) to optimize long-term gains.</p> <p>Formally, the MAB problem consists of:</p> <ul> <li>A finite action set \\(\\mathcal{A}\\) with \\(k\\) possible actions.</li> <li>A reward distribution \\(\\mathcal{R}^a\\) for each action \\(a\\) where \\(R_t \\sim \\mathcal{R}^{A_t}\\) represents the reward obtained at time \\(t\\).</li> <li>The objective is to maximize the cumulative reward over \\(T\\) steps:</li> </ul> \\[ G_T = \\sum_{t=1}^{T} R_{t}. \\]"},{"location":"recitations/week6/#non-associativity-property","title":"Non-Associativity Property","text":"<ul> <li>Unlike full reinforcement learning problems, multi-armed bandits are non-associative.</li> <li>This means that the best action does not depend on the state; the optimal action is the same for all time steps.</li> <li>Formally, we do not consider state transitions: The bandit setting only involves selecting actions and receiving rewards, without any long-term effects from past decisions.</li> </ul> <p>Unlike full Markov Decision Processes (MDPs), the MAB setting does not involve state transitions, meaning the agent must learn optimal actions purely through repeated trials.</p>"},{"location":"recitations/week6/#real-world-applications-of-mab","title":"Real-World Applications of MAB","text":"<p>The MAB framework is used in several fields:</p> <ul> <li>Medical Trials: Identifying the best treatment while minimizing patient risk.</li> <li>Online Advertising: Choosing optimal ad placements for revenue maximization.</li> <li>Recommendation Systems: Dynamically selecting personalized content.</li> <li>Financial Investments: Allocating assets to maximize returns under uncertainty.</li> </ul>"},{"location":"recitations/week6/#2-action-value-methods-and-types","title":"2. Action-Value Methods and Types","text":"<p>To solve the MAB problem, we need a way to estimate action values. The action-value function \\(Q_t(a)\\) estimates the expected reward of choosing action \\(a\\) at time step \\(t\\):</p> \\[ Q_t(a) \\approx q_*(a). \\] <p>We define sample-average estimation of \\(Q_t(a)\\) as:</p> \\[ Q_t(a) = \\frac{1}{N_t(a)} \\sum_{i=1}^{N_t(a)} R_i \\] <p>where: - \\(N_t(a)\\) is the number of times action \\(a\\) has been selected. - \\(R_i\\) is the reward received when selecting \\(a\\).</p>"},{"location":"recitations/week6/#incremental-update-rule-for-efficient-computation","title":"Incremental Update Rule for Efficient Computation","text":"<p>Instead of storing all past rewards, we can update \\(Q_t(a)\\) incrementally:</p> \\[ Q_{t+1}(a) = Q_t(a) + \\frac{1}{N_t(a)} (R_t - Q_t(a)). \\]"},{"location":"recitations/week6/#derivation","title":"Derivation","text":"<p>The sample-average estimate at time step \\(t+1\\) is:</p> \\[ Q_{t+1}(a) = \\frac{1}{N_{t+1}(a)} \\sum_{i=1}^{N_{t+1}(a)} R_i \\] <p>Expanding this in terms of \\(Q_t(a)\\):</p> \\[ Q_{t+1}(a) = \\frac{1}{N_{t+1}(a)} \\left( \\sum_{i=1}^{N_t(a)} R_i + R_t \\right) \\] <p>Since \\(Q_t(a)\\) is the average of previous rewards:</p> \\[ Q_t(a) = \\frac{1}{N_t(a)} \\sum_{i=1}^{N_t(a)} R_i \\] <p>Multiplying by \\(N_t(a)\\):</p> \\[ N_t(a) Q_t(a) = \\sum_{i=1}^{N_t(a)} R_i \\] <p>Substituting into the equation:</p> \\[ Q_{t+1}(a) = \\frac{1}{N_{t+1}(a)} \\left( N_t(a) Q_t(a) + R_t \\right) \\] <p>Rewriting in update form:</p> \\[ Q_{t+1}(a) = Q_t(a) + \\frac{1}{N_t(a)} (R_t - Q_t(a)) \\] <p>This allows us to update \\(Q_t(a)\\) without storing all past rewards.</p>"},{"location":"recitations/week6/#constant-step-size-update-for-nonstationary-problems","title":"Constant Step-Size Update (For Nonstationary Problems)","text":"<p>When dealing with changing reward distributions, we use a constant step-size \\(\\alpha\\):</p> \\[ Q_{t+1}(a) = Q_t(a) + \\alpha (R_t - Q_t(a)). \\] <p>where \\(\\alpha\\) determines how much weight is given to recent rewards.</p> <ul> <li>If \\(\\alpha = \\frac{1}{N_t(a)}\\), this becomes sample-average estimation.</li> <li>If \\(\\alpha\\) is constant, this results in exponentially weighted averaging, useful for nonstationary problems.</li> </ul>"},{"location":"recitations/week6/#exponential-weighted-averaging","title":"Exponential Weighted Averaging","text":"<p>Expanding recursively:</p> \\[ Q_{t+1}(a) = (1 - \\alpha) Q_t(a) + \\alpha R_t \\] \\[ Q_{t+2}(a) = (1 - \\alpha) Q_{t+1}(a) + \\alpha R_{t+1} \\] <p>Expanding further:</p> \\[ Q_{t+2}(a) = (1 - \\alpha)^2 Q_t(a) + \\alpha (1 - \\alpha) R_t + \\alpha R_{t+1} \\] <p>This pattern continues, showing that older rewards have exponentially decaying influence:</p> \\[ Q_t(a) = (1 - \\alpha)^t Q_0(a) + \\sum_{i=0}^{t-1} \\alpha (1 - \\alpha)^i R_{t-i} \\] <p>which demonstrates the effect of exponential weighting.</p>"},{"location":"recitations/week6/#3-regret-measuring-suboptimality","title":"3. Regret: Measuring Suboptimality","text":"<p>Furthermore, we'll now want to investigate a novel quantity known as the regret. Regret gives us an indication of the opportunity loss for picking a particular action \\(A_t\\) compared to the optimal action \\(a^\\star\\) In symbols:</p> \\[ I_t = \\mathbb{E}[v_\\star - q(A_t)]. \\] <p>Finally, we can define the total regret as the total opportunity loss over an entire episode:</p> \\[ L_t = \\mathbb{E}\\left[\\sum_{\\tau=1}^t v_\\star - q(A_\\tau)\\right]. \\] <p>The goal of the agent is then to minimize the total regret, which is equivalent to maximizing the cumulative (total) reward. In addition to the above definitions, we'll now also keep a count \\(N_t(a)\\) which is the expected number of selections for an action a. Moreover, we define the gap \\(\\delta_a\\) as the difference between the expected value \\(q(a)\\) of an action a and the optimal action:</p> \\[ \\Delta_a = v_\\star - q(a). \\] <p>As such, we can give an equivalent definition of the regret \\(L_t\\) in terms of the gap:</p> \\[ \\begin{align}   L_t &amp;= \\mathbb{E}\\left[\\sum_{\\tau=1}^t v_\\star - q(A_\\tau)\\right] \\\\       &amp;= \\sum_{a \\in \\mathcal{A}}\\mathbb{E}[N_t(a)](v_\\star - q(a)) \\\\       &amp;= \\sum_{a \\in \\mathcal{A}}\\mathbb{E}[N_t(a)]\\Delta_a. \\end{align} \\] <p>Now, if we think of the regret as a function of iterations, we can make some observations. For example, we observe that the regret of a greedy algorithm \\(A_t = \\text{argmax}_{a \\in \\mathcal{A}} Q_t(a)\\) is a linear function, i.e. it increases linearly with each iteration. The reason why is that we may \"lock\" onto a suboptimal action forever, thus adding a certain fixed amount of regret each time. An alteration that we can make here is to initialize the Q-value of each action to the maximum reward. This is called optimistic initialization. Note that updates to \\(Q(a)\\) are made via an averaging process:</p> \\[ Q(a) = \\frac{1}{N_t(a)}\\sum_{t=1}^T \\mathbf{1}(A_t = a)R_t. \\] <p></p> <p>Now, while \\(\\varepsilon\\)-greedy approaches incur linear regret, certain strategies that decay \\(\\varepsilon\\) can actually only incur logarithmic (asymptotic) regret. Either way, there is actually a lower bound on the regret that any algorithm can achieve (i.e. no algorithm can do better), which is logarithmic:</p> \\[ \\lim_{t\\rightarrow\\infty} L_t \\geq \\log t \\sum_{a \\,|\\, \\Delta_a &gt; 0} \\frac{\\Delta_A}{\\text{KL}(\\mathcal{R}^a || \\mathcal{R}^{a^\\star})}. \\] <p>As you can see, this lower bound is proportional to the gap size (higher gap size means higher lower-bound for regret, i.e. more regret) and indirectly proportional to the similarity of bandits, given by the Kullback-Leibler divergence.</p>"},{"location":"recitations/week6/#4-exploration-exploitation-dilemma-and-uncertainty","title":"4. Exploration-Exploitation Dilemma and Uncertainty","text":"<p>The exploration-exploitation tradeoff is at the heart of MAB problems:</p> <ul> <li>Exploitation: Selecting the action with the highest known reward.</li> <li>Exploration: Trying less-known actions to gather new information.</li> </ul> <p>Since the agent lacks prior knowledge of the optimal action, exploration is necessary to reduce uncertainty. However, excessive exploration may lead to unnecessary loss of rewards. Finding the right balance is essential for optimal performance.</p>"},{"location":"recitations/week6/#5-exploration-strategies-in-bandits","title":"5. Exploration Strategies in Bandits","text":"<p>Several strategies help balance exploration and exploitation effectively:</p>"},{"location":"recitations/week6/#51-epsilon-greedy-exploration","title":"5.1. \\(\\epsilon\\)-Greedy Exploration","text":"<p>One of the simplest strategies for exploration is \\(\\epsilon\\)-greedy, where:</p> <ul> <li>With probability \\(1 - \\epsilon\\), we exploit by selecting the best-known action.</li> <li>With probability \\(\\epsilon\\), we explore by selecting a random action.</li> </ul> \\[ A_t = \\begin{cases} \\arg\\max_a Q_t(a), &amp; \\text{with probability } 1-\\epsilon \\\\ \\text{random } a, &amp; \\text{with probability } \\epsilon. \\end{cases} \\] <p>A variation, decaying \\(\\varepsilon\\)-greedy, reduces \\(\\varepsilon\\) over time to favor exploitation as learning progresses.</p>"},{"location":"recitations/week6/#52-optimistic-initialization","title":"5.2 Optimistic Initialization","text":"<p>Optimistic initialization encourages exploration by initializing action-values with an artificially high value:</p> \\[ Q_1(a) = Q_{  ext{max}}. \\] <p>This approach works by ensuring that initially, every action appears to be promising, forcing the agent to sample all actions before settling on an optimal choice. The key idea is to assume that each action is better than it actually is, prompting the agent to try them all.</p> <p>In practice, an optimistic estimate is set as:</p> \\[ Q_1(a) = R_{\\max}, \\] <p>where \\(R_{\\max}\\) is an upper bound on the highest possible reward. Since the agent updates estimates based on actual experience, actions with lower rewards will eventually have their values corrected downward, while the truly optimal actions will remain highly rated.</p> <p>Optimistic initialization is particularly effective when: - The environment is stationary, meaning reward distributions do not change over time. - The number of actions is small, ensuring each action is explored adequately. - The upper bound estimate \\(R_{\\max}\\) is not too high, as overly optimistic values can cause unnecessary exploration.</p> <p>This method is simple yet effective for balancing exploration and exploitation, particularly when rewards are initially unknown. method ensures early exploration before settling on the best action.</p>"},{"location":"recitations/week6/#53-ucb","title":"5.3 UCB","text":"<p>A very fundamental idea within the exploration/exploitation domain is that of optimism in the face of uncertainty. This idea tells us that if we know very well of the value of one action, but not as well about the value of another action, but do know that that other value may have a greater value, then we should go for the other action. So if you imagine a Gaussian distribution for the first action and a Gaussian for the second, then if the second Gaussian has a higher tail such that it could have its mean higher than the first Gaussian, even if the first Gaussian has currently a greater mean (but shorter tail), then we should pick the second one.</p> <p> </p> <p>To formalize this idea, we can think of confidence bounds. Let \\(U_t(a)\\) be an upper confidence bound on the value of action \\(a\\) at time \\(t\\), such that with high probability, the expected value \\(q(a)\\) is bounded by the current value \\(Q_t(a)\\) plus this bound:</p> \\[q(a) \\geq Q_t(a) + U_t(a).\\] <p>Then what the above paragraph described is equivalent to saying that we should pick the action with the highest value for \\(Q_t(a) + U_t(a)\\), i.e.</p> \\[A_t = \\text{argmax}_{a \\in \\mathcal{A}} Q_t(a) + U_t(a).\\] <p>Furthermore, one property of these upper confidence bounds that we require is that they should get smaller over time, meaning the variance should become smaller and smaller (the Gaussian becomes thinner).</p> <p>So how do we find \\(U_t(a)\\)? To do this, we'll use Hoeffding's inequality, which says that for some independently and identically distributed (i.i.d) random variables \\(X_1, ..., X_t \\in [0, 1]\\) with sample mean \\(\\bar{X} = \\frac{1}{t}\\sum_\\tau^t X_{\\tau}\\), we have</p> \\[\\Pr[\\mathbb{E}[X] &gt; \\bar{X} + u] \\leq e^{-2tu^2}\\] <p>as an upper bound on the probability that the true mean of \\(X\\) will be greater than the sample mean \\(\\bar{X}\\) plus some value \\(u\\). Differently put, this is an upper bound on the probability that the difference between the true mean and the sample mean will be greater than some value \\(u\\). For our purposes, we an plug in \\(q(a)\\) for the expectation (true mean), \\(Q_t(a)\\) for the sample mean and our upper confidence bound \\(U_t(a)\\) for the bounding value. This gives us</p> \\[\\Pr[q(a) &gt; Q_t(a) + U_t(a)] \\leq e^{-2N_t(a)U_t(a)^2}.\\] <p>We can now use this inequality to solve for \\(U_t(a)\\), giving us a way to compute this quantity. If we set \\(p\\) to be some probability that we want for  our confidence interval, we get</p> \\[ \\begin{align}   e^{-2N_t(a)U_t(a)^2} &amp;= p \\\\   -2N_t(a)U_t(a)^2 &amp;= \\log p \\\\   U_t(a)^2 &amp;= \\frac{\\log p}{-2N_t(a)} \\\\   U_t(a) &amp;= \\sqrt{\\frac{\\log p}{-2N_t(a)}}. \\end{align} \\] <p>As we can see, this gives us precisely the property that we wanted: Since \\(N_t(a)\\) is in the denominator, this upper bound will decrease over time, giving us more and more certainty about the true mean of the action-value \\(Q(a)\\).</p> <p>Now that we have a way to compute \\(U_t(a)\\), we know how to pick actions according to the formula we defined earlier. We can now develop an algorithm to solve multi-armed bandit problems, called the UCB1 algorithm. It picks an action according</p> \\[ \\begin{align}   A_t &amp;= \\text{argmax}_{a \\in \\mathcal{A}} Q_t(a) + U_t(a) \\\\       &amp;= \\text{argmax}_{a \\in \\mathcal{A}} Q_t(a) + \\sqrt{\\frac{\\log p}{-2N_t(a)}} \\end{align} \\] <p>This algorithm achieves the logarithmic regret we discussed earlier. Another algorithm which achieves this bound is called Thompson sampling, which sets the policy to</p> \\[\\pi(a) = \\mathbb{E}[\\mathbf{1}(Q(a) = \\max_{a'}Q(a')) | R_1,...,R_{t-1}]\\] <p>where use Bayes law to compute a posterior distribution \\(p_{\\mathbf{w}}(Q | R_1,...,R_{t-1})\\) and then sample an action-value function \\(Q(a)\\) from the posterior. We then simply pick the action that maximizes the action value functions.</p>"},{"location":"recitations/week6/#54-thompson-sampling","title":"5.4 Thompson Sampling","text":"<p>Thompson Sampling is a Bayesian approach that models action rewards probabilistically:</p> \\[ A_t = \\text{argmax}_{a} Q_t(a), \\quad Q_t(a) \\sim p(Q | R_1, ..., R_{t-1}). \\] <p>Thompson Sampling balances exploration and exploitation in an elegant, probabilistic manner and achieves logarithmic regret:</p> \\[ L_T = O(\\log T). \\] <p>The idea is to maintain a posterior distribution over the possible reward distributions (or parameters) of each arm and to sample an arm according to the probability it is the best arm. In essence, at each step Thompson Sampling randomizes its action in a way that is proportional to the credibility of each action being optimal given the observed data.</p> <p>Bayesian Formulation: Assume a prior distribution for the unknown parameters of each arm\u2019s reward distribution. For example, in a Bernoulli bandit (each play is success/failure with some unknown probability \\(\\theta_i\\)), one can use independent Beta priors \\(\\theta_i \\sim \\mathrm{Beta}(\\alpha_i, \\beta_i)\\) for each arm \\(i\\). When an arm is played and a reward observed, the prior for that arm is updated via Bayes\u2019 rule to a posterior. Thompson Sampling then selects an arm by drawing one sample from each arm\u2019s posterior distribution for the mean and then choosing the arm with the highest sampled mean. Concretely: - For each arm \\(i\\), sample \\(\\tilde{\\mu}_i\\) from the posterior of \\(\\mu_i\\) (given all data observed so far for that arm). - Play the arm \\(I_t = \\arg\\max_i \\tilde{\\mu}_i\\) that has the highest sampled value.</p> <p>After observing the reward for arm \\(I_t\\), update that arm\u2019s posterior. Repeat.</p> <p>This procedure intuitively balances exploration and exploitation: arms that are currently uncertain (with a wide posterior) have a higher chance of occasionally yielding a high sampled \\(\\tilde{\\mu}_i\\), prompting exploration, whereas arms that are likely to be good (posterior concentrated at a high mean) will usually win the sampling competition and be selected.</p> <p>Derivation (Probability Matching): Thompson Sampling can be derived as attempting to minimize Bayesian regret (expected regret with respect to the prior). It can be shown that at each step, the probability that TS selects arm \\(i\\) is equal to the probability (under the current posterior) that arm \\(i\\) is the optimal arm (i.e., has the highest true mean). Thus TS \u201cmatches\u201d the selection probability to the belief of optimality. This is in fact the optimal way to choose if one were to maximize the expected reward at the next play according to the posterior. Another way to see it: TS maximizes \\(\\mathbb{E}[\\mu_{I_t}]\\) given the current posterior by averaging over the uncertainty (it is equivalent to selecting an arm with probability of being best) \u2013 this can be shown to be the same decision a Bayesian decision-maker would make for one-step lookahead optimality.</p> <p>Posterior Updating: The exact implementation of TS depends on the reward model. In the simplest case of Bernoulli rewards: - Prior for arm \\(i\\): \\(\\mathrm{Beta}(\\alpha_i, \\beta_i)\\). - Each success (reward = 1) updates \\(\\alpha_i \\leftarrow \\alpha_i + 1\\); each failure (0) updates \\(\\beta_i \\leftarrow \\beta_i + 1\\). - Sampling: draw \\(\\tilde{\\theta}_i \\sim \\mathrm{Beta}(\\alpha_i, \\beta_i)\\) for each arm, then pick arm with largest \\(\\tilde{\\theta}_i\\).</p> <p>For Gaussian rewards with unknown mean (and known variance), one could use a normal prior on the mean and update it with observed rewards (obtaining a normal posterior). For arbitrary distributions, one may use a conjugate prior if available, or approximate posteriors (leading to variants like Bootstrapped Thompson Sampling). The Bayesian nature of TS allows incorporation of prior knowledge and naturally provides a way to handle complicated reward models.</p> <p>Regret and Theoretical Results: For a long time, Thompson Sampling was used heuristically without performance guarantees, but recent advances have provided rigorous analyses. In 2012, Agrawal and Goyal proved the first regret bound for Thompson Sampling in the stochastic multi-armed bandit, showing that TS achieves \\(O(\\ln n)\\) expected regret for certain classes of problems. For instance, in a Bernoulli bandit, TS with Beta(1,1) priors (uniform) was shown to satisfy a bound of the same order as UCB1. Further work tightened these results to show that TS can achieve the Lai-Robbins lower bound constants (it is asymptotically optimal) for Bernoulli and more general parametric reward distributions. In other words, Thompson Sampling enjoys logarithmic regret in the stochastic setting, putting it on par with UCB in terms of order-of-growth.</p> <p>One notable aspect of TS is that it naturally handles the exploration\u2013exploitation trade-off via randomness without an explicit exploration bonus or threshold. This often makes TS very effective empirically; it tends to explore \u201cjust enough\u201d based on the uncertainty encoded in the posterior. It is also versatile and has been extended to various settings (contextual bandits, delayed rewards, etc.). The regret analysis of TS is more involved than UCB \u2013 often combining Bayesian priors with frequentist regret arguments or appealing to martingale concentration applied to the posterior \u2013 but the end result is that TS is provably good.</p> <p>Pros and Cons: Thompson Sampling is conceptually elegant and often empirically superior or comparable to UCB. It\u2019s easy to implement for simple models (like Beta-Bernoulli). One potential downside is that it requires maintaining and sampling from a posterior; if the reward model is complex, this could be computationally heavy (though approximate methods exist). Another consideration is that TS is a randomized algorithm (the action selection is stochastic by design), so any single run is subject to randomness; however, in expectation it performs well. Unlike UCB, TS inherently uses prior assumptions; if the prior is poor, early behavior might be suboptimal (though the algorithm will eventually correct it as data overwhelms the prior).</p>"},{"location":"recitations/week6/#6-contextual-bandits","title":"6. Contextual Bandits","text":"<p>Many practical problems extend the basic multi-armed bandit by introducing a dependency on an observed context (also called state or feature) at each decision point. This leads to the contextual bandit model (sometimes called bandit with side information or associative bandit).</p> <p>Definition and Motivation: In a contextual bandit problem, each round \\(t\\) provides the decision-maker with additional information \\(x_t\\) (the context) before an arm is chosen. The context \\(x_t\\) could be user features in an ad-serving scenario or the current state of a system. Formally: - Context space \\(\\mathcal{X}\\) and action (arm) set \\(\\mathcal{A} = \\{1,\\dots,K\\}\\). - At each time \\(t=1,2,\\dots\\), a context \\(x_t \\in \\mathcal{X}\\) is observed. - The agent chooses an arm \\(I_t \\in \\mathcal{A}\\), based on the context and past observations. - A reward \\(R_t\\) is then obtained, drawn from some distribution dependent on both context and chosen arm: \\(R_t \\sim D(\\cdot\\mid x_t, I_t)\\). - The goal is to maximize total reward over time, equivalently minimizing regret against the best policy mapping contexts to arms.</p> <p>In contextual bandits, the optimal action varies with context. For instance, in news recommendations, context is user or time information, and different articles (arms) might be optimal for different users. The bandit algorithm learns a policy \\(\\pi: \\mathcal{X} \\to \\mathcal{A}\\) by trial and error, observing only rewards from chosen arms.</p> <p>Mathematical Formulation: Let \\(\\pi^*\\) be the optimal policy choosing the arm with highest expected reward per context. At time \\(t\\), denote the expected reward for arm \\(a\\) in context \\(x\\) as:</p> \\[ \\mu_a(x) = \\mathbb{E}[R_t \\mid x_t=x, I_t=a] \\] <p>For each context \\(x\\), the optimal arm is:</p> \\[ a^*(x) = \\arg\\max_{a\\in\\mathcal{A}} \\mu_a(x) \\] <p>The contextual regret after \\(n\\) rounds is:</p> \\[ R_n^{\\text{ctx}} = \\sum_{t=1}^n \\left(\\mu_{a^*(x_t)}(x_t) - \\mu_{I_t}(x_t)\\right) \\] <p>The aim is for \\(R_n^{\\text{ctx}}\\) to grow sublinearly in \\(n\\). Contexts can be stochastic (i.i.d.) or adversarial. Often, one assumes contexts are i.i.d. or \\(\\mu_a(x)\\) has structure (like linearity) for tractability.</p> <p>Comparison with Standard Multi-Armed Bandits: The standard (context-free) MAB is a special case where the context \\(x_t\\) is constant or irrelevant. Contextual bandits must learn a more complex mapping from contexts to actions. The challenge is to learn \\(\\mu_a(x)\\) from bandit feedback (observing rewards only for chosen arms).</p> <p>A contextual bandit resembles performing a new bandit problem for each context type, but generalization across contexts is crucial. Typically, a structured assumption for generalization is employed, such as parametric models </p> \\[ \\mu_a(x) = f(x,\\theta_a) \\] <p>For example, linear models:</p> \\[ \\mu_a(x) = x^\\top \\beta_a \\quad\\text{or}\\quad \\mu_a(x) = x^\\top \\beta \\] <p>Thus, the contextual bandit blends supervised learning (reward prediction) with exploration-driven bandit methods.</p> <p>Another difference: exploration never fully stops in contextual bandits, as each new context may be unseen. Algorithms continuously explore new contexts.</p> <p>Exploration Strategies in Contextual Bandits: Common strategies include: - \\(\\varepsilon\\)-greedy: Occasionally choosing random arms to explore. - Optimism (LinUCB): Assuming linear rewards, maintaining estimates \\(\\hat{\\theta}_a\\) and selecting:</p> \\[ I_t = \\arg\\max_a \\left(x_t^\\top \\hat{\\theta}_a + \\alpha\\sqrt{x_t^\\top A_a^{-1} x_t}\\right) \\] <p>achieving regret \\(\\tilde{O}(d\\sqrt{n})\\).</p> <ul> <li>Thompson Sampling: Placing a prior on parameters, sampling from the posterior, and selecting the best sampled arm per context, also achieving \\(\\tilde{O}(d\\sqrt{n})\\) regret.</li> <li>Epoch-Greedy (Policy Learning): Allocating periods for exploration and re-training empirical policy, achieving \\(O(n^{2/3})\\) regret or better.</li> </ul> <p>Directed exploration: Contextual bandits require more directed exploration. Random exploration is inefficient due to context complexity. Algorithms use structured exploration based on predictive uncertainty (UCB).</p> <p>Regret in Contextual Bandits: Regret bounds typically: - Linear contextual bandit: \\(\\tilde{O}(d\\sqrt{n})\\) - Finite policy classes: \\(O(\\sqrt{|\\Pi|n\\log(n)})\\)</p> <p>Complex policy spaces yield higher regret unless additional structural assumptions are provided.</p>"},{"location":"recitations/week6/#authors","title":"Author(s)","text":"<ul> <li> <p>Arshia Gharooni</p> <p>Teaching Assistant</p> <p>arshiyagharoony@gmail.com</p> <p> </p> </li> </ul>"},{"location":"recitations/week7/","title":"Week 7: Value-Based Theory","text":""},{"location":"recitations/week7/#camera-record","title":"Camera Record","text":""},{"location":"recitations/week8/","title":"Week 8: Policy-Based Theory","text":""},{"location":"recitations/week8/#camera-record","title":"Camera Record","text":""},{"location":"recitations/week9/","title":"Week 9: Advanced Theory","text":""},{"location":"resources/","title":"Introduction","text":""},{"location":"workshops/","title":"Introduction","text":""},{"location":"workshops/week1/","title":"Week 1: Introduction to RL","text":""},{"location":"workshops/week1/#screen-record","title":"Screen Record","text":""},{"location":"workshops/week1/#pygame-extra","title":"PyGame [Extra]","text":""},{"location":"workshops/week1/#notebooks","title":"Notebook(s)","text":"Workshop 1 Notebook(s)"},{"location":"workshops/week10/","title":"Week 10: Exploration Methods","text":""},{"location":"workshops/week11/","title":"Week 11: Imitation &amp; Inverse RL","text":""},{"location":"workshops/week12/","title":"Week 12: Offline Methods","text":""},{"location":"workshops/week13/","title":"Week 13: Multi-Agent Methods","text":""},{"location":"workshops/week14/","title":"Week 14: Hierarchical &amp; Meta RL","text":""},{"location":"workshops/week2/","title":"Week 2: Value-Based Methods","text":""},{"location":"workshops/week2/#screen-records","title":"Screen Record(s)","text":""},{"location":"workshops/week2/#notebooks","title":"Notebook(s)","text":"Workshop 2 Notebook(s)"},{"location":"workshops/week3/","title":"Week 3: Policy-Based Methods","text":""},{"location":"workshops/week3/#screen-records","title":"Screen Record(s)","text":""},{"location":"workshops/week3/#notebooks","title":"Notebook(s)","text":"Workshop 3 Notebook(s)"},{"location":"workshops/week4/","title":"Week 4: Advanced Methods","text":""},{"location":"workshops/week4/#screen-records","title":"Screen Record(s)","text":""},{"location":"workshops/week4/#notebooks","title":"Notebook(s)","text":"Workshop 4 Notebook(s)"},{"location":"workshops/week5/","title":"Week 5: Model-Based Methods","text":""},{"location":"workshops/week5/#screen-records","title":"Screen Record(s)","text":""},{"location":"workshops/week5/#notebooks","title":"Notebook(s)","text":"Workshop 5 Notebook(s)"},{"location":"workshops/week6/","title":"Week 6: Multi-Armed Bandits","text":""},{"location":"workshops/week6/#screen-records","title":"Screen Record(s)","text":""},{"location":"workshops/week6/#notebooks","title":"Notebook(s)","text":"Workshop 6 Notebook(s)"},{"location":"blog/archive/2025/02/","title":"February 2025","text":""},{"location":"blog/category/course-updates/","title":"Course Updates","text":""}]}